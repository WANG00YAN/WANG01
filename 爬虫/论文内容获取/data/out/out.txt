{'论文标题':['Scale Invariant Learning from Trapezoidal Data Streams'],'论文作者':['Alagurajah, J', ';\xa0', 'Yuan, X', ';\xa0', 'Wu, XD'],'论文摘要':['Trapezoidal data streams pervasively exist in many applications, where the number of features keeps increasing over time with the continuous arrival of new instances. Extensive learning solutions on such data streams with a doubly streaming nature make a strong assumption that all features have similar scales. However, this is impractical in many applications where features may vary with time. When applying on streaming data with different scaled features, they may cause a poor convergence and be unable to rescale these features under streaming conditions. In this paper, we propose two effective online learning algorithms that can maintain feature scale-invariants even with arbitrary scaling of features in the incoming trapezoidal data streams. Specifically, two algorithms are designed to learn a classifier by keeping track of the cumulative sum of squared gradients, the negative cumulative sum of gradients, maximum feature values occurred and the cumulative sum of occurrences of features. The scaled gradients are employed to update their corresponding classifier weights. We also conduct experiments on nine UCI datasets to evaluate the effectiveness of our proposed algorithms. The experimental results demonstrate that the features in the incoming trapezoidal data streams indeed have different scales and our proposed solutions can significantly outperform the state-of-the-art solution in terms of prediction accuracy.']}
{'论文标题':['Attribute Selection Based on Constraint Gain and Depth Optimal for a Decision Tree'],'论文作者':['Sun, HN', ';\xa0', 'Hu, XG', ';\xa0', 'Zhang, YH'],'论文摘要':['Uncertainty evaluation based on statistical probabilistic information entropy is a commonly used mechanism for a heuristic method construction of decision tree learning. The entropy kernel potentially links its deviation and decision tree classification performance. This paper presents a decision tree learning algorithm based on constrained gain and depth induction optimization. Firstly, the calculation and analysis of single- and multi-value event uncertainty distributions of information entropy is followed by an enhanced property of single-value event entropy kernel and multi-value event entropy peaks as well as a reciprocal relationship between peak location and the number of possible events. Secondly, this study proposed an estimated method for information entropy whose entropy kernel is replaced with a peak-shift sine function to establish a decision tree learning (CGDT) algorithm on the basis of constraint gain. Finally, by combining branch convergence and fan-out indices under an inductive depth of a decision tree, we built a constraint gained and depth inductive improved decision tree (CGDIDT) learning algorithm. Results show the benefits of the CGDT and CGDIDT algorithms.']}
{'论文标题':['A Label-based Partitioning Strategy for Mining Link Patterns'],'论文作者':['Zhao, CF', ';\xa0', 'Zhang, X', ';\xa0', 'Wang, P'],'论文摘要':['As the explosive growth of online linked data, the task of mining link patterns attracts more and more attention. A practical issue is how to perform mining efficiently in large-scale linked data. Existing pattern mining algorithms usually assume that the dataset can fit into the main memory, while linked data of billion triples is far beyond the memory limitation. In this paper we give a pilot study of a novel partitioning strategy for mining link patterns in large-scale linked data. First, we propose an algorithm named ParGroup to divide and group large linked data to partitions based on vertex label; Second, an adapted gSpan is applied for mining link patterns in each partition; At last, discovered link patterns are merged into a global result set. Experiments show that our strategy is feasible and promising in some scenarios.']}
{'论文标题':['Learning Interpretable Representations with Informative Entanglements'],'论文作者':['Beyazit, E', ';\xa0', 'Tuncel, D', ';\xa0', 'Wu, XD'],'论文摘要':['Learning interpretable representations in an unsupervised setting is an important yet a challenging task. Existing unsupervised interpretable methods focus on extracting independent salient features from data. However they miss out the fact that the entanglement of salient features may also be informative. Acknowledging these entanglements can improve the interpretability, resulting in extraction of higher quality and a wider variety of salient features. In this paper, we propose a new method to enable Generative Adversarial Networks (GANs) to discover salient features that may be entangled in an informative manner, instead of extracting only disentangled features. Specifically, we propose a regularizer to punish the disagreement between the extracted feature interactions and a given dependency structure while training. We model these interactions using a Bayesian network, estimate the maximum likelihood parameters and calculate a negative likelihood score to measure the disagreement. Upon qualitatively and quantitatively evaluating the proposed method using both synthetic and real-world datasets, we show that our proposed regularizer guides GANs to learn representations with disentanglement scores competing with the state-of-the-art, while extracting a wider variety of salient features.']}
{'论文标题':['Mining frequent itemsets using a pruned concept lattice'],'论文作者':['Hu, XG', ';\xa0', 'Liu, W', ';\xa0', 'Wu, XD'],'论文摘要':["Mining frequent itemsets is a crucial step in association rule mining, However, most algorithms mining frequent itemsets scan databases many times, which decreases the efficiency. In this paper, the relationship between a concept lattice and frequent itemsets is discussed, and the model of pruned concept lattice (PCL) is introduced to represent frequent itemsets in a given database, and the scale of frequent itemsets is compressed effectively. An algorithm for mining frequent itemsets based on PCL is proposed, which prunes infrequent concepts timely and dynamically during the PCL's construction according to the Apriori property. The efficiency of the algorithm is demonstrated with experiments."]}
{'论文标题':['Feature-induced Partial Multi-label Learning'],'论文作者':['Yu, GX', ';\xa0', 'Chen, X', ';\xa0', 'Wu, XD'],'论文摘要':['Current efforts on multi-label learning generally assume that the given labels of training instances are noise-free. However, obtaining noise-free labels is quite difficult and often impractical, and the presence of noisy labels may compromise the performance of multi-label learning. Partial multi-label learning (PML) addresses the scenario in which each instance is annotated with a set of candidate labels, of which only a subset corresponds to the ground-truth. The PML problem is more challenging than partial-label learning, since the latter assumes that only one label is valid and may ignore the correlation among candidate labels. To tackle the PML challenge, we introduce a feature induced PML approach called fPML, which simultaneously estimates noisy labels and trains multi-label classifiers. In particular, fPML simultaneously factorizes the observed instance-label association matrix and the instance-feature matrix into low-rank matrices to achieve coherent low-rank matrices from the label and the feature spaces, and a low-rank label correlation matrix as well. The low-rank approximation of the instance-label association matrix is leveraged to estimate the association confidence. To predict the labels of unlabeled instances, fPML learns a matrix that maps the instances to labels based on the estimated association confidence. An empirical study on public multi-label datasets with injected noisy labels, and on archived proteomic datasets, shows that fPML can more accurately identify noisy labels than related solutions, and consequently can achieve better performance on predicting labels of instances than competitive methods.']}
{'论文标题':['Scalable Semi-Supervised Learning by Efficient Anchor Graph Regularization'],'论文作者':['Wang, M', ';\xa0', 'Fu, WJ', ';\xa0', 'Wu, XD'],'论文摘要':['Many graph-based semi-supervised learning methods for large datasets have been proposed to cope with the rapidly increasing size of data, such as Anchor Graph Regularization (AGR). This model builds a regularization framework by exploring the underlying structure of the whole dataset with both datapoints and anchors. Nevertheless, AGR still has limitations in its two components: (1) in anchor graph construction, the estimation of the local weights between each datapoint and its neighboring anchors could be biased and relatively slow; and (2) in anchor graph regularization, the adjacency matrix that estimates the relationship between datapoints, is not sufficiently effective. In this paper, we develop an Efficient Anchor Graph Regularization (EAGR) by tackling these issues. First, we propose a fast local anchor embedding method, which reformulates the optimization of local weights and obtains an analytical solution. We show that this method better reconstructs datapoints with anchors and speeds up the optimizing process. Second, we propose a new adjacency matrix among anchors by considering the commonly linked datapoints, which leads to a more effective normalized graph Laplacian over anchors. We show that, with the novel local weight estimation and normalized graph Laplacian, EAGR is able to achieve better classification accuracy with much less computational costs. Experimental results on several publicly available datasets demonstrate the effectiveness of our approach.']}
{'论文标题':['Automatic recognition of news web pages'],'论文作者':['Zhu, Z', ';\xa0', 'Wu, GQ', ';\xa0', 'Wang, FY'],'论文摘要':['The information on the World Wide Web is congested with large amounts of news contents. The filtering, summarization and classification of news Web pages have become hot topics of research, aiming for useful news contents. Accurately identifying news Web pages is a crucial problem in these research topics. To solve this problem, this paper proposes an automatic recognition method for news Web pages based on a combination of URL attributes, structure attributes and content attributes. Our experimental results demonstrate that this method provides a high accuracy of above 96% with the recognition of news Web page.']}
{'论文标题':['A Data-Characteristic-Aware Latent Factor Model for Web Services QoS Prediction'],'论文作者':['Wu, D', ';\xa0', 'Luo, X', ';\xa0', 'Wu, XD'],'论文摘要':["How to accurately predict unknown quality-of-service (QoS) data based on observed ones is a hot yet thorny issue in Web service-related applications. Recently, a latent factor (LF) model has shown its efficiency in addressing this issue owing to its high accuracy and scalability. An LF model can be improved by identifying user and service neighborhoods based on user and service geographical information. However, such information can be difficult to acquire in most applications with the considerations of information security, identity privacy, and commercial interests in a real system. Besides, the existing LF model-based QoS predictors mostly ignore the reliability of given QoS data where noises commonly exist to cause accuracy loss. To address the above issues, this paper proposes a data-characteristic-aware latent factor (DCALF) model to implement highly accurate QoS predictions, where 'data-characteristic-aware' indicates that it can appropriately implement QoS prediction according to the characteristics of given QoS data. Its main idea is two-fold: a) it detects the neighborhoods and noises of users and services based on the dense LFs extracted from the original sparse QoS data, b) it incorporates a density peaks-based clustering method into its modeling process for achieving the simultaneous detections of both neighborhoods and noises of QoS data. With such designs, it precisely represents the given QoS data in spite of their sparsity, thereby achieving highly accurate predictions for unknown ones. Experimental results on two QoS datasets generated by real-world Web services demonstrate that the proposed DCALF model outperforms state-of-the-art QoS predictors, making it highly competitive in addressing the issue of Web service selection and recommendation."]}
{'论文标题':['Method for unsupervised feature selection for online machine learning, involves determining aggregate feature set for updating one line machine learning models in real time at periodic frequency'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation'],'论文作者':['Aliferis, CF', ';\xa0', 'Statnikov, A', ';\xa0', 'Koutsoukos, XD'],'论文摘要':['We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classification. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-defined sufficient conditions. In a first set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance.', 'Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distributions, types of classifiers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we find that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought.', 'In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show how local techniques can be used for scalable and accurate global causal graph learning.']}
{'论文标题':['D-S Evidence Theory based Digital Image Trustworthiness Evaluation model'],'论文作者':['Hu, DH', ';\xa0', 'Wang, LN', ';\xa0', 'Ma, LF'],'论文摘要':["Digital images are facing trustworthy crisis with the emergence of various digital image process tools and steganography tools. Current digital image forensic methods can not evaluate the digital image's trustworthiness. Base on Dempster-Shafer (D-S) evidence theory, this paper proposed a new model which can effectively evaluate digital image trustworthiness. Instead of using Dempster's original rule of combination, we use feature fusion method to resolve the problem of evidence conflict. Experimental results show that the new proposed model not only can evaluate digital image trustworthiness effectively, but also resolve some problems which puzzling current digital image forensic technology."]}
{'论文标题':['Towards Effective Bug Triage with Software Data Reduction Techniques'],'论文作者':['Xuan, JF', ';\xa0', 'Jiang, H', ';\xa0', 'Wu, XD'],'论文摘要':['Software companies spend over 45 percent of cost in dealing with software bugs. An inevitable step of fixing bugs is bug triage, which aims to correctly assign a developer to a new bug. To decrease the time cost in manual work, text classification techniques are applied to conduct automatic bug triage. In this paper, we address the problem of data reduction for bug triage, i.e., how to reduce the scale and improve the quality of bug data. We combine instance selection with feature selection to simultaneously reduce data scale on the bug dimension and the word dimension. To determine the order of applying instance selection and feature selection, we extract attributes from historical bug data sets and build a predictive model for a new bug data set. We empirically investigate the performance of data reduction on totally 600,000 bug reports of two large open source projects, namely Eclipse and Mozilla. The results show that our data reduction can effectively reduce the data scale and improve the accuracy of bug triage. Our work provides an approach to leveraging techniques on data processing to form reduced and high-quality bug data in software development and maintenance.']}
{'论文标题':['Wide area selection as a hyperdocument search interface'],'论文作者':['Lowder, J', ';\xa0', 'Wu, XD'],'论文摘要':['Area selection allows readers to form their own associations between documents by presenting selected text as a search query. Results can be kept along with the original text to make persistent collections of links and association text. This overcomes problems in query formulation and loss of relevant information due to document alteration. (C) 1998 Published by Elsevier Science B.V. All rights reserved.']}
{'论文标题':['Intrusion detection system based on data mining'],'论文作者':['Zhang, JS', ' (', 'Zhang, Jishan', ') ', ';\xa0', 'Gan, Y', ' (', 'Gan, Yong', ') ', ';\xa0', 'Bian, ZW', ' (', 'Bian, Zhiwei', ') '],'论文摘要':['Aiming at intrusion detection system based on data mining existing problems, this paper proposes a new intrusion detection system based on data mining. Data filtration method, association/sequence rules and sorting algorithm of data mining are used in this model. It can make intrusion detection more automatic and improve the efficiency and accuracy of intrusion detection.']}
{'论文标题':['Online Web News Extraction via Tag Path Feature Fusion'],'论文作者':['Wu Gong-Qing', ';\xa0', 'Hu Jun', ';\xa0', 'Wu Xin-Dong'],'论文摘要':['Accurately extracting content from Web news is a key technology for quality improvement in Web news analysis and applications. Due to the lack of publication standards, differences in publishing formats, and a highly heterogeneous big data carrier of the Web itself, Web news extraction has become an open research problem. Extensive case studies by this research indicate that there is potential relevance between Web content layouts and their tag paths. Inspired by this observation, this paper designs a series of tag path extraction features to distinguish the Web content and noise from different perspectives. Based on the similarity analysis of these features, the paper proposes a features fusion strategy with group feature selection, and provides a Web news extraction method via feature fusion, CEPF. CEPF is a fast, universal, no-training and online Web news extraction algorithm. It can extract Web news pages across multi-resources, multi-styles, and multi-languages. Experimental results with public data sets such as CleanEval show that the CEPF method achieves better performance than the state-of-the-art CETR method.', '精准地抽取新闻网页的内容,是提高Web新闻分析等应用系统工作质量的关键技术之一.由于缺少Web新闻出版的标准,存在大量不同的出版格式,并且Web本身是一种具有高度异构性的大数据载体,导致Web新闻内容抽取成为一个开放性问题.经大量实例分析发现,新闻网页内容与其上的标签路径存在潜在的关联性.因此,设计了标签路径特征系,以从不同视角区分网页内容和噪音.在特征相似性分析的基础上,提出了一种基于组合特征选择的特征融合策略,并设计了基于融合特征的Web新闻内容抽取方法CEPF.CEPF是一种快速的通用、无需训练的在线Web新闻内容抽取算法,可抽取多种来源、多种风格、多种语言的Web新闻网页.在CleanEval等测试数据集上的实验结果表明,CEPF方法优于CETR等抽取方法.']}
{'论文标题':['Weighted Nonnegative Matrix Tri-Factorization for Co-Clustering'],'论文作者':['Li, Z', ';\xa0', 'Wu, XD'],'论文摘要':['Nonnegative matrix tri-factorization and spectral co-clustering are two popular techniques that allow simultaneous clustering of the rows and columns of a matrix. In this paper, by adding a weighting scheme derived from spectral co-clustering into the objective function of nonnegative matrix tri-factorization, we show that the normalized cut information for co-clustering can be incorporated into nonnegative matrix tri-factorization. With the weighting scheme, a weighted nonnegative matrix tri-factorization algorithm for co-clustering is proposed, and extensive experiments show that our method statistically outperforms state-of-the-art co-clustering algorithms.']}
{'论文标题':['Scalable nearest-neighbor sparse graph approximation by exploiting graph structure'],'论文作者':['Ming Shao', ';\xa0', 'Xindong Wu', ';\xa0', 'Yun Fu'],'论文摘要':['We consider exploiting graph structure for sparse graph approximations. Graphs play increasingly important roles in learning problems: manifold learning, kernel learning, and spectral clustering. Specifically, in this paper, we concentrate on nearest neighbor sparse graphs which are widely adopted in learning problems due to its spatial efficiency. Nonetheless, we raise an even challenging problem: can we save more memory space while keep competitive performance for the sparse graph? To this end, first, we propose to partition the entire graph into intra- and inter-graphs by exploring the graph structure, and use both of them for graph approximation. Therefore, neighborhood similarities within each cluster, and between different clusters are well preserved. Second, we improve the space use of the entire approximation algorithm. Specially, a novel sparse inter-graph approximation algorithm is proposed, and corresponding approximation error bound is provided. Third, extensive experiments are conducted on 11 real-world datasets, ranging from small to large-scales, demonstrating that when using less space, our approximate graph can provide comparable or even better performance, in terms of approximation error, and clustering accuracy. In large-scale test, we use less than 1/100 memory of comparable algorithms, but achieve very appealing results.']}
{'论文标题':['Spectral-Spatial Classification of Hyperspectral Images with Semi-Supervised Graph Learning'],'论文作者':['Luo, RB', ';\xa0', 'Liao, WZ', ';\xa0', 'Philips, W'],'论文摘要':['In this paper, we propose a novel semi-supervised graph leaning method to fuse spectral (of original hyperspectral (HS) image) and spatial (from morphological features) information for classification of HS image. In our proposed semi-supervised graph, samples are connected according to either label information (labeled samples) or their k-nearest spectral and spatial neighbors (unlabeled samples). Furthermore, we link the unlabeled sample with all labeled samples in one class which is the closest to this unlabeled sample in both spectral and spatial feature spaces. Thus, the connected samples have similar characteristics on both spectral and spatial domains, and have high possibilities to belong to the same class. By exploiting the fused semi-supervised graph, we then get transformation matrices to project high-dimensional HS image and morphological features to their lower dimensional subspaces. The final classification map is obtained by concentrating the lower-dimensional features together as an input of SVM classifier. Experimental results on a real hyperspectral data demonstrate the efficiency of our proposed semi-supervised fusion method. Compared to the methods using unsupervised fusion or supervised fusion, the proposed semi-supervised fusion method enables improved performances on classification. Moreover, the classification performances keep stable even when a small number of labeled training samples is available.']}
{'论文标题':['Markov Boundary Learning With Streaming Data for Supervised Classification'],'论文作者':['Liu, CF', ';\xa0', 'Yang, S', ';\xa0', 'Yu, K'],'论文摘要':['In this paper, we study the problem of Markov boundary (MB) learning with streaming data. A MB is a crucial concept in a Bayesian network (BN) and plays an important role in BN structure learning. In addition, in the supervised learning setting, the MB of a class attribute is optimal to feature selection for classification. Almost all existing MB learning algorithms focus on static data, but few efforts have been proposed to learning MBs with streaming data. In this paper, by linking dynamic AD-trees with streaming data, we proposed a new SDMB (streaming data-based MB) algorithm for learning MBs with streaming data. Specifically, given a target variable, SDMB employs a dynamic AD-tree to summarize the historical data, then the SDMB sequentially learns the MB of the target upon all available data by calculating independence tests using the dynamic AD-tree. In experiments, using the synthetic and real-world data sets, we evaluate the SDMB algorithm and compared it with the state-of-the-art online feature selection algorithms and data stream mining methods, and the experimental results validate the SDMB algorithm.']}
{'论文标题':['Incremental algorithms for attribute reduction in decision table'],'论文作者':['Hu Feng', ';\xa0', 'Dai Jin', ';\xa0', 'Wang Guo-yin'],'论文摘要':['Incremental algorithms for attribute reduction based on modified discernibility matrix are proposed, by which minimal attribute reduction cluster of new decision table can be obtained quickly when new records are added to primary decision table. A distributed model of incremental attribute reduction is also presented by decomposing values of decision attribute of positive region and boundary region in non-tolerant decision table. The simulation experiments show the validity and effectiveness of algorithms.', '为了对动态变化的决策表进行属性约简处理，在改进的分辨矩阵的基础上，提出一种增量式属性约简算法，当决策表添加新的记录后．能快速得到新决策表的所有约简和最小约筒．此外，通过对不相容决策表的正区域的决策值和边界域对原决策表进行分解．得到了一种分布式增量属性约简模型．仿真研究表明了算法的正确性和高效性．']}
{'论文标题':['Mining sequential patterns across multiple sequence databases'],'论文作者':['Peng, WC', ';\xa0', 'Liao, ZX'],'论文摘要':['In this paper, given a set of sequence databases across multiple domains, we aim at mining multi-domain sequential patterns, where a multi-domain sequential pattern is a sequence of events whose occurrence time is within a pre-defined time window. We first: propose algorithm Naive in which multiple sequence databases are joined as one sequence database for utilizing traditional sequential pattern mining algorithms (e.g., PrefixSpan). Due to the nature of join operations, algorithm Naive is costly and is developed for comparison purposes. Thus, we propose two algorithms without any join operations for mining multidomain sequential patterns. Explicitly, algorithm IndividualMine derives sequential patterns in each domain and then iteratively combines sequential patterns among sequence databases of multiple domains to derive candidate multi-domain sequential patterns. However, not all sequential patterns mined in the sequence database of each domain are able to form multi-domain sequential patterns. To avoid the mining cost incurred in algorithm IndividualMine, algorithm PropagatedMine is developed. Algorithm PropagatedMine first performs one sequential pattern mining from one sequence database. In light of sequential patterns mined, algorithm PropagatedMine propagates sequential patterns mined to other sequence databases. Furthermore, sequential patterns mined are represented as a lattice structure for further reducing the number of sequential patterns to be propagated. In addition. we develop some mechanisms to allow some empty sets in multi-domain sequential patterns. Performance of the proposed algorithms is comparatively analyzed and sensitivity analysis is conducted. Experimental results show that by exploring propagation and lattice structures, algorithm PropagatedMine outperforms algorithm IndividualMine in terms of efficiency (i.e., the execution time). (C) 2009 Elsevier B.V. All rights reserved.']}
{'论文标题':['Learning Markov Blankets From Multiple Interventional Data Sets'],'论文作者':['Yu, K', ';\xa0', 'Liu, L', ';\xa0', 'Li, JY'],'论文摘要':['Learning Markov blankets (MBs) plays an important role in many machine learning tasks, such as causal Bayesian network structure learning, feature selection, and domain adaptation. Since variables included in the MB of a target variable of interest have causal relationships with the target, the MB can serve as the basis of learning the global structure of a causal Bayesian network or as a reliable and robust feature set for classification, both within the same domain or across domains. In this article, we study the problem of learning the MB of a target variable from multiple interventional data sets. Data sets attained from interventional experiments contain richer causal information than passively observed data (observational data) for MB discovery. However, almost all existing MB discovery methods are designed for learning MBs from a single observational data set. To learn MBs from multiple interventional data sets, we face two challenges: 1) unknown intervention variables and 2) nonidentical data distributions. To address these challenges, we theoretically analyze: 1) under what conditions we can find the correct MB of a target variable and 2) under what conditions we can identify the causes of the target variable via discovering its MB. Based on the theoretical analysis, we propose a new algorithm for learning MBs from multiple interventional data sets, and we present the conditions/assumptions that assure the correctness of the algorithm. To the best of our knowledge, this article is the first to present the theoretical analyses about the conditions for MB discovery in multiple interventional data sets and the algorithm to find the MBs in relation to the conditions. Using benchmark Bayesian networks and real-world data sets, the experiments have validated the effectiveness and efficiency of the proposed algorithm in this article.']}
{'论文标题':['Robust object tracking via multi-scale patch based sparse coding histogram'],'论文作者':['Wang, ZP', ';\xa0', 'Wang, H', ';\xa0', 'Xie, CJ'],'论文摘要':['There are many visual tracking algorithms that are based on sparse representation appearance model. Most of them are modeled by local patches with fixed patch scale, which make trackers less effective when objects undergone appearance changes such as illumination variation, pose change or partial occlusion. To solve the problem, a novel appearance representation model is proposed via multi-scale patch based sparse coding histogram for robust visual tracking. In this paper, the appearance of an object is modeled by different scale patches, which are represented by sparse coding histogram with different scale dictionaries. Then a similarity measure is applied to the calculation of the distance between the sparse coding histograms of target candidate and target template. Finally, the similarity score of the target candidate is passed to a particle filter to estimate the target state sequentially in the tracking process. Additionally, in order to decrease the visual drift caused by partial occlusion, an occlusion handling strategy is adopted, which takes the spatial information of multi-scale patches and occlusion into account. Based on the experimental results on some benchmarks of video sequences, our tracker outperforms state-of-the-art tracking methods.']}
{'论文标题':['Online streaming feature selection with incremental feature grouping'],'论文作者':['Al Nuaimi, N', ';\xa0', 'Masud, MM'],'论文摘要':['Today, the dimensionality of data is increasing in a massive way. Thus, traditional feature selection techniques are not directly applicable. Consequently, recent research has led to the development of a more efficient approach to the selection of features from a feature stream, known as streaming feature selection. Another active research area, related to feature selection, is feature grouping. Feature grouping selects relevant features by evaluating the hidden information of selected features. However, although feature grouping is a promising technique, it is not directly applicable to feature streams. In this paper, we propose a novel and efficient algorithm that uses online feature grouping, embedded within a new incremental technique, to select features from a feature stream. This technique groups similar features together; it assigns new incoming features to an existing group or creates a new group. To the best of our knowledge, this is the first approach that proposes the use of incremental feature grouping to perform feature selection from features. We have implemented this algorithm and evaluated it, using benchmark datasets, against state-of-the-art streaming feature selection algorithms that use feature grouping or incremental selection techniques. The results show superior performance by the proposed technique through combining the online selection and grouping, in terms of prediction accuracy and running time. This article is categorized under: Algorithmic Development > Spatial and Temporal Data Mining Technologies > Data Preprocessing Technologies > Classification Technologies > Machine Learning']}
{'论文标题':['Keyword extraction based on lexical chains and word co-occurrence for Chinese news Web pages'],'论文作者':['Xinghua Li', ';\xa0', 'Xindong Wu', ';\xa0', 'Zhaozhong Jiang'],'论文摘要':['This paper presents a new keyword extraction algorithm for Chinese news Web pages using lexical chains and word co-occurrence combined with frequency features, cohesion features, and corelation features. A lexical chain is an external performance consistency by semantically related words of a text, and is the representation of the semantic content of a portion of the text. Word co-occurrence distribution is an important statistical model widely used in natural language processing that reflects the correlation of the words. Lexical chains and word co-occurrence are combined in this paper to extract keywords for Chinese news Web pages in our proposed algorithm KELCC. This algorithm is not domain-specific and can be applied to a single Web page without corpus. Experiments on randomly selected Web pages have been performed to demonstrate the quality of the keywords extracted by our proposed algorithm.']}
{'论文标题':['An new immune genetic algorithm based on uniform design sampling'],'论文作者':['Zhou, BD', ';\xa0', 'Yao, HL', ';\xa0', 'Wang, H'],'论文摘要':['The deficiencies of keeping population diversity, prematurity and low success rate of searching the global optimal solution are the shortcomings of genetic algorithm (GA). Based on the bias of samples in the uniform design sampling (UDS) point set, the crossover operation in GA is redesigned. Using the concentrations of antibodies in artificial immune system (AIS), the chromosomes concentration in GA is defined and the clonal selection strategy is designed. In order to solve the maximum clique problem (MCP), an new immune GA (UIGA) is presented based on the clonal selection strategy and UDS. The simulation results show that the UIGA provides superior solution quality, convergence rate, and other various indices to those of the simple and good point GA when solving MCPs.']}
{'论文标题':['Semi-supervised representation learning via dual autoencoders for domain adaptation'],'论文作者':['Yang, S', ';\xa0', 'Wang, H', ';\xa0', 'Hu, XG'],'论文摘要':['Domain adaptation aims to exploit the knowledge in source domain to promote the learning tasks in target domain, which plays a critical role in real-world applications. Recently, lots of deep learning approaches based on autoencoders have achieved a significance performance in domain adaptation. However, most existing methods focus on minimizing the distribution divergence by putting the source and target data together to learn global feature representations, while they do not consider the local relationship between instances in the same category from different domains. To address this problem, we propose a novel Semi-Supervised Representation Learning framework via Dual Autoencoders for domain adaptation, named SSRLDA. More specifically, we extract richer feature representations by learning the global and local feature representations simultaneously using two novel autoencoders, which are referred to as marginalized denoising autoencoder with adaptation distribution (MDA(ad)) and multi-class marginalized denoising autoencoder (MMDA) respectively. Meanwhile, we make full use of label information to optimize feature representations. Experimental results show that our proposed approach outperforms several state-of-the-art baseline methods. (C) 2019 Published by Elsevier B.V.']}
{'论文标题':['European approach to nuclear and radiological emergency management and rehabilitation strategies (EURANOS)'],'论文作者':['Raskob, W'],'论文摘要':["The 5-year multi-national project EURANOS, funded by the European Commission and 23 European Member States, started in April 2004. Integrating 17 national emergency management organisations with 33 research institutes, it brings together best practice, knowledge and technology to enhance the preparedness for Europe's response to any radiation emergency and long-term contamination. Key objectives of the project are to collate information on the likely effectiveness and consequences of a wide range of countermeasures, to provide guidance to emergency management organisations and decision makers on the establishment of an appropriate response strategy and to further enhance advanced decision support systems, in particular, RODOS (Real-time On-line DecisiOn Support), through feedback from their operational use. The research activities focused on emergency actions and countermeasures, the enhancement of decision support systems for operational application and the development of strategies for the sustainable rehabilitation of contaminated territories. The demonstration activities exercise the developed methods and tools in the actual operational environment."]}
{'论文标题':['A set-level joint sparse representation for image set classification'],'论文作者':['Zheng, P', ';\xa0', 'Zhao, ZQ', ';\xa0', 'Wu, XD'],'论文摘要':["Traditional image set classification methods measure the similarities between different sets based on the extracted characteristics of each set. Most of these methods build their models on one kind of visual features or a simply concatenated feature vector of several kinds of features. However, due to redundant or irrelevant information, the concatenated features are usually not discriminant or suffer from the curse of dimensionality. Meanwhile, if the sizes of sets are small, or improper features are employed, the useful information will be limited and conflictive. So in this paper, we propose a set-level joint sparse representation classification (SJSRC) model to combine multiple features to accomplish image set classification task. In the SJSRC, the atom-level and concept-level regularization terms are both imposed to obtain robust representations and the images in the same concept are regarded as a whole to optimize the objective on them jointly to strengthen the intra similarities via a set-level regularization. In addition, we adopt two schemes, namely 'Anchor Graph' and 'Regularized Nearest Points (RNP)', to improve computational efficiency and identification rate. Experiments on several benchmark datasets show that our model obtains competitive recognition performance for image set classification. (C) 2018 Elsevier Inc. All rights reserved."]}
{'论文标题':['Multi-level rough set reduction for decision rule mining'],'论文作者':['Ye, MQ', ';\xa0', 'Wu, XD', ';\xa0', 'Hu, DH'],'论文摘要':["Most previous studies on rough sets focused on attribute reduction and decision rule mining on a single concept level. Data with attribute value taxonomies (AVTs) are, however, commonly seen in real-world applications. In this paper, we extend Pawlak's rough set model, and propose a novel multi-level rough set model (MLRS) based on AVTs and a full-subtree generalization scheme. Paralleling with Pawlak's rough set model, some conclusions related to the MLRS are given. Meanwhile, a novel concept of cut reduction based on MLRS is presented. A cut reduction can induce the most abstract multi-level decision table with the same classification ability on the raw decision table, and no other multi-level decision table exists that is more abstract. Furthermore, the relationships between attribute reduction in Pawlak's rough set model and cut reduction in MLRS are discussed. We also prove that the problem of cut reduction generation is NP-hard, and develop a heuristic algorithm named CRTDR for computing the cut reduction. Finally, an approach named RMTDR for mining multi-level decision rule is provided. It can mine decision rules from different concept levels. Example analysis and comparative experiments show that the proposed methods are efficient and effective in handling the problems where data is associated with AVTs."]}
{'论文标题':['Nonlinear cross-domain feature representation learning method based on dual constraints'],'论文作者':['Ding, H', ';\xa0', 'Zhang, YH', ';\xa0', 'Lin, YJ'],'论文摘要':['Feature representation learning is a research focus in domain adaptation. Recently, due to the fast training speed, the marginalized Denoising Autoencoder (mDA) as a standing deep learning model has been widely utilized for feature representation learning. However, the training of mDA suffers from the lack of nonlinear relationship and does not explicitly consider the distribution discrepancy between domains. To address these problems, this paper proposes a novel method for feature representation learning, namely Nonlinear cross-domain Feature learning based Dual Constraints (NFDC), which consists of kernelization and dual constraints. Firstly, we introduce kernelization to effectively extract nonlinear relationship in feature representation learning. Secondly, we design dual constraints including Maximum Mean Discrepancy (MMD) and Manifold Regularization (MR) in order to minimize distribution discrepancy during the training process. Experimental results show that our approach is superior to several state-of-the-art methods in domain adaptation tasks.']}
{'论文标题':['Inductive learning model processing method for intrusion detection, involves partitioning dataset into number of subsets, and developing estimated learning model for dataset by developing learning model for one subset'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Competitive Pricing Strategies for Software and SaaS Products'],'论文作者':['Zhang, Z'],'论文摘要':["We study the pricing strategies for a software firm and an entrant software-as-a-service (SaaS) firm in two customer markets: the market composed of the incumbent's past customers and the market of new customers. We build a game theoretical model to investigate how user costs and the quality differential between products affect firms' pricing in different customer markets. Our findings show that it is not always optimal for the software firm to price discriminate between its old users and the new customers. The entrant firm would be better off acquiring only the new customers when the SaaS quality is sufficiently low."]}
{'论文标题':['The research of sampling for mining frequent itemsets'],'论文作者':['Hu, XG', ';\xa0', 'Yu, HT'],'论文摘要':['Efficiently mining frequent itemsets is the key step in extracting association rules from large scale databases. Considering the restriction of min-support in mining association rules, a weighted sampling algorithm for mining frequent itemsets is proposed in the paper. First of all, a weight is given to each transaction data. Then according to the statistical optimal sample size of database, a sample is extracted based on weight of data. In terms of the algorithm, the sample includes large amounts of transaction data consisting of the frequent itemsets with many items inside, so that the frequent itemsets mined from sample are similar to those gained from the original data. Furthermore, the algorithm can shrink the sample size and guarantee the sample quality at the same time. The experiment verifys the validity.']}
{'论文标题':['HDCache: A Distributed Cache System for Real-Time Cloud Services'],'论文作者':['Zhang, J', ';\xa0', 'Li, QM', ';\xa0', 'Zhou, W'],'论文摘要':['Providing a real-time cloud service requires simultaneously retrieving a large amount of data. How to improve the performance of file access becomes a great challenge. This paper first addresses the preconditions of dealing with this problem considering the requirements of applications, hardware, software, and network environments in the cloud. Then, a novel distributed layered cache system named HDCache is proposed. HDCahe is built on the top of Hadoop Distributed File System (HDFS). Applications can integrate the client library of HDCache to access the multiple cache services. The cache services are built up with three access layers an in-memory cache, a snapshot of the local disk, and a network disk provided by HDFS. The files loaded from HDFS are cached in a shared memory which can be directly accessed by the client library. In order to improve robustness and alleviate workload, the cache services are organized in a peer-to-peer style using a distributed hash table and every cached file has three replicas scattered in different cache service nodes. Experimental results show that HDCache can store files with a wide range in their sizes and has the access performance in a millisecond level under highly concurrent environments. The tested hit ratio obtained from a real-world cloud serviced is higher than 95 %.']}
{'论文标题':['Visual Classification by l(1)-Hypergraph Modeling'],'论文作者':['Wang, M', ';\xa0', 'Liu, XL', ';\xa0', 'Wu, XD'],'论文摘要':['Visual classification has attracted considerable research interests in the past decades. In this paper, a novel l(1)-hypergraph model for visual classification is proposed. Hypergraph learning, as a natural extension of graph model, has been widely used in many machine learning tasks. In previous work, hypergraph is usually constructed by attribute-based or neighborhood-based methods. That is, a hyperedge is generated by connecting a set of samples sharing a same feature attribute or in a neighborhood. However, these methods are unable to explore feature space globally or sensitive to noises. To address these problems, we propose a novel hypergraph construction approach that leverages sparse representation to generate hyperedges and learns the relationship among hyperedges and their vertices. First, for each sample, a hyperedge is generated by regarding it as the centroid and linking it as well as its nearest neighbors. Then, the sparse representation method is applied to represent the centroid vertex by other vertices within the same hyperedge. The vertices with zero coefficients are removed from the hyperedge. Finally, the representation coefficients are used to define the incidence relation between the hyperedge and the vertices. In our approach, we also optimize the hyperedge weights to modulate the effects of different hyperedges. We leverage the prior knowledge on the hyperedges so that the hyperedges sharing more vertices can have closer weights, where a graph Laplacian is used to regularize the optimization of the weights. Our approach is named l(1)-hypergraph since the l(1) sparse representation is employed in the hypergraph construction process. The method is evaluated on various visual classification tasks, and it demonstrates promising performance.']}
{'论文标题':['Multimodal Deep Network Embedding with Integrated Structure and Attribute Information [arXiv]'],'论文作者':['Conghui Zheng', ';\xa0', 'Li Pan', ';\xa0', 'Peng Wu'],'论文摘要':['Network embedding is the process of learning low-dimensional representations for nodes in a network, while preserving node features. Existing studies only leverage network structure information and focus on preserving structural features. However, nodes in real-world networks often have a rich set of attributes providing extra semantic information. It has been demonstrated that both structural and attribute features are important for network analysis tasks. To preserve both features, we investigate the problem of integrating structure and attribute information to perform network embedding and propose a Multimodal Deep Network Embedding (MDNE) method. MDNE captures the non-linear network structures and the complex interactions among structures and attributes, using a deep model consisting of multiple layers of non-linear functions. Since structures and attributes are two different types of information, a multimodal learning method is adopted to pre-process them and help the model to better capture the correlations between node structure and attribute information. We employ both structural proximity and attribute proximity in the loss function to preserve the respective features and the representations are obtained by minimizing the loss function. Results of extensive experiments on four real-world datasets show that the proposed method performs significantly better than baselines on a variety of tasks, which demonstrate the effectiveness and generality of our method.']}
{'论文标题':['Document-Specific Keyphrase Extraction Using Sequential Patterns with Wildcards'],'论文作者':['Xie, F', ';\xa0', 'Wu, XD', ';\xa0', 'Zhu, XQ'],'论文摘要':['Finding good keyphrases for a document is beneficial for many applications, such as text summarization, browsing, and indexing. In this paper, we propose a sequential pattern mining based document-specific keyphrase extraction method. Our key innovation is to use wildcards (or gap constraints) to help extract sequential patterns, where the flexible wildcard constraints within a pattern can capture semantic relationships between words. To achieve this goal, we regard each single document as a sequential dataset, and propose an efficient algorithm to mine sequential patterns with wildcard and one-off conditions that allows important keyphrases to be captured during the mining process. For each extracted keyphrase candidate, we use some statistical pattern features to characterize it. A supervised learning classifier is trained to identify keyphrases from a test document. Comparisons on keyphrase benchmark datasets confirm that our document-specific keyphrase extraction method is effective in improving the quality of extracted keyphrases.']}
{'论文标题':['Transfer learning with stacked reconstruction independent component analysis'],'论文作者':['Zhu, Y', ' (', 'Zhu, Yi', ') ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') ', ';\xa0', 'Zhang, YH', ' (', 'Zhang, Yuhong', ') ', ';\xa0', 'Li, PP', ' (', 'Li, Peipei', ') '],'论文摘要':['Significant improvements to transfer learning have emerged in recent years, because deep learning has been proposed to learn more higher level and robust features. However, most of existing deep learning approaches are based on the framework of auto-encoder or sparse auto-encoder, which pose challenges for independent component analysis and fail to measure similarities between data spaces. Therefore, in this paper, we propose a new strategy to achieve a better feature representation performance for transfer learning. There are several advantages in our method as follows: 1) The model of Stacked Reconstruction Independent Component Analysis (SRICA) is used to pursuit an optimal feature representation; 2) The label information is used by Logistic Regression Model to optimize representation features and the distance of distributions between domains is minimized by the method of KL-Divergence. Extensive experiments conducted on several image datasets demonstrate the superiority of our proposed method compared with all competing state-of-the-art methods. (C) 2018 Elsevier B.V. All rights reserved.']}
{'论文标题':['Towards Mining Trapezoidal Data Streams'],'论文作者':['Zhang, Q', ';\xa0', 'Zhang, P', ';\xa0', 'Wu, XD'],'论文摘要':['We study a new problem of learning from doubly-streaming data where both data volume and feature space increase over time. We refer to the problem as mining trapezoidal data streams. The problem is challenging because both data volume and feature space are increasing, to which existing online learning, online feature selection and streaming feature selection algorithms are inapplicable. We propose a new Sparse Trapezoidal Streaming Data mining algorithm (STSD) and its two variants which combine online learning and online feature selection to enable learning trapezoidal data streams with infinite training instances and features. Specifically, when new training instances carrying new features arrive, the classifier updates the existing features by following the passive-aggressive update rule used in online learning and updates the new features with the structural risk minimization principle. Feature sparsity is also introduced using the projected truncation techniques. Extensive experiments on the demonstrated UCI data sets show the performance of the proposed algorithms.']}
{'论文标题':['K-part Lasso based on feature selection algorithm for high-dimensional data'],'论文作者':['Shi Wanfeng', ';\xa0', 'Hu Xuegang', ';\xa0', 'Yu Kui'],'论文摘要':['Lasso is a feature selection method based on 1-norm. Compared with the existing feature selection methods, it not only selects the features with strong correlation with the class label, but also has good stability, so that it has received considerable attention. However, with a high-dimensional and large dataset, like other feature selection methods, Lasso encounters the problems of large computation and overfitting. To address this issue, this paper proposes an improved Lasso method, called K-part Lasso. The K-part Lasso method divides the feature set into K-parts. It selects the features from each feature subset, and then merges the selected features into one feature set. It selects the features from this merged feature set. Experimental results show that the K-part Lasso method can effectively deal with the high-dimensional and large.sample datasets.', 'Lasso是一种基于一范式的特征选择方法。与已有的特征选择方法相比较，Lasso不仅能够准确地选择出与类标签强相关的变量，同时还具有特征选择的稳定性，因而成为人们研究的一个热点。但是，Lasso方法与其他特征选择方法一样，在高维海量或高维小样本数据集的特征选择容易出现计算开销过大或过学习问题（过拟和）。为解决此问题，提出一种改进的Lasso方法：均分式Lasso方法。均分式Lasso方法将特征集均分成K份，对每份特征子集进行特征选择，将每份所选的特征进行合并，再进行一次特征选择。实验表明，均分式Lasso方法能够很好地对高维海量或高维小样本数据集进行特征选择，是一种有效的特征选择方法。']}
{'论文标题':['A Digital-Controlled Soft-Start Circuit for Negative Output DC-DC Converter'],'论文作者':['Li, YM', ';\xa0', 'Xi, XL', ';\xa0', 'Zhang, Z'],'论文摘要':['To suppress the inrush current and overshoot voltage generated at the start-up stage of Buck-Boost converter, a digital-controlled soft-start circuit based on digital-to-analog converter (DAC) control technology is proposed in this paper. The power consumption of the circuit is zero and the circuit is also keeps the characteristics of simple structure and high reliability. The circuit has been integrated into a Buck-Boost converter with negative voltage output by using the 0.18 mu m CDMOS high voltage process. The experimental results show that this circuit can effectively suppress the rush current, and the output voltage drops smoothly from 0 to the adjustment value, -4V.']}
{'论文标题':['Data mining method involves deploying data mining programs, determining actual data mining instance based on real-time load information of instances, and using data mining parameter files for data mining to determine mining information'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Imbalanced Multiple Noisy Labeling'],'论文作者':['Zhang, J', ' (', 'Zhang, Jing', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', ', ';\xa0', 'Sheng, VS', ' (', 'Sheng, Victor S.', ') '],'论文摘要':['It can be easy to collect multiple noisy labels for the same object via Internet-based crowdsourcing systems. Labelers may have bias when labeling, due to lacking expertise, dedication, and personal preference. These cause Imbalanced Multiple Noisy Labeling. In most cases, we have no information about the labeling qualities of labelers and the underlying class distributions. It is important to design agnostic solutions to utilize these noisy labels for supervised learning. We first investigate how imbalanced multiple noisy labeling affects the class distributions of training sets and the performance of classification. Then, an agnostic algorithm Positive LAbel frequency Threshold (PLAT) is proposed to deal with the imbalanced labeling issue. Simulations on eight UCI data sets with different underlying class distributions show that PLAT not only effectively deals with the imbalanced multiple noisy labeling problems that off-the-shelf agnostic methods cannot cope with, but also performs nearly the same as majority voting under the circumstances without imbalance. We also apply PLAT to eight real-world data sets with imbalanced labels collected from Amazon Mechanical Turk, and the experimental results show that PLAT is efficient and better than other ground truth inference algorithms.']}
{'论文标题':['Compressed labeling on distilled labelsets for multi-label learning'],'论文作者':['Zhou, TY', ';\xa0', 'Tao, DC', ';\xa0', 'Wu, XD'],'论文摘要':['Directly applying single-label classification methods to the multi-label learning problems substantially limits both the performance and speed due to the imbalance, dependence and high dimensionality of the given label matrix. Existing methods either ignore these three problems or reduce one with the price of aggravating another. In this paper, we propose a {0,1} label matrix compression and recovery method termed "compressed labeling (CL)" to simultaneously solve or at least reduce these three problems. CL first compresses the original label matrix to improve balance and independence by preserving the signs of its Gaussian random projections. Afterward, we directly utilize popular binary classification methods (e.g., support vector machines) for each new label. A fast recovery algorithm is developed to recover the original labels from the predicted new labels. In the recovery algorithm, a "labelset distilling method" is designed to extract distilled labelsets (DLs), i.e., the frequently appeared label subsets from the original labels via recursive clustering and subtraction. Given a distilled and an original label vector, we discover that the signs of their random projections have an explicit joint distribution that can be quickly computed from a geometric inference. Based on this observation, the original label vector is exactly determined after performing a series of Kullback-Leibler divergence based hypothesis tests on the distribution about the new labels. CL significantly improves the balance of the training samples and reduces the dependence between different labels. Moreover, it accelerates the learning process by training fewer binary classifiers for compressed labels, and makes use of label dependence via DLs based tests. Theoretically, we prove the recovery bounds of CL which verifies the effectiveness of CL for label compression and multi-label classification performance improvement brought by label correlations preserved in DLs. We show the effectiveness, efficiency and robustness of CL via 5 groups of experiments on 21 datasets from text classification, image annotation, scene classification, music categorization, genomics and web page classification.']}
{'论文标题':['Representation learning via serial robust autoencoder for domain adaptation'],'论文作者':['Yang, S', ';\xa0', 'Zhang, YH', ';\xa0', 'Hu, XG'],'论文摘要':['Domain adaptation aims to apply knowledge obtained from a labeled source domain to an unseen target domain from a different distribution. Recently, domain adaptation approaches based on autoencoder have achieved promising performances. However, almost of these approaches ignore the potential relationships of intra-domain features, which can be used to further reduce the distribution discrepancy between source and target domains. Furthermore, almost of them depend on the single autoencoder model, which brings the challenge of extracting multiple characteristics of data. To address these issues, in this paper, we propose a new representation learning method based on serial robust autoencoder for domain adaptation, named SERA. SERA first enriches intra-domain knowledge by mining the potential relationships of features in the source domain and target domain, respectively. Then, SERA learns domain invariant representations by serially connecting two new proposed autoencoder models, including marginalized denoising autoencoder via adaptation regularization (AMDA) and robust autoencoder via graph regularization (GRA). Extensive experiments on four public datasets demonstrate the effectiveness of the proposed method. (c) 2020 Published by Elsevier Ltd.']}
{'论文标题':['Block-Structured Optimization for Anomalous Pattern Detection in Interdependent Networks'],'论文作者':['Jie, F', ';\xa0', 'Wang, CP', ';\xa0', 'Wu, XD'],'论文摘要':['We propose a generalized optimization framework for detecting anomalous patterns (subgraphs that are interesting or unexpected) in interdependent networks, such as multi-layer networks, temporal networks, networks of networks, and many others. We frame the problem as a non-convex optimization that has a general nonlinear score function and a set of block-structured and non-convex constraints. We develop an effective, efficient, and parallelizable projection-based algorithm, namely Graph Block-structured Gradient Projection (GBGP), to solve the problem. It is proved that our algorithm 1) runs in nearly-linear time on the network size, and 2) enjoys a theoretical approximation guarantee. Moreover, we demonstrate how our framework can be applied to two very practical applications, and we conduct comprehensive experiments to show the effectiveness and efficiency of our proposed algorithm.']}
{'论文标题':['Knowledge Graph enhanced Neural Collaborative Filtering with Residual Recurrent Network'],'论文作者':['Sang, L', ';\xa0', 'Xu, M', ';\xa0', 'Wu, XD'],'论文摘要':["Knowledge Graph (KG), which commonly consists of fruitful connected facts about items, presents an unprecedented opportunity to alleviate the sparsity problem in recommender system. However, existing KG based recommendation methods mainly rely on handcrafted meta-path features or simple triple-level entity embedding, which cannot automatically capture entities' long-term relational dependencies for the recommendation. Specially, entity embedding learning is not properly designed to combine user item interaction information with KG context information. In this paper, a two-channel neural interaction method named Knowledge Graph enhanced Neural Collaborative Filtering with Residual Recurrent Network (KGNCF-RRN) is proposed, which leverages both long-term relational dependencies KG context and user-item interaction for recommendation. (1) For the KG context interaction channel, we propose Residual Recurrent Network (RRN) to construct context-based path embedding, which incorporates residual learning into traditional recurrent neural networks (RNNs) to efficiently encode the long-term relational dependencies of KG. The self-attention network is then applied to the path embedding to capture the polysemy of various user interaction behaviours. (2) For the user-item interaction channel, the user and item embeddings are fed into a newly designed two-dimensional interaction map. (3) Finally, above the two-channel neural interaction matrix, we employ a convolutional neural network to learn complex correlations between user and item. Extensive experimental results on three benchmark data sets show that our proposed approach outperforms existing state-of-the-art approaches for knowledge graph based recommendation. CO 2021 Published by Elsevier B.V."]}
{'论文标题':['Parallel PSO combined with K-means clustering algorithm based on MPI'],'论文作者':['Lu Yi-qing', ';\xa0', 'Lin Jin-xian'],'论文摘要':['The performance of traditional serial clustering algorithm cannot meet the needs of data clustering of the huge amounts of data. To enhance the performance of clustering algorithm, a new clustering algorithm combining parallel Particle Swarm Optimization (PSO) with K-means based on MPI was proposed in this paper. Firstly, the improved PSO was combined with K-means to enhance the capacity of global search, and then a new parallel clustering algorithm was proposed, which was compared with K-means and PSO clustering algorithms. The experimental results show that the new algorithm has better global convergence, and also has higher speed-up ratio.', '传统的串行聚类算法在对海量数据进行聚类时性能往往不尽如人意,为了适应海量数据聚类分析的性能要求,针对传统聚类算法的不足,提出一种基于消息传递接口(MPI)集群的并行PSO混合K均值聚类算法.首先将改进的粒子群与K均值结合,提高该算法的全局搜索能力,然后利用该算法提出一种新的并行聚类策略,并将该算法与K均值聚类算法,粒子群优化(PSO)聚类算法进行比较.实验结果表明,该算法不仅具有较好的全局收敛性,而且具有较高的加速比']}
{'论文标题':['Constructing importance measure of attributes in covering decision table'],'论文作者':['Li, FC', ';\xa0', 'Zhang, Z', ';\xa0', 'Jin, CX'],'论文摘要':["In rough set theory, attributes importance measure is a crucial factor in applications of attribute reduction and feature selection. Many importance measure methodologies for discrete-valued information system or decision table have been developed. However, there are only limited studies on importance measurement for numerical-valued information system. In this paper, knowledge change-based importance measure, with the structural characteristics of fuzzy measure, is introduced to evaluate the importance of attributes in covering decision table. We first present the concept of similarity block in attribute space, based on which coverings are induced to construct the lower and upper approximation operators in covering-based rough sets. In particular, the traditional importance measure is extended to deal with covering decision table. Further, an evaluation model based on the knowledge change-based importance measure is constructed. Experiments are conducted on the public data sets from UCI, and a case study on the students' overall evaluation is given finally. Theoretical analysis and experimental results show that the proposed importance measure is effective for evaluating the importance of attributes in covering decision table. (C) 2014 Elsevier B.V. All rights reserved."]}
{'论文标题':['Optimal pricing for new product entry under free strategy'],'论文作者':['Nan, GF', ';\xa0', 'Li, XT', ';\xa0', 'Li, MQ'],'论文摘要':["A growing number of firms in software industry are embracing free entry strategy to promote product adoption. The prevalence of free strategy can be partly attributed to the positive network externalities exhibited by the information goods. In this paper, we model a new firm's entry into an existing market with the free strategy. Consumers can use the new product's basic functionality for free and pay a subscription fee for accessing the add-ons. The entrant firm's new product infringes on the market in one of the three ways: homogeneous product competition, high-end encroachment and low-end encroachment. We find that the equilibrium market structure varies across the three settings. In particular, there exists a Bertrand equilibrium when the new firm provides a homogeneous product. When the new firm offers a heterogeneous product, our results show that the network externalities intensify the price competition and thus lead to a reduction in the profits. Moreover, whether the new firm should encroach on the existing market with high-end product or low-end product depends on the level of switching cost. If the switching cost is low, the new firm will benefit more from high-end encroachment and vice versa. We also find that it is not always optimal for the new firm to adopt the free entry strategy. In the high-end encroachment, the new firm will be better off providing a product for free if the network intensity is high enough, whereas in the low-end encroachment, the free strategy is dominant only when the network intensity falls within a given threshold."]}
{'论文标题':['Suffix array for multi-pattern matching with variable length wildcards'],'论文作者':['Liu, N', ';\xa0', 'Xie, F', ';\xa0', 'Wu, XD'],'论文摘要':['Approximate multi-pattern matching is an important issue that is widely and frequently utilized, when the pattern contains variable-length wildcards. In this paper, two suffix array-based algorithms have been proposed to solve this problem. Suffix array is an efficient data structure for exact string matching in existing studies, as well as for approximate pattern matching and multi-pattern matching. An algorithm called MMSA-S is for the short exact characters in a pattern by dynamic programming, while another algorithm called MMSA-L deals with the long exact characters by the edit distance method. Experimental results of Pizza & Chili corpus demonstrate that these two newly proposed algorithms, in most cases, are more time-efficient than the state-of-the-art comparison algorithms.']}
{'论文标题':['Method for generating and adjusting knowledge learning plan based on DIKWP system, involves generating optimal learning plan for target object based on DIKWP model of target object and necessary knowledge content grasping progress'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Pattern matching with wildcards and length constraints using maximum network flow'],'论文作者':['Arslan, AN', ';\xa0', 'He, D', ';\xa0', 'Wu, XD'],'论文摘要':['In 2006 Chen et al. introduced an interesting text pattern matching problem with unique features. A pattern is described by a sequence of letters (literals) separated by a number of wildcards. For each pair of consecutive literals in the pattern description, a local length constraint specifies the minimum and maximum number of wildcards. Also, the pattern must occur in the text within a window for which the minimum and maximum lengths are specified by the global length constraint. Text letters are exclusively assigned to occurrences (the one-off condition). The objective is to find the maximum number of occurrences of the pattern in the text with all the constraints. For the problem (in this paper we call it Problem 1), there is a polynomial time online algorithm which does not guarantee an optimal solution. There are also polynomial time heuristic algorithms for the problem, and for some of its restricted cases. It is not known if Problem 1 is NP-hard. In this work, for two special cases of Problem 1, we give reductions to the maximum flow problem which can be solved in polynomial time. (C) 2015 Elsevier B.V. All rights reserved.']}
{'论文标题':['Collaborative social deep learning for celebrity recommendation'],'论文作者':['Liu, HT', ';\xa0', 'Yang, LQ', ';\xa0', 'Wu, XD'],'论文摘要':["Recently how to recommend celebrities to public has become an interesting problem in real social network applications. In this problem, a matrix of users' following actions is usually very sparse. Therefore, it causes conventional collaborative filtering based methods to degrade significantly in recommendation performance. To address the sparsity problem, side information could be rendered. Collaborative social topic regression (CSTR) is an appealing new method, which combines the matrix of general users' following actions, content information and a social network of celebrities. However, this method is limited by using the topic model latent Dirichlet allocation (LDA) as the critical component. The learned content representation may not be compact and effective enough. Moreover, the social network of general users also exists, which is helpful for recommendation. In this paper, we employ a deep learning component to learn more effective feature representation and incorporate social network information of general users by adding social regularization terms. We propose a novel hierarchical Bayesian model named collaborative social deep learning (CSDL), which jointly handles deep learning for the content information and collaborative filtering for general users' following actions, the social network of celebrities and that of general users. Experiments on two real-world datasets show the effectiveness of our proposed model."]}
{'论文标题':['Markov Blanket Feature Selection Using Representative Sets'],'论文作者':['Yu, K', ';\xa0', 'Wu, XD', ';\xa0', 'Wang, H'],'论文摘要':['It has received much attention in recent years to use Markov blankets in a Bayesian network for feature selection. The Markov blanket of a class attribute in a Bayesian network is a unique yet minimal feature subset for optimal feature selection if the probability distribution of a data set can be faithfully represented by this Bayesian network. However, if a data set violates the faithful condition, Markov blankets of a class attribute may not be unique. To tackle this issue, in this paper, we propose a new concept of representative sets and then design the selection via group alpha-investing (SGAI) algorithm to perform Markov blanket feature selection with representative sets for classification. Using a comprehensive set of real data, our empirical studies have demonstrated that SGAI outperforms the state-of-the-art Markov blanket feature selectors and other well-established feature selection methods.']}
{'论文标题':['Multicellular gene expression programming algorithm for function optimization'],'论文作者':['Peng Yu-zhong', ';\xa0', 'Yuan Chang-an', ';\xa0', 'Chen Jian-wei', ';\xa0', 'Wu Xin-dong', ';\xa0', 'Wang Ru-liang'],'论文摘要':['In dealing with complex function optimization problems, many existing evolutionary algorithms have performance limitations such as inability of convergence, poor searching efficiency and low precision. To cope with this problem, we adopt the idea of homeotic genes and cellular system, and propose a new algorithm based on multicell genetic expression programming. In addition, a new relevant individual coding method and new schemes of population generating and genetic operation are designed. Compared with other algorithms on eight Benchmark functions testing, the proposed algorithm shows higher precision, improved convergence ability and global search ability.']}
{'论文标题':['Recommending New Features from Mobile App Descriptions'],'论文作者':['Jiang, H', ' (', 'Jiang, He', ') ', ', ', ';\xa0', 'Zhang, JX', ' (', 'Zhang, Jingxuan', ') ', ';\xa0', 'Li, XC', ' (', 'Li, Xiaochen', ') ', ', ', ';\xa0', 'Ren, ZL', ' (', 'Ren, Zhilei', ') ', ', ', ';\xa0', 'Lo, D', ' (', 'Lo, David', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', ', ';\xa0', 'Luo, ZX', ' (', 'Luo, Zhongxuan', ') ', ', '],'论文摘要':['The rapidly evolving mobile applications (apps) have brought great demand for developers to identify new features by inspecting the descriptions of similar apps and acquire missing features for their apps. Unfortunately, due to the huge number of apps, this manual process is time-consuming and unscalable. To help developers identify new features, we propose a new approach named SAFER. In this study, we first develop a tool to automatically extract features from app descriptions. Then, given an app, we leverage the topic model to identify its similar apps based on the extracted features and API names of apps. Finally, we design a feature recommendation algorithm to aggregate and recommend the features of identified similar apps to the specified app. Evaluated over a collection of 533 annotated features from 100 apps, SAFER achieves a Hit@15 score of up to 78.68% and outperforms the baseline approach KNN+ by 17.23% on average. In addition, we also compare SAFER against a typical technique of recommending features from user reviews, i.e., CLAP. Experimental results reveal that SAFER is superior to CLAP by 23.54% in terms of Hit@15.']}
{'论文标题':['Heterogeneous-Length Text Topic Modeling for Reader-Aware Multi-Document Summarization'],'论文作者':['Qiang, JP', ';\xa0', 'Chen, P', ';\xa0', 'Wu, XD'],'论文摘要':['More and more user comments like Tweets are available, which often contain user concerns. In order to meet the demands of users, a good summary generating from multiple documents should consider reader interests as reflected in reader comments. In this article, we focus on how to generate a summary from multi-document documents by considering reader comments, named as reader-aware multi-document summarization (RA-MDS). We present an innovative topic-based method for RA-MDA, which exploits latent topics to obtain the most salient and lessen redundancy summary from multiple documents. Since finding latent topics for RA-MDS is a crucial step. we also present a Heterogeneous-length Text Topic Modeling (HTTM) to extract topics from the corpus that includes both news reports and user comments, denoted as heterogeneous-length texts. In this case, the latent topics extract by HTTM cover not only important aspects of the event, but also aspects that attract reader interests. Comparisons on summary benchmark datasets also confirm that the proposed RA-MDS method is effective in improving the quality of extracted summaries. In addition, experimental results demonstrate that the proposed topic modeling method outperforms existing topic modeling algorithms.']}
{'论文标题':['Collective behavior learning by differentiating personal preference from peer influence'],'论文作者':['Zhang, Z', ';\xa0', 'Liu, L', ';\xa0', 'Meierer, M'],'论文摘要':["Networked data, generated by social media, presents opportunities and challenges to the study of collective behaviors in a social networking environment. In this paper, we focus on multi-label classification on networked data, for which behaviors are represented as labels and an individual can have multiple labels. Existing relational learning methods exploit the connectivity of individuals and they have shown better performance than traditional multi-label classification methods. However, an individual's behavior may be influenced by other factors, particularly personal preference. Hence, we propose a novel approach that integrates causal analysis into multi label classification to learn collective behaviors. We employ propensity score matching and causal effect estimation to distinguish the contributions of peer influence and personal preference to collective behaviors and incorporate the findings into the design of the classifier. We further study behavior heterogeneity across subgroups in social networks, as people with different demographic features may behave differently due to different impacts of peer influence and personal preference. We estimate conditional average causal effects to analyze the impacts of peer influence and personal preference in different subgroups in social networks. Experiments on real world datasets demonstrate that our proposed methods improve classification performance over existing methods."]}
{'论文标题':['An Online Transfer Learning Algorithm with Adaptive Cost'],'论文作者':['Zhang, YH', ';\xa0', 'Wu, MM', ';\xa0', 'Zhu, Y'],'论文摘要':['Online transfer learning aims to attack an online learning task on a target domain by transferring knowledge from some source domains, which has received more attentions. And most online transfer learning methods adapt the classifier according to its accuracy on new coming data. However, in real-world applications, such as anomaly detection and credit card fraud detection, the cost may be more important than the accuracy. Moreover, the cost usually changes in these online data, which challenges state-of-art-methods. Therefore, this paper introduces the cost of misclassification into transfer-learning of classifier, and proposes a novel online transfer learning algorithm with adaptive cost (OLAC). Firstly, we introduce the label distribution into traditional Hinge Loss Function to compute the cost of classification adaptively. Secondly, we transfer learn the classifier according to its performance on new coming data including both accuracy and cost. Extensive experimental results show that our method can achieve higher accuracy and less classification lost, especially for the samples with higher costs.']}
{'论文标题':['Efficient string matching with wildcards and length constraints'],'论文作者':['Chen, G', ';\xa0', 'Wu, XD', ';\xa0', 'He, Y'],'论文摘要':["This paper defines a challenging problem of pattern matching between a pattern P and a text T, with wildcards and length constraints, and designs an efficient algorithm to return each pattern occurrence in an online manner. In this pattern matching problem, the user can specify the constraints on the number of wildcards, between each two consecutive letters of P and the constraints on the length of each matching substring in T. We design a complete algorithm, SAIL(1) that returns each matching substring of P in T as soon as it appears in T in an O(n + klmg) time with an O(lm) space overhead, where n is the length of T, k is the frequency of P's last letter occurring in T, l is the user-specified maximum length for each matching substring, m is the length of P, and g is the maximum difference between the user-specified maximum and minimum numbers of wildcards allowed between two consecutive letters in P."]}
{'论文标题':['Causal Query in Observational Data with Hidden Variables'],'论文作者':['Cheng, DB', ';\xa0', 'Li, JY', ';\xa0', 'Le, TD'],'论文摘要':['This paper discusses the problem of causal query in observational data with hidden variables, with the aim of seeking the change of an outcome when "manipulating" a variable while given a set of plausible confounding variables which affect the manipulated variable and the outcome. Such an "experiment on data" to estimate the causal effect of the manipulated variable is useful for validating an experiment design using historical data or for exploring confounders when studying a new relationship. However, existing data-driven methods for causal effect estimation face some major challenges, including poor scalability with high dimensional data, low estimation accuracy due to heuristics used by the global causal structure learning algorithms, and the assumption of causal sufficiency when hidden variables are inevitable in data. In this paper, we develop theorems for using local search to find a superset of the adjustment (or confounding) variables for causal effect estimation from observational data under a realistic pretreatment assumption. The theorems ensure that the unbiased estimate of causal effect is included in the set of causal effects estimated by the superset of adjustment variables. Based on the developed theorems, we propose a data-driven algorithm for causal query. Experiments show that the proposed algorithm is faster and produces better causal effect estimation than an existing data-driven causal effect estimation method with hidden variables. The causal effects estimated by the proposed algorithm are as accurate as those by the state-of-the-art methods using domain knowledge.']}
{'论文标题':['Smoothed l0 norm constrained nonnegative matrix factorization on orthogonal subspace'],'论文作者':['Ye Jun', ';\xa0', 'Jin Zhong'],'论文摘要':['In order to improve the ability of the sparse representations of the NMF, this paper proposed the new algorithm for nonnegative matrix factorization, denoted smoothed l', ' norm constrained nonnegative matrix factorization on orthogonal subspace, in which the generation of orthogonal factor matrices with smoothed l', ' norm constrained were the parts of objective function minimization. Also it developed simple multiplicative updates for the proposed method. Experiments on three real-world databases (Iris, UCI, ORL) show that the proposed method can achieve the best or close to the best in clustering and in the way of the sparse representation than other methods.', '为了能够提升分解矩阵的稀疏表达能力，提出了一种新的基于平滑l_0范数的正交子空间非负矩阵分解方法。通过将分解矩阵的正交性及平滑l_0范数约束同时引入矩阵分解的目标函数中一起进行优化，大大降低了计算复杂度，并提升了分解矩阵的稀疏表达能力。同时给出了分解矩阵的乘积更新迭代规则。通过在三个真实数据库(Iris,UCI,ORL)上的实验表明，该方法在分解所得矩阵的稀疏表示方面及将其应用于聚类问题所取得的聚类效果方面优于其他方法。']}
{'论文标题':['A coarse-to-fine collective entity linking method for heterogeneous information networks'],'论文作者':['Li, J', ';\xa0', 'Bu, CY', ';\xa0', 'Wu, XD'],'论文摘要':['Linking ambiguous entity mentions in a text with their true mapping entities in a heterogeneous information network (HIN) is important. Most of existing entity linking methods with HINs assume that the entities in a text are independent while ignoring the relationships between the entities in context. Recent studies have shown that collective entity linking methods are more effective than traditional independent entity linking methods because they consider the relationships between different entities in the same text. However, few studies focus on collective entity linking for HINs. Most of collective entity linking methods rely largely on special features in Wikipedia, and may not be suitable for the HINs that are not mapped to Wikipedia. Moreover, existing collective entity linking methods may have high time complexity. Therefore, a Coarse-to-Fine collective Entity Linking algorithm (called CFEL) is proposed for the case the Wikipedia cannot be used. CFEL is composed of a coarse-grained model and a fine-grained model. In the coarse-grained model, a pruning strategy motivated by the human cognition mechanism, is adopted to reduce the number of candidates for each entity mention in texts. The candidates in HINs that are inconsistent with the type of entity mentions can be deleted. In the fine-grained model, we present a probabilistic method that combines the semantic information in a text with the structural information in HINs. The experimental results on four real-world datasets verify the effectiveness of our algorithm compared to the baselines. (C) 2021 Elsevier B.V. All rights reserved.']}
{'论文标题':['NetDPO: (delta, gamma)-approximate pattern matching with gap constraints under one-off condition'],'论文作者':['Li, Y', ';\xa0', 'Yu, L', ';\xa0', 'Wu, XD'],'论文摘要':['Approximate pattern matching not only is more general than exact pattern matching, but also allows some data noise. Most of them adopt the Hamming distance to measure similarity, which indicates the number of different characters in two sequences, but it cannot reflect the approximation between two characters. This paper addresses the approximate pattern matching with a local distance no larger than delta and a global distance no larger than gamma, which is named Delta and gamma Pattern matching with gap constraints under One-off condition (DPO). First, we show that the problem is an NP-Hard problem. Therefore, we construct a heuristic algorithm named approximate Nettree for DPO (NetDPO), which transforms the problem into an approximate Nettree based on delta distance which is a specially designed data structure. Then, NetDPO calculates the number of paths that reach the roots within gamma distance. To find the maximal occurrences, we employ the rightmost parent strategy and the optimal parent strategy to select the better occurrence which can minimize the influence after removing the occurrence. Iterate this process until there are no occurrences. Finally, we analyze the time and space complexities of NetDPO. Extensive experimental results verify the superiority of the proposed algorithm.']}
{'论文标题':['Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data'],'论文作者':['Li, PP', ';\xa0', 'Wu, M', ';\xa0', 'Hu, XG'],'论文摘要':['Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.']}
{'论文标题':['A parallel algorithm for learning Bayesian networks'],'论文作者':['Yu, K', ';\xa0', 'Wang, H', ';\xa0', 'Wu, XD'],'论文摘要':['Computing the expected statistics is the main bottleneck in learning Bayesian networks in large-scale problem domains. This paper presents a parallel learning algorithm, PL-SEM, for learning Bayesian networks, based on an existing structural EM algorithm (SEM). Since the computation of the expected statistics is in the parametric learning part of the SEM algorithm, PL-SEM exploits a parallel EM algorithm to compute the expected statistics. The parallel EM algorithm parallelizes the E-step and M-step. At the E-step, PL-SEM parallel computes the expected statistics of each sample; and at the M-step, with the conditional independence of Bayesian networks and the expected statistics computed at the E-step, PL-SEM exploits the decomposition property of the likelihood function under the completed data to parallel estimate each local likelihood function. PL-SEM effectively computes the expected statistics, and greatly reduces the time complexity of learning Bayesian networks.']}
{'论文标题':['A Multiple Feature Integration Model to Infer Occupation from Social Media Records'],'论文作者':['Wang, X', ';\xa0', 'Yu, LL', ';\xa0', 'Cui, B'],'论文摘要':["With the rapid development of more and more social media applications, lots of users are connected with friends and their daily life and opinions are recorded. Social media provides us an unprecedented way to collect and analyze billions of users' information. Proper user attribute identification or profile inference becomes more and more attractive and feasible. However, the flourishing social records also pose great challenge in effective feature selection and integration for user profile inference. This is mainly caused by the text sparsity and complex community structures.", "In this paper, we propose a comprehensive framework to infer user's occupation from his/her social activities recorded in micro-blog message streams. A multi-source integrated classification model is set up with some fine selected features. We first identify some beneficial basic content features, and then we proceed to tailor a community discovery based latent dimension solution to extract community features.", "Extensive empirical studies are conducted on a large real micro-blog dataset. Not only we demonstrate the integrated model shows advantages over several baseline methods, but also we verify the effect of homophily in users' interaction records. The different effects of heterogeneous interactive networks are also revealed."]}
{'论文标题':['Mining frequent patterns with gaps and one-off condition'],'论文作者':['Yongming Huang', ';\xa0', 'Xindong Wu', ';\xa0', 'Gongqing Wu'],'论文摘要':['Mining frequent patterns with a gap requirement from sequences is an important step in many domains, such as biological sciences. Given a character sequence S of length L, a certain threshold and a gap constraint, we aim to discover frequent patterns whose supports in S are no less than the given threshold value. A frequent pattern P can have wildcards, and the numbers of the wildcards between elements of P must fulfill user-specified gap constraints. Also, this mining process satisfies the one-off condition and an apriori-like property to be efficient. Experiments show that our method can mine as many frequent patterns with wildcards as the existing MPP algorithm, but has a much better performance in time.']}
{'论文标题':['Multi-label Online Streaming Feature Selection Based on Spectral Granulation and Mutual Information'],'论文作者':['Wang, HM', ' (', 'Wang, Huaming', ') ', ';\xa0', 'Yu, DM', ' (', 'Yu, Dongming', ') ', ';\xa0', 'Li, Y', ' (', 'Li, Yuan', ') ', ';\xa0', 'Li, ZX', ' (', 'Li, Zhixing', ') ', ';\xa0', 'Wang, GY', ' (', 'Wang, Guoyin', ') '],'论文摘要':['Instances in multi-label data sets are generally described as a high-dimensional feature vector, as brings the "curse of dimensionality" problem. To ease this problem, some multi-label feature selection algorithms have been proposed. However, they all handle feature selection problems with the assumption that all candidate features are available beforehand. While in some real applications, feature selection must be conducted in the online manner with dynamic features, for example, novel topics arise constantly with a set of features in social networks. Online streaming feature selection (OSFS), dealing with dynamic features, has attracted intensive interest in recent years. Some online feature selection methods are designed for single-label applications, They can not be directly applied in multi-label scenarios. In this paper, we propose a multi-label online streaming feature selection algorithm based on spectral granulation and mutual information (ML-OSMI), which takes high-order label correlations into consideration. Moreover, comprehensive experiments are conducted to verify the effectiveness of the proposed algorithm on twelve multi-label high-dimensional benchmark data sets.']}
{'论文标题':['Cost-constrained data acquisition for intelligent data preparation'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['Real-world data is noisy and can often suffer from corruptions or incomplete values that may impact the models created from the data. To build accurate predictive models, data acquisition is usually adopted to prepare the data and complete missing values. However, due to the significant cost of doing so and the inherent correlations in the data set, acquiring correct information for all instances is prohibitive and unnecessary. An interesting and important problem that arises here is to select what kinds of instances to complete so the model built from the processed data can receive the "maximum" performance improvement. This problem is complicated by the reality that the costs associated with the attributes are different, and fixing the missing values of some attributes is inherently more expensive than others. Therefore, the problem becomes that given a fixed budget, what kinds of instances should be selected for preparation, so that the learner built from the processed data set can maximize its performance? In this paper, we propose a solution for this problem, and the essential idea is to combine attribute costs and the relevance of each attribute to the target concept, so that the data acquisition can pay more attention to those attributes that are cheap in price but informative for classification. To this end, we will first introduce a unique Economical Factor (EF) that seamlessly integrates the cost and the importance ( in terms of classification) of each attribute. Then, we will propose a cost-constrained data acquisition model, where active learning, missing value prediction, and impact-sensitive instance ranking are combined for effective data acquisition. Experimental results and comparative studies from real-world data sets demonstrate the effectiveness of our method.']}
{'论文标题':['Computerized-method for real-time detection of real concept drift in predictive machine learning models, involves computing posterior distribution of reliable labels of portion of instances, and operating drift-detection module'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Strict approximate pattern matching with general gaps'],'论文作者':['Wu, YX', ';\xa0', 'Fu, S', ';\xa0', 'Wu, XD'],'论文摘要':['Pattern matching with gap constraints is one of the essential problems in computer science such as music information retrieval and sequential pattern mining. One of the cases is called loose matching, which only considers the matching position of the last pattern substring in the sequence. One more challenging problem is considering the matching positions of each character in the sequence, called strict pattern matching which is one of the essential tasks of sequential pattern mining with gap constraints. Some strict pattern matching algorithms were designed to handle pattern mining tasks, since strict pattern matching can be used to compute the frequency of some patterns occurring in the given sequence and then the frequent patterns can be derived. In this article, we address a more general strict approximate pattern matching with Hamming distance, named SAP (Strict Approximate Pattern matching with general gaps and length constraints), which means that the gap constraints can be negative. We show that a SAP instance can be transformed into an exponential amount of the exact pattern matching with general gaps instances. Hence, we propose an effective online algorithm, named SETA (SubnETtree for sAp), based on the subnettree structure (a Nettree is an extension of a tree with multi-parents and multi-roots) and show the completeness of the algorithm. The space and time complexities of the algorithm are O(m x Maxlen x W x d) and O(Maxlen x W x m (2) x n x d), respectively, where m, Maxlen, W, and d are the length of pattern P, the maximal length constraint, the maximal gap length of pattern P and the approximate threshold. Extensive experimental results validate the correctness and effectiveness of SETA.']}
{'论文标题':['Vertical union algorithm of concept lattices based on synonymous concept'],'论文作者':['Zhang Lei', ';\xa0', 'Shen Xia-jiong', ';\xa0', 'An Guang-wei'],'论文摘要':['One of the research emphases in the formal concept analysis field is how to improve the efficiency of constructing a concept lattice from a formal context. An efficient solution to this problem is to construct a parallel algorithm after dividing the formal context. A linear index structure of the concept lattice is introduced in the lattice union algorithm, which is to update each father node according to the general-special relation between the concepts by finding out the synonymous concepts of the common universe concept lattices. Comparing to the common universe concept lattice vertical union algorithms proposed in other papers, this algorithm makes great improvement in time complexity.', '在FCA中，如何提高概念格构造算法的效率一直是研究的重点之一。将形式背景进行分解后造格是解决这一问题的有效途径，其中概念格的合并算法是关键。提出同艾概念的观点。在格的合并算法中引入了概念格的线性索引结构。通过寻找同域概念格之间的同艾概念，根据父概念-子概念的关系实现对其所有父节点的快速更新。实验表明，谊算法和文献中的同域概念格纵向合并算法相比，其时间性能有明显改善。']}
{'论文标题':['Data acquisition with active and impact-sensitive instance selection'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['Real-world data is never perfect and can often suffer from corruptions or missing values that may impact models created from the data. To build accurate predictive models, data acquisition is usually adopted to complete missing values in the incomplete instances. Due to the significant cost of doing so and the inherent correlations in the dataset, acquiring complete information for all instances is likely prohibitive and unnecessary. An interesting and important problem raises here is to select what kind of instances to complete so the model built from the data can receive significant improvement. In this paper, we propose two solutions to resolve this problem, and the essential idea is to complete the attributes with higher impacts to the system performance. The first solution is based on an impact-sensitive instance ranking mechanism [1]. We explore the correlation between attributes and the class and use the correlation as weights of the attributes; the larger the weight, the higher the impacts of the attribute. For each incomplete instance, we sum all weights of the attributes with missing values, and the instance with larger sum appears to be more important for users to complete their missing information. In the second solution, active teaming, impact-sensitive instance ranking and missing value prediction are combined for data acquisition. Experimental results from real-world datasets demonstrate the effectiveness of our strategies.']}
{'论文标题':['Online Feature Selection with Capricious Streaming Features: A General Framework'],'论文作者':['Wu, D', ';\xa0', 'He, Y', ';\xa0', 'Wu, XD'],'论文摘要':['Online streaming feature selection has received extensive attention in the past few years. Existing approaches have a common assumption that the feature space of the fixed data instances increases dynamically without any missing entry. This assumption, however, does not always hold in many real-world applications. For example, in a credit evaluation system, we cannot collect the complete dynamic features for each person and/or enterprise. Motivated by this observation, this paper aims at conducting online feature selection from capricious streaming features, where features flow in one by one with some random missing entries while the number of data instances remains fixed. To do so, we propose a general framework named GF-CSF. The main idea of GF-CSF is to adopt latent factor analysis to preprocess capricious streaming features for completing their missing entries before conducting feature selection. Both theoretical and experimental analyses indicate that GF-CSF can efficiently improve any existing model of online streaming features selection to achieve online capricious streaming features selection.']}
{'论文标题':['A Speech-to-Knowledge-Graph Construction System'],'论文作者':['Fu, XY', ';\xa0', 'Zhang, J', ';\xa0', 'Wu, XD'],'论文摘要':["This paper presents a HAO-Graph system that generates and visualizes knowledge graphs from a speech in real-time. When a user speaks to the system, HAO-Graph transforms the voice into knowledge graphs with key phrases from the original speech as nodes and edges. Different from language-to-language systems, such as Chinese-to-English and English-to-English, HAO-Graph converts a speech into graphs, and is the first of its kind. The effectiveness of our HAO-Graph system is verified by a two-hour chairman's talk in front of two thousand participants at an annual meeting in the form of a satisfaction survey."]}
{'论文标题':['Document Specific Supervised Keyphrase Extraction With Strong Semantic Relations'],'论文作者':['Liu, HT', ';\xa0', 'Wang, LL', ';\xa0', 'Wu, XD'],'论文摘要':['Keyphrase extraction is the task of automatically extracting descriptive phrases or concepts that represent the main topics in a document. Finding good keyphrases in a document can quickly summarize knowledge for information retrieval and decision making. Existing keyphrase extraction methods cannot be customized to each specific document, and cannot capture flexible semantic relations. In this paper, a keyphrase extraction algorithm using maximum sequential pattern mining with one-off and general gaps condition, called Ke-MSMING, is presented. Ke_MSMING first searches all keyphrase candidates from a document using sequential patterns mining and the topic model, and then adopts supervised machine learning to classify each keyphrase candidate as a keyphrase or not. Finally, Ke_MSMING selects top-N keyphrases as the final keyphrases. Ke_MSMING not only uses baseline features and pattern features but also uses centrality features obtained from the cooccurrence semantic network, and the cooccurrence networks can yield powerful semantic relations for keyphrase extraction. Experimental results on two datasets demonstrate that Ke_MSMING has better performance than other state-of-the-art keyphrase extraction approaches.']}
{'论文标题':['Improving Gradient-based DAG Learning by Structural Asymmetry'],'论文作者':['Yujie Wang', ';\xa0', 'Shuai Yang', ';\xa0', 'Kui Yu'],'论文摘要':['Directed acyclic graph (DAG) learning plays a fun-damental role in causal inference and other scientific scenes, which aims to uncover the relationships between variables. However, identifying a DAG from observational data has al-ways been a challenging task. Recently, gradient-based DAG learning algorithms that convert a combination-optimization DAG learning problem into a continuous-optimization problem have achieved emerging successes. These algorithms are easy to optimize and able to deal with both parametric and non-parametric data but suffer from many reversed edges learnt by these algorithms. In this paper, we propose a framework named Residual Independence Test (RIT) to correct those reversed edges by leveraging the structural asymmetry reflected in the depen-dence between regression residual and direct cause. We conduct extensive experiments on both synthetic and benchmark datasets, the results show that the RIT framework significantly improve the performance of gradient-based DAG learning algorithms.']}
{'论文标题':['Local semi-linear regression for river runoff forecasting'],'论文作者':['Fan Min', ';\xa0', 'Xindong Wu'],'论文摘要':['Time series data such as river runoff and stock prices are important for the human society. Many time series data exhibit a linear property. That is, a sequence of neighboring data points are approximately on one straight line. Consequently, linear regression techniques are developed for the purpose of forecasting, and linear segmentation techniques are developed for the purposes of data compression, exact sequence matching and forecasting. In this paper, we point out that for some real world data the linear property is strong, however quite local, and linear segmentation techniques produce rather short segments. Therefore we consider an extremely simple regression, called local linear regression, which only takes into account the most recent time series change. While applied on the daily runoff data of Mississippi river, this approach performs better than linear regression, higher order polynomial regressions, and even M5 model trees. Also, it gives quite satisfactory results in flood seasons. To further improve the forecasting accuracy, we propose a local semi-linear regression (LSLR) approach by introducing a factor alpha to the runoff changing speed. Experimental results show that our approach can achieve an accuracy of 86.68% while the error tolerance is only +or-1.5% in flood seasons.']}
{'论文标题':['An efficient framework for multi-label learning in non-stationary data stream'],'论文作者':['Xiulin Zheng', ';\xa0', 'Peipei Li'],'论文摘要':['Mining multi-label data stream is a huge challenge due to its properties of multiple labels and dynamic streaming characteristics, where each label may experience different con-cept drifts simultaneously or distinctly and class imbalance, etc. Existing works either concentrate on only one type of drift for all labels or ignore the dynamic class imbalance, which usually re-sults in a great degeneration in classification performance. To this end, we propose an efficient ensemble based on the Self-Adjusting Memory (SAM) and Enhanced Punishment mechanism, named as SAMEP, where the Self Adjusting Memory is used to adapt to the heterogeneous concept drift for different labels in multi-label data and the Enhanced Punishment mechanism is utilised to deal with the mining difficulties brought by multiple labels, such as class imbalance, etc. Extensive experiments conducted on multi-label datasets for six common metrics demonstrate the effectiveness of our proposed algorithm as compared to 7 latest well-known multi-label data stream classification methods demonstrated the effectiveness of our proposed method. In addition, non-parametric statistical (the Friedman test with Nemenyi post-hoc) analysis validate the effectiveness of our proposed framework.']}
{'论文标题':['Active Learning with Imbalanced Multiple Noisy Labeling'],'论文作者':['Zhang, J', ';\xa0', 'Wu, XD', ';\xa0', 'Sheng, VS'],'论文摘要':['With crowdsourcing systems, it is easy to collect multiple noisy labels for the same object for supervised learning. This dynamic annotation procedure fits the active learning perspective and accompanies the imbalanced multiple noisy labeling problem. This paper proposes a novel active learning framework with multiple imperfect annotators involved in crowdsourcing systems. The framework contains two core procedures: label integration and instance selection. In the label integration procedure, a positive label threshold (PLAT) algorithm is introduced to induce the class membership from the multiple noisy label set of each instance in a training set. PLAT solves the imbalanced labeling problem by dynamically adjusting the threshold for determining the class membership of an example. Furthermore, three novel instance selection strategies are proposed to adapt PLAT for improving the learning performance. These strategies are respectively based on the uncertainty derived from the multiple labels, the uncertainty derived from the learned model, and the combination method (CFI). Experimental results on 12 datasets with different underlying class distributions demonstrate that the three novel instance selection strategies significantly improve the learning performance, and CFI has the best performance when labeling behaviors exhibit different levels of imbalance in crowdsourcing systems. We also apply our methods to a real-world scenario, obtaining noisy labels from Amazon Mechanical Turk, and show that our proposed strategies achieve very high performance.']}
{'论文标题':['Ubiquitous Personalized Information Processing with Wildcards'],'论文作者':['Wu, XD'],'论文摘要':['With the rapid development of computer, communication and networking technologies, Web information resources are becoming increasingly rich, and on-line applications are required to be more and more flexible. Therefore, a new generation of information models and mechanisms for these information processing requirements has become a major challenge.', 'This talk will discuss four scientific problems in ubiquitous personalized information processing with wildcards, including demand driven aggregation of information resources, mining and analysis for aggregated information, user interest modeling, and system security. With these four components, a pervasive and personalized information processing mechanism can be constructed for dynamically organizing and optimizing multiple information resources, implementing a transparent and scalable information processing architecture, and flexibly building information and analysis services to meet various user demands.']}
{'论文标题':['An Event-Centric Prediction System for COVID-19'],'论文作者':['Fu, XY', ';\xa0', 'Jiang, X', ';\xa0', 'Wu, XD'],'论文摘要':['As COVID-19 evolved into a pandemic, a lot of effort has been made by scientific community to intervene in its spread. One of them was to predict the trend of the epidemic to provide a basis for the decision making of both the public and private sectors. In this paper, a system for predicting the spread of COVID-19 based on detecting and tracking events evolution in social media is proposed. The system includes a pipeline for building Event-Centric Knowledge Graphs from Twitter data streams about COVID-19, and uses the graph statistics to obtain a more accurate prediction based on the simulation of epidemic dynamic models. Experiments of 128 countries or regions conducted on the data set released by Johns Hopkins University on COVID-19 confirmed the effectiveness of the system. At the same time, the guidance our system provided to the plan of return-to-work for an enterprise has attracted the attention of and reported by top influential media.']}
{'论文标题':['Robust Joint Graph Sparse Coding for Unsupervised Spectral Feature Selection'],'论文作者':['Zhu, XF', ';\xa0', 'Li, XL', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we propose a new unsupervised spectral feature selection model by embedding a graph regularizer into the framework of joint sparse regression for preserving the local structures of data. To do this, we first extract the bases of training data by previous dictionary learning methods and, then, map original data into the basis space to generate their new representations, by proposing a novel joint graph sparse coding (JGSC) model. In JGSC, we first formulate its objective function by simultaneously taking subspace learning and joint sparse regression into account, then, design a new optimization solution to solve the resulting objective function, and further prove the convergence of the proposed solution. Furthermore, we extend JGSC to a robust JGSC (RJGSC) via replacing the least square loss function with a robust loss function, for achieving the same goals and also avoiding the impact of outliers. Finally, experimental results on real data sets showed that both JGSC and RJGSC outperformed the state-of-the-art algorithms in terms of k-nearest neighbor classification performance.']}
{'论文标题':['Marginalized Stacked Denoising Autoencoder With Adaptive Noise Probability for Cross Domain Classification'],'论文作者':['Zhang, YH', ';\xa0', 'Yang, S', ';\xa0', 'Wang, H'],'论文摘要':['Cross-domain classification is a challenging problem, in which, how to learn domain invariant features is critical. Recently, significant improvements to this problem have emerged with the wide application of deep learning models, which have been proposed to learn higher level and robust feature representation. Marginalized stacked denoising autoencoder model (mSDA) has proved to be effective to address this problem. However, the performance of mSDA is sensitive to the noise probability. In previous works, the noise probability is usually set as a constant value by cross-validation in the source domain. There is few work focus on the relationship between the noise probability and cross-domain task. In this paper, we try to compute the value of noise probability adaptively. Thus, an approach called Marginalized Stacked Denoising Autoencoders with Adaptive noise Probability (mSDA-AP) is proposed. Firstly, we extract an informative feature space by an improved index, weighted log-likehood ratio (IWLLR), then aggregate these informative features by weighting. Secondly, we compute the value of noise probability adaptively according to the distance between source domain and target domain, and then with the adaptive noise probability, we disturb the input data to learn a stronger feature space with mSDA. Finally, experimental results show the effectiveness of our proposed approach.']}
{'论文标题':['Semantic-based Concept Drift Detection Algorithm for Text Data Stream'],'论文作者':['Chu Guang', ';\xa0', 'Hu Xuegang', ';\xa0', 'Zhang Yuhong'],'论文摘要':['In text data stream, frequent concept drifts result in the poor effective information, thus the accuracy rates of drift detection and stream classification are lower. To address this problem, by introducing Latent Dirichlet Allocation ( LDA) model and considering the semantic information of text data stream, this paper proposes a new concept drift detection algorithm. It calculates the semantic similarities of both word and topic feature spaces between adjacent modules ,in which the similarity of topics is evaluated by the probability distribution of topic-word. It is considered that concept drifts occur when the similarities are lower in these two spaces. Experimental results show that, compared with DDM, CDRDT, DW CDS, HDDM-W -Test and REDLLA algorithms, the proposed algorithm can improve the performance in the concept drift detection. Especially, it can significantly reduce the missing drifts when concept frequently drifts.', '文本数据流中概念的频繁漂移导致有效信息不足,从而使得漂移检测和数据流分类准确率下降.针对该问题,引入潜在狄利克雷分布模型并考虑文本数据流隐含的语义信息,提出一种新的概念漂移检测算法.计算相邻模块中词和主题特征空间的语义相似度,其中主题的相似度根据主题-单词概率分布进行评估,当2个特征空间相似度都较低时判断为发生概念漂移.实验结果表明,与DDM、CDRDT、DWCDS、HDDM-W-Test和REDLLA算法相比,该算法对文本数据流中概念漂移的检测性能均有所提升,尤其在概念频繁漂移时可以显著减少漏检数量.']}
{'论文标题':['Simultaneous perturbation stochastic approximation-based radio occultation data assimilation for sensing atmospheric parameters'],'论文作者':['Du, HZ', ';\xa0', 'Chen, GY', ';\xa0', 'Xu, BD'],'论文摘要':['Global positioning system-based meteorological parameters sensing has become a hot topic in the field of satellite navigation application. The major research content is global positioning system radio occultation observation, which utilizes the delay and bending of global positioning system signal to compute the meteorological parameters (temperature, pressure, and water vapor), so as to improve the accuracy of numerical weather prediction. In this article, the atmospheric parameters computing algorithm based on simultaneous perturbation stochastic approximation is proposed. Perturbation effect is used to obtain the approximate gradient of cost function, which can guide the searching to achieve the optimal solution gradually. The proposed algorithm avoids the complicated derivative computing for the cost function, and without designing the tangent linear and adjoint operators. The algorithm can converge to the optimal or approximately optimal solution quickly. The validity and superiority of this method has been proved by extensive comparative experiment results.']}
{'论文标题':['Application-level Buffer Reclaim Policy Designed for Storage System'],'论文作者':['Meng Xiao-xuan', ';\xa0', 'Si Cheng-xiang', ';\xa0', 'Xu Lu'],'论文摘要':["Presented a priority-based application-level buffer reclaim policy-PARP designed for the buffer management unit in storage system. Based on partitioned buffer management model, it can differentially reclaim buffer resource from multiple applications according to their priorities. This realizes the dynamically adjustment of the application's buffer capacity to provide application-level buffer management semantics. Experiment results show that PARP effectively supports application priority in real system, which can not only be utilized to achieve storage QoS but can also be used to optimize the overall storage performance.", '针对存储系统中的缓存管理单元设计一种区分应用优先级的缓存回收策略,简称PARP.该策略基于分区缓存管理模型,它能够根据应用优先级区分回收缓存资源以实现对各应用缓存分区容量的在线动态调节进而达成应用级缓存管理语义.实验数据表明PARP策略能够在实际系统中有效的支持区分应用优先级,这不仅可以用于实现存储系统的服务质量保证同时也能够改善存储系统的整体性能.']}
{'论文标题':['A Multi-domain Adaptation for Sentiment Classification Algorithm Based on Class Distribution'],'论文作者':['Hu, KB', ';\xa0', 'Zhang, YH', ';\xa0', 'Hu, XG'],'论文摘要':["At present, most multi-domain adaptation for sentiment classification algorithms use all source domains to train the classifier with no selecting and dynamic dealing with the different source domains. This will result in those source domains very dissimilar to target have negative impact on domain adaptation. In this paper, we propose Multi-domain Adaptation algorithm based on the Class Distribution (MACD). First, the information of class distribution is used to select some adaptive base classifiers from all source domains. Then add the 'self-labeled' samples into training data, in which, the selection of samples is dynamically adjusted with the similarity between the source and target domain. Last, the final ensemble classifier is constructed using the information of class distribution. The experimental results have shown that the MACD algorithm is effective and superior to some existing approaches in accuracy."]}
{'论文标题':['Self-Adaptive Skeleton Approaches to Detect Self-Organized Coalitions From Brain Functional Networks Through Probabilistic Mixture Models'],'论文作者':['Liu, K', ';\xa0', 'Liu, HB', ';\xa0', 'Wu, XD'],'论文摘要':['Detecting self-organized coalitions from functional networks is one of the most important ways to uncover functional mechanisms in the brain. Determining these raises well-known technical challenges in terms of scale imbalance, outliers and hard-examples. In this article, we propose a novel self-adaptive skeleton approach to detect coalitions through an approximation method based on probabilistic mixture models. The nodes in the networks are characterized in terms of robust k-order complete subgraphs (k-clique) as essential substructures. The k-clique enumeration algorithm quickly enumerates all k-cliques in a parallel manner for a given network. Then, the cliques, from max-clique down to min-clique, of each order k, are hierarchically embedded into a probabilistic mixture model. They are self-adapted to the corresponding structure density of coalitions in the brain functional networks through different order k. All the cliques are merged and evolved into robust skeletons to sustain each unbalanced coalition by eliminating outliers and separating overlaps. We call this the k-CLIque Merging Evolution (CLIME) algorithm. The experimental results illustrate that the proposed approaches are robust to density variation and coalition mixture and can enable the effective detection of coalitions from real brain functional networks. There exist potential cognitive functional relations between the regions of interest in the coalitions revealed by our methods, which suggests the approach can be usefully applied in neuroscientific studies.']}
{'论文标题':['Computer-implemented error-tolerant frequent item set determination method for e-commerce applications, involves validating additional sample determined within uniform samples, until additional sample set is empty'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Mining globally interesting patterns from multiple databases using kernel estimation'],'论文作者':['Zhang, SC', ';\xa0', 'You, XF', ';\xa0', 'Wu, XD'],'论文摘要':['When extracting knowledge (or patterns) from multiple databases, the data from different databases might be too large in volume to be merged into one database for centralized mining on one computer, the local information sources might be hidden from a global decision maker due to privacy concerns, and different local databases may have different contribution to the global pattern. Dealing with multiple databases is essentially different from mining from a single database. In multi-database mining, the global patterns must be obtained by carefully analyzing the local patterns from individual databases. In this paper, we propose a nonlinear method, named KEMGP (kernel estimation for mining global patterns), to tackle this problem, which adopts kernel estimation to synthesizing local patterns for global patterns. We also adopt a method to divide all the data in different databases according to attribute dimensionality, which reduces the total space complexity. We test our algorithm on a customer management system, where the application is to obtain all globally interesting patterns by analyzing the individual databases. The experimental results show that our method is efficient. (C) 2009 Elsevier Ltd. All rights reserved.']}
{'论文标题':['Multi-fuzzy-constrained graph pattern matching with big graph data'],'论文作者':['Liu, GL', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['Graph pattern matching has been widespread used for protein structure analysis, expert finding and social group selection, ect. Recently, the study of graph pattern matching using the abundant attribute information of vertices and edges as constraint conditions has attracted the attention of scholars, and multi-constrained simulation has been proposed to address the problem in contextual social networks. Actually, multi-constrained graph pattern matching is an NP-complete problem and the fuzziness of constraint variables may exist in many applications. In this paper, we introduce a multi-fuzzy-constrained graph pattern matching problem in big graph data, and propose an efficient first-k algorithm Fuzzy-ETOF-K for solving it. Specifically, exploration-based method based on edge topology is adopted to improve the efficiency of edge connection, and breadth-first bounded search is used for edge matching instead of shortest path query between two nodes to improve the efficiency of edge matching. The results of our experiments conducted on three datasets of real social networks illustrate that our proposed algorithm Fuzzy-ETOF-K significantly outperforms existing approaches in efficiency and the introduction of fuzzy constraints makes our proposed algorithm more efficient and effective.']}
{'论文标题':['Pattern Matching with Wildcards based on Multiple Suffix Trees'],'论文作者':['Liu, YL', ';\xa0', 'Wu, XD', ';\xa0', 'Wang, C'],'论文摘要':['Pattern matching with wildcards is very important in many fields such as information retrieval and bioinformatics. Suffix trees are used in pattern matching with variable length wildcards. But the construction of a suffix tree needs significant time and space overload. This paper presents a new pattern matching algorithm, PST, based on multiple suffix trees. The PST algorithm uses a cutting process to divide a string S into several parts. firstly, and then establishes a suffix tree for each part of S respectively. If multiple patterns are to be retrieved, the suffix trees should be adjusted according to the cutting points: prefix sequence deletion and suffix sequence addition; prefix sequence addition and suffix sequence deletion. Theoretical analysis and experiments show that the PST algorithm can decrease the time and space overload than other peers.']}
{'论文标题':['LDA-BASED TOPIC STRENGTH ANALYSIS'],'论文作者':['Wang, JM', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['Topic strength is an important hotspot in topic research. The evolution of topic strength not only indicates emerging new topics, but also helps us to determine whether a topic will produce some fluctuation of topic strength over time. Thus, topic strength analysis can provide significant findings in public opinion monitoring and user personalization. In this paper, we present an LDA-based topic strength analysis approach. We take topic quality into our topic strength consideration by combining local LDA and global LDA. For empirical studies, we use three data sets in real applications: film critic data of "A Chinese Odyssey" in Douban Movies, corruption news data in Sina News, and public paper data. Compared to existing approaches, experimental results show that our proposed approach can obtain better results of topic strength analysis in detecting the time of event topic occurrences and distinguishing different types of topics, and it can be used to monitor the occurrences of public opinions and the changes of public concerns.']}
{'论文标题':['Proceedings of the 1st International Conference on Operations Research and Enterprise Systems (ICORES 2012)'],'论文作者':[],'论文摘要':['The following topics are dealt with: graph problems; elementary robotics; decision aid models; disasters management; complex global enterprise transformation; business process enterprise model; strategic simulation tool; capability-based joint force structure analysis; general integer programs; maximal dual feasible functions; case-based reasoning; negotiation profile identification; electronic negotiation system users; Fokker-Planck equation; multiobjective unit commitment; continuous-time revenue management; carparks; forward-backward algorithm; stochastic control problems; MEWMA VSSI control charts; stakeholder preferences aggregation; sustainable forest management; AHP; price skimming strategy; new product development; franchising services; bi-level clustering; telecommunication fraud; 3D container ship loading planning problem; rules and beam search; queuing behavior; multichannel service facility; simulated annealing algorithm; semisupervised linear SVMS problems; evolutionary algorithm; column generation approach; bi-objective max-min knapsack problem; optimization model development; image collection planning model; soft drink production process planning; enterprise network redesign; server consolidation; multiserver retrial queues; maximum likelihood estimation; multivariate skew t-distribution; production planning; maintenance planning; economic-probabilistic model; risk analysis; text classification method; service-oriented approach; decision support system; analytical hierarchy process approach; corporate climate change response modelling; traveling salesman problem; arc covering constraints; market risk models; stochastic shortest path problem; prize-collecting vehicle routing problem; cooperative model; multiclass peer-to-peer streaming networks; inventory allocation; online graphical display advertising; engine calibration process optimization; beam angle optimization; pattern search methods; smart grids; multicriteria sorting approach; mental disabilities diagnosis; automata theory; railway traffic disturbances; bus driver roster planning; bus tourism; aircraft multicriteria selection flow-based programming; cloud computing; wireless mesh networks planning; quality of service; dynamic response analysis; discrete event simulation; genetic algorithm; crop rotation; multiobjective helicopter routing problem; electric power distribution networks; academic units efficiency evaluation; data envelopment analysis; linear programming; stochastic programming; empty container allocation; service company; stock market crash and multicriteria technique.']}
{'论文标题':['Error Detection and Uncertainty Modeling for Imprecise Data'],'论文作者':['He, D', ';\xa0', 'Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we propose a method to derive and model data uncertainty from imprecise data. We view data imprecision and errors as the outcome of the precise data exposed to some uncertain channels, and our scheme is to directly derive the data uncertainty model from imprecise data, such that the derived data uncertainty information (EM), be integrated into the succeeding mining process. To achieve the goal, we propose an Expectation Maximization (EM) based approach to detect erroneous data entries from the input data. The data uncertainty models are constructed by applying statistical analysis to the detected errors. Experimental results show that the proposed error detection approach can locate data errors and suggest alternative data entry values to improve classifiers built from imprecise data. In addition, the uncertain models derived for each individual attributes are shown to be close to the genuine uncertainty models used to corrupt the data.']}
{'论文标题':['Which Type of Classifier to Use for Networked Data, Connectivity Based or Feature Based?'],'论文作者':['Zhang, Z', ';\xa0', 'Li, JY', ';\xa0', 'Liu, JX'],'论文摘要':["Multi-label classification of social network data has become an important problem. Two types of information have been used to classify nodes in a social network: characteristics of nodes, and the connectivity between nodes. Existing classification methods can be categorized to two types too, feature based methods, and connectivity based methods. We observe that there are no one size fits all classification methods, since the performance is data dependent, but in general node's class labels are determined by two factors, personal preference and peer influence. However, some data sets are personal preference dominated and are suitable for feature based methods, whereas some data sets are peer influence dominated and are suitable for connectivity based methods. The challenge then is how to judge if a data set is personal preference dominated or peer influence dominated, so a suitable classification method can be selected for its classification. In this paper, we develop a causality based criterion to determine the characteristics of a data set. Experiments on real-world data sets demonstrate the criterion can predict the suitability of a classification method for a data set."]}
{'论文标题':['Multi-sphere Support Vector Data Description for Outliers Detection on Multi-distribution Data'],'论文作者':['Xiao, YS', ';\xa0', 'Liu, B', ';\xa0', 'Cao, J'],'论文摘要':['SVDD has been proved a powerful tool for outlier detection. However, in detecting outliers on multi-distribution data, namely there are distinctive distributions in the data, it is very challenging for SVDD to generate a hyper-sphere for distinguishing outliers from normal data. Even if such a hyper-sphere can be identified, its performance is usually not good enough. This paper proposes an multi-sphere SVDD approach, named MS-SVDD, for outlier detection on multi-distribution data. First, an adaptive sphere detection method is proposed to detect data distributions in the dataset. The data is partitioned in terms of the identified data distributions, and the corresponding SVDD classifiers are constructed separately. Substantial experiments on both artificial and real-world datasets have demonstrated that the proposed approach outperforms original SVDD.(1)']}
{'论文标题':['Local Graph Edge Partitioning with a Two-Stage Heuristic Method'],'论文作者':['Ji, SW', ';\xa0', 'Bu, CY', ';\xa0', 'Wu, XD'],'论文摘要':['Graph edge partitioning divides the edges of an input graph into multiple balanced partitions of a given size to minimize the sum of vertices that are cut, which is critical to the performance of distributed graph computation platforms. Existing graph partitioning methods can be classified into two categories: offline graph partitioning and streaming graph partitioning. The first category requires global information for a graph during the partitioning, which is expensive in terms of time and memory for large-scale graphs. The second category, however, creates partitions solely based on the received edge information, which may result in lower performance than the offline methods. Therefore, in this study, the concept of local graph partitioning is introduced from local community detection to consider only local information, i.e., a part of the graph, instead of the graph as a whole, during the partitioning. The characteristic of storing only local information is important because real-world graphs are often large in scale, or they increase incrementally. Based on this idea, we propose a two-stage local partitioning algorithm, where the partitioning process is divided into two stages according to the structural changes of the current partition, and two different strategies are introduced to deal with the respective stages. Experimental results with real-world graphs demonstrate that the proposed algorithm outperforms the rival algorithms in most cases, including the state-of-the-art algorithm METIS.']}
{'论文标题':['URL attribute integration method based on link path search'],'论文作者':['Ma Yan-hong', ';\xa0', 'Hu Xue-gang', ';\xa0', 'Wu Gong-ging'],'论文摘要':['As some anchor texts of webpage can not provide sufficient information, the performance of filling the structured database with semi-structured information of Web pages is not good in the experiments of W2DR method. This paper proposes a method based on the link path bag to integrate URL attributes. Through integrating anchor texts with Web titles, the URL attribute values are obtained and filled into the destination database from the searched Web page datasets according to the best matching strategy. Experimental results show that the F value of this method increases by 13.91% and 3.54% in two different datasets respectively.', '在W2DR算法实验中,部分网页因其锚文本提供的信息量不足,导致利用半结构化的网页信息填充结构化数据库内容效果不佳。为此,提出一种基于链接路径包的URL属性集成方法。采用将锚文本和网页标题相结合的机制,从被搜索网页集中,根据最佳匹配策略求解得到URL属性值,并将其填充到目标数据库。实验结果表明,与W2DR算法相比,该方法在2个不同数据集中的F值分别提高13.91%和3.54%。']}
{'论文标题':['Online feature selection for high-dimensional class-imbalanced data'],'论文作者':['Zhou, P', ';\xa0', 'Hu, XG', ';\xa0', 'Wu, XD'],'论文摘要':['When tackling high dimensionality in data mining, online feature selection which deals with features flowing in one by one over time, presents more advantages than traditional feature selection methods. However, in real-world applications, such as fraud detection and medical diagnosis, the data is high dimensional and highly class imbalanced, namely there are many more instances of some classes than others. In such cases of class imbalance, existing online feature selection algorithms usually ignore the small classes which can be important in these applications. It is hence a challenge to learn from high dimensional and class imbalanced data in an online manner. Motivated by this, we first formalize the problem of online streaming feature selection for class imbalanced data, and then present an efficient online feature selection framework regarding the dependency between condition features and decision classes. Meanwhile, we propose a new algorithm of Online Feature Selection based on the Dependency in K nearest neighbors, called K-OFSD. In terms of Neighborhood Rough Set theory, K-OFSD uses the information of nearest neighbors to select relevant features which can get higher separability between the majority class and the minority class. Finally, experimental studies on seven high-dimensional and class imbalanced data sets show that our algorithm can achieve better performance than traditional feature selection methods with the same numbers of features and state-of-the-art online streaming feature selection algorithms in an online manner. (C) 2017 Elsevier B.V. All rights reserved.']}
{'论文标题':['PERFORMANCE ANALYSIS AND DISCUSSION ON A HEURISTIC APPROACH FOR SCHEDULING MULTIPROCESSOR TASKS IN A GRID COMPUTING ENVIRONMENT'],'论文作者':['Lin, JF'],'论文摘要':['The problem considered in this paper is motivated by recent studies in scheduling tasks in a grid computing environment. A grid infrastructure is a virtual organization that integrates a large number of distributed resources and high performance computing capabilities into a super service, which can be viewed as an agreement to share resources among independent organizations. Users may dispatch their tasks to the grid computing environment and use remote computing resources instead of computing locally. Hence, the issues of resource management and tasks scheduling are essential in a grid computing environment. As the problem of nonpreemptively scheduling independent multiprocessor tasks in a grid computing environment is NP-hard, we propose a heuristic algorithm for such a problem. We evaluate the performance of the heuristic algorithm not only by mathematical analysis, but also implementing the heuristic algorithm and exploring phenomena from the experimental results. The derived performance bound of the heuristic algorithm falls between (2-1/P) and (3-2/P), where P is the total number of processors in a grid computing environment. To explain the performance analysis of the heuristic algorithm, we implement the heuristic algorithm and discuss the observed phenomena from the experimental results.']}
{'论文标题':['Special issue on selected papers from IEEE DMF 2008'],'论文作者':['Chan, KCC', ';\xa0', 'Wu, XD'],'论文摘要':[]}
{'论文标题':['Link Prediction-based Multi-label Classification on Networked Data'],'论文作者':['Zhao, YF', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':["In this paper, we study the problem of performing multi-label classification on networked data, where each instance in the network is assigned with multiple labels and the connections between instances are driven by various casual reasons. Networked data extracted from social media or web pages may not reflect the relationship between users in real life accurately. By mining the links that actually exist but have not yet been found in the network, the potential relations between users can be discovered, and thus help us to predict the users' labels more accurately. In this work, we propose a link prediction-based multi-label relational neighbor classifier which employs social context features (LP-SCRN). It firstly predicts missing links in the network, and then calculates the weights of the links according to the similarity between nodes in their social features. In addition, by capturing the potential correlation between nodes, we expand a node's neighbor set, and refine the multi-label relational classifier. Experiments on two real-world datasets demonstrate that our proposed method improves the performance of multi-label classification on networked data."]}
{'论文标题':['Parameter tuning for induction-algorithm-oriented feature elimination'],'论文作者':['Yang, Y', ';\xa0', 'Wu, XD'],'论文摘要':[]}
{'论文标题':['Social Group Query Based on Multi-Fuzzy-Constrained Strong Simulation'],'论文作者':['Liu, GL', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['Traditional social group analysis mostly uses interaction models, event models, or other social network analysis methods to identify and distinguish groups. This type of method can divide social participants into different groups based on their geographic location, social relationships, and/or related events. However, in some applications, it is necessary to make more specific restrictions on the members and the interactions between members of the group. Generally, Graph Pattern Matching (GPM) technique is used to solve this problem. However, the existing GPM methods rarely consider the rich contextual information of nodes and edges to measure the credibility between members. In this article, first, a social group query problem that needs to consider the trust between members of the group is proposed. Then, to solve this problem, a multifuzzy-constrained strong simulation matching model is proposed based on multi-constrained simulation, and a Strong Simulation GPM algorithm (NTSS) based on the exploration of pattern Node Topological ordered sequence is proposed. Aiming at the inefficiency of the NTSS algorithm when pattern graph with multiple nodes with zero in-degree and the problem of repeated calculation of matching edges shared by multiple matching subgraphs, two optimization strategies are proposed. Finally, we conduct verification experiments on the effectiveness and efficiency of the NTSS algorithm and the algorithms with the optimization strategies on four social network datasets in real applications. Experimental results show that the NTSS algorithm is significantly better than the existing multi-constrained GPM algorithm, and the NTSS_Inv_EdgC algorithm, which combines two optimization strategies, greatly improves the efficiency of the NTSS algorithm.']}
{'论文标题':['Web News Extraction via Path Ratios'],'论文作者':['Wu, GQ', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['In addition to the news content, most web news pages also contain navigation panels, advertisements, related news links etc. These non-news items not only exist outside the news region, but are also present in the news content region. Effectively extracting the news content and filtering the noise have important effects on the follow-up activities of content management and analysis. Our extensive case studies have indicated that there exists potential relevance between web content layouts and their tag paths. Based on this observation, we design two tag path features to measure the importance of nodes: Text to tag Path Ratio (TPR) and Extended Text to tag Path Ratio (ETPR), and describe the calculation process of TPR by traversing the parsing tree of a web news page. In this paper, we present Content Extraction via Path Ratios (CEPR) - a fast, accurate and general on-line method for distinguishing news content from non-news content by the TPR/ETPR histogram effectively. In order to improve the ability of CEPR in extracting short texts, we propose a Gaussian smoothing method weighted by a tag path edit distance. This approach can enhance the importance of internal-link nodes but ignore noise nodes existing in news content. Experimental results on the CleanEval datasets and web news pages randomly selected from well-known websites show that CEPR can extract across multi-resources, multi-styles, and multi-languages. The average F and average score with CEPR is 8.69% and 14.25% higher than CETR, which demonstrates better web news extraction performance than most existing methods.']}
{'论文标题':['Multi-Marginalized Denoising Autoencoders for Domain Adaptation'],'论文作者':['Yang Shuai', ';\xa0', 'Hu Xuegang', ';\xa0', 'Zhang Yuhong'],'论文摘要':['Neural network models are used to address domain adaptation. As a model of neural network, marginalized stacked denoising autoencoders (mSDA) can extract and encode more robust feature space. mSDA tends to learn a common and robust feature representation to solve the problem of domain adaptation by marginalizing corruption with noise to the source and target domain data. However, mSDA uses the same marginalized and denoising method to corrupt all features. But in fact, features have different effects on the classification. This paper tries to corrupt the different features with a variant noise, and proposes the approach named multi-marginalized denoising autoencoders (M-MDA) for domain adaptation. Firstly, a polarity index WLLRU (weighted log-likelihood ratio update) which is improved from weight likelihood ratio, is proposed to distinguish the shared features from specific features. Then, the shared features and specific features are corrupted with different noises, and the noise is computed according to the distance of features between the source and target domain. And then marginalized denoising autoencoders (MDA) is used to learn a more robust feature space with the corrupted data. Lastly, the new feature space is corrupted again to enhance the proportion of shared features. The experimental results show that the proposed method outperforms state-of-the-art methods in cross-domain sentiment classification.', '神经网络模型被广泛用于跨领域分类学习。边缘堆叠降噪自动编码器(marginalized stacked denoising autoencoders,mSDA)作为一种神经网络模型,通过对源领域和目标领域数据进行边缘化加噪损坏,学习一个公共的、健壮的特征表示空间,从而解决领域适应问题。然而,mSDA对所有的特征都采取相同的边缘化加噪处理方式,没有考虑到不同特征对分类结果的影响不同。为此,对特征进行区分性的噪音系数干扰,提出多边缘降噪自动编码器(multi-marginalized denoising autoencoders,M-MDA)。首先,利用改进的权重似然率(weighted log-likelihood ratio update,WLLRU)区分出领域间的共享和特有特征;然后,通过计算特征在两个领域的距离,对共享特征和特有特征进行不同方式的边缘化降噪处理,并基于单层边缘降噪自动编码器(marginalized denoising autoencoders,MDA)学习获取更健壮的特征;最后,对新的特征空间进行二次损坏以强化共享特征的比例。实验结果表明,该方法在跨领域情感分类方面优于基线算法。']}
{'论文标题':['Induction by attribute elimination'],'论文作者':['Wu, XD', ';\xa0', 'Urpani, D'],'论文摘要':['In most data-mining applications where induction is used as the primary tool for knowledge extraction from real-world databases, it is difficult to precisely identify a complete set of relevant attributes. This paper introduces a new rule induction algorithm called Rule Induction Two In One (RITIO), which eliminates attributes in the order of decreasing irrelevancy. Like ID3-like decision tree construction algorithms, RITIO makes use of the entropy measure as a means of constraining the hypothesis search space; but, unlike ID3-like algorithms, the hypotheses language is the rule structure and RITIO generates rules without constructing decision trees. The final concept description produced by RITIO is shown to be largely based on only the most relevant attributes. Experimental results confirm that, even on noisy, industrial databases, RITIO achieves high levels of predictive accuracy.']}
{'论文标题':['A New Automatic Categorization Algorithm for Web Services'],'论文作者':['Jiaying He', ';\xa0', 'Peipei Li', ';\xa0', 'Xindong Wu'],'论文摘要':['With the development of network technology, Web services are becoming more and more popular. The categorization of Web services is significant in many applications, such as service publishing, discovery and semantic annotation, and all existing categorization algorithms have inherent limitations. Therefore, a new automatic categorization algorithm, called NACWS, is presented in this paper, which takes advantage of existing algorithms for Web service categorization. Our algorithm of NACWS starts with traditional text categorization technology, and takes into account both the structure features and semantics to categorize Web services. Extensive experiments demonstrate that NACWS performs with a higher precision and a better recall on the categorization of Web services in comparison with several state-of-the-art categorization algorithms for Web services.']}
{'论文标题':['Tornado Forecasting with Multiple Markov Boundaries'],'论文作者':['Yu, K', ';\xa0', 'Wang, DW', ';\xa0', 'Wu, XD'],'论文摘要':['Reliable tornado forecasting with a long-lead time can greatly support emergency response and is of vital importance for the economy and society. The large number of meteorological variables in spatiotemporal domains and the complex relationships among variables remain the top difficulties for a long-lead tornado forecasting.', 'Standard data mining approaches to tackle high dimensionality are usually designed to discover a single set of features without alternating options for domain scientists to select more reliable and physical interpretable variables.', 'In this work, we provide a new solution to use the concept of multiple Markov boundaries in local causal discovery to identify multiple sets of the precursors for tornado forecasting. Specifically, our algorithm first confines the extremely large feature spaces to a small core feature space, then it mines multiple sets of the precursors from the core feature space that may equally contribute to tornado forecasting. With the multiple sets of the precursors, we are able to report to domain scientists the predictive but practical set of precursors.', 'An extensive empirical study is conducted on eight benchmark data sets and the historical tornado data near Oklahoma City, OK in the United States. Experimental results show that the tornado precursors we identified can help to improve the reliability of long-lead time catastrophic tornado forecasting.']}
{'论文标题':['Exploring Causal Relationships with Streaming Features'],'论文作者':['Yu, K', ';\xa0', 'Wu, XD', ';\xa0', 'Wang, H'],'论文摘要':['Causal discovery is highly desirable in science and technology. In this paper, we study a new research problem of discovery of causal relationships in the context of streaming features, where the features steam in one by one. With a Bayesian network to represent causal relationships, we propose a novel algorithm called causal discovery from streaming features (CDFSF) which consists of a two-phase scheme. In the first phase, CDFSF dynamically discovers causal relationships between each feature seen so far with an arriving feature, while in the second phase CDFSF removes the false positives of each arrived feature from its current set of direct causes and effects. To improve the efficiency of CDFSF, using the symmetry properties between parents (causes) and children (effects) in a faithful Bayesian network, we present a variant of CDFSF, S-CDFSF. Experimental results validate our algorithms in comparison with the existing algorithms of causal relationship discovery.']}
{'论文标题':['Novel broken line detection circuit for multi-cells Li-ion battery protection ICs'],'论文作者':['Li, YM', ';\xa0', 'Sun, J', ';\xa0', 'Wen, CB'],'论文摘要':['To reduce the excessive power consumption and eliminate the battery voltage imbalance caused in conventional method, a novel broken line detection scheme for Li-ion battery protection integrated circuits (ICs) is presented in this study. The main part of the proposed circuit consists of pull-up, pull-down current source, source-driven MOS, a control switch and a bias current source. A narrow pulse control signal is adopted to trigger broken line detection periodically so as to suppress the detection current consumption, while the disadvantage of battery voltage imbalance is overcome. The proposed circuit has been implemented in a seven cells Li-ion battery protection IC with 0.18 mu m 45 V bipolar-CMOS-DMOS process successfully. The experimental results confirm that the chip can reliably detect the disconnections of Li-ion batteries and take protective measures in a wide cell voltage range from 2.2 to 4.2 V. Furthermore, based on the derivation in this study, the proposed technique can significantly reduce the detection current consumption of each cell, which is well beneficial for low power consumption and battery voltage balance.']}
{'论文标题':['A Dynamic Convolutional Neural Network Based Shared-Bike Demand Forecasting Model'],'论文作者':['Qiao, SJ', ';\xa0', 'Han, N', ';\xa0', 'Wu, XD'],'论文摘要':['Bike-sharing systems are becoming popular and generate a large volume of trajectory data. In a bike-sharing system, users can borrow and return bikes at different stations. In particular, a bike-sharing system will be affected by weather, the time period, and other dynamic factors, which challenges the scheduling of shared bikes. In this article, a new shared-bike demand forecasting model based on dynamic convolutional neural networks, called SDF, is proposed to predict the demand of shared bikes. SDF chooses the most relevant weather features from real weather data by using the Pearson correlation coefficient and transforms them into a two-dimensional dynamic feature matrix, taking into account the states of stations from historical data. The feature information in the matrix is extracted, learned, and trained with a newly proposed dynamic convolutional neural network to predict the demand of shared bikes in a dynamical and intelligent fashion. The phase of parameter update is optimized from three aspects: the loss function, optimization algorithm, and learning rate. Then, an accurate shared-bike demand forecasting model is designed based on the basic idea of minimizing the loss value. By comparing with classical machine learning models, the weight sharing strategy employed by SDF reduces the complexity of the network. It allows a high prediction accuracy to be achieved within a relatively short period of time. Extensive experiments are conducted on real-world bike-sharing datasets to evaluate SDF. The results show that SDF significantly outperforms classical machine learning models in prediction accuracy and efficiency.']}
{'论文标题':['Merchandise selection system i.e. online group-buying system, for use in retail business, has merchandise targeted selling module for determining feature of product for sales period in selected city if estimated sales volume meets threshold'],'论文作者':[],'论文摘要':[]}
{'论文标题':['A social tag clustering method based on common co-occurrence group similarity'],'论文作者':['Li, HZ', ';\xa0', 'Hu, XG', ';\xa0', 'Pan, JH'],'论文摘要':["Social tagging systems are widely applied in Web 2.0. Many users use these systems to create, organize, manage, and share Internet resources freely. However, many ambiguous and uncontrolled tags produced by social tagging systems not only worsen users' experience, but also restrict resources' retrieval efficiency. Tag clustering can aggregate tags with similar semantics together, and help mitigate the above problems. In this paper, we first present a common co-occurrence group similarity based approach, which employs the ternary relation among users, resources, and tags to measure the semantic relevance between tags. Then we propose a spectral clustering method to address the high dimensionality and sparsity of the annotating data. Finally, experimental results show that the proposed method is useful and efficient."]}
{'论文标题':['Penalized partial least square discriminant analysis with l(1)-norm for multi-label data'],'论文作者':['Liu, HW', ';\xa0', 'Ma, ZJ', ';\xa0', 'Wu, XD'],'论文摘要':['Multi-label data are prevalent in real world. Due to its great potential applications, multi-label learning has now been receiving more and more attention from many fields. However, how to effectively exploit the correlations of variables and labels, and tackle the high-dimensional problems of data are two major challenging issues for multi-label learning. In this paper we make an attempt to cope with these two problems by proposing an effective multi-label learning algorithm. Specifically, we make use of the technique of partial least square discriminant analysis to identify a common latent space between the variable space and the label space of multi-label data. Moreover, considering the label space of the multi-label data is sparse, a l(1)-norm penalty is further performed to constrain the Y-loadings of the optimization problem of partial least squares, making them sparse. The merit of our method is that it can capture the correlations and perform dimension reduction at the same time. The experimental results conducted on eleven public data sets show that our method is promising and superior to the state-of-the-art multi-label classifiers in most cases. (C) 2014 Elsevier Ltd. All rights reserved.']}
{'论文标题':['Learning Cross-Lingual Mappings in Imperfectly Isomorphic Embedding Spaces'],'论文作者':['Li, YL', ';\xa0', 'Yu, K', ';\xa0', 'Zhang, YH'],'论文摘要':['One mainstream method in cross-lingual word embeddings is to learn a linear mapping between two monolingual embedding spaces using a training dictionary. Successful linear mappings require isomorphic embedding spaces. However, monolingual embedding spaces are not perfectly isomorphic, and therefore, a linear mapping cannot align them accurately. In this study, we assume that two embedding spaces are composed of near-isomorphic translation pairs (NearITP) and non-isomorphic translation pairs. Owing to the nature of similar substructures, NearITP can make linear mapping work well. Motivated by this, we design a screening strategy to identify NearITP effectively. Based on this strategy, we find that the proportion of NearITP in the commonly used training dictionary is relatively low, leading to sub-optimal results. To address this problem, we propose a general framework that can be combined with any of the mapping methods, which further boosts subsequent mapping. Experimental results demonstrate that our framework is an improvement over existing mapping-based methods, and outperforms state-of-the-art models on two public data sets. Moreover, we show that our framework can be successfully generalized to contextual word embeddings such as multilingual BERT (mBERT), and further enhances the cross-lingual properties of mBERT.']}
{'论文标题':['MINING APPROXIMATE REPEATING PATTERNS FROM SEQUENCE DATA WITH GAP CONSTRAINTS'],'论文作者':['He, D', ';\xa0', 'Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['The rapid increase of available DNA, protein, and other biological sequences has made the problem of discovering meaningful patterns from sequences an important task for Bioinformatics research. Among all types of patterns defined in the literature, the most challenging one is to find repeating patterns with gap constraints. In this article, we identify a new research problem for mining approximate repeating patterns (ARPs) with gap constraints, where the appearance of a pattern is subject to an approximate match, which is very common in biological sequences. To solve the problem, we propose an ArpGap (ARP mining with Gap constraints) algorithm with three major components for ARP mining: (1) a data-driven pattern generation approach to avoid generating unnecessary candidates for validation; (2) a back-tracking pattern search process to discover approximate occurrences of a pattern under user specified gap constraints; and (3) an Apriori-like deterministic pruning approach to progressively prune patterns and cease the search process if necessary. Experimental results on synthetic and real-world protein sequences assert that ArpGap is efficient in terms of memory consumption and computational cost. The results further suggest that the proposed method is practical for discovering approximate patterns for protein sequences where the sequence length is usually several hundreds to one thousand and the pattern length is relatively short.']}
{'论文标题':['Corrupted and occluded face recognition via cooperative sparse representation'],'论文作者':['Zhao, ZQ', ';\xa0', 'Cheung, YM', ';\xa0', 'Wu, XD'],'论文摘要':['In image classification, can sparse representation (SR) associate one test image with all training ones from the correct class, but not associate with any training ones from the incorrect classes? The backward sparse representation (bSR) which contains complementary information in an opposite direction can remedy the imperfect associations discovered by the general forward sparse representation (ER). Unfortunately, this complementarity between the fSR and the bSR has not been studied in face recognition. There are two key problems to be solved. One is how to produce additional bases for the bSR. In face recognition, there is no other bases than the single test face image itself for the bSR, which results in large reconstruction residual and weak classification capability of the bSR. The other problem is how to deal with the robustness of the bSR to image corruption. In this paper, we introduce a CoSR model, which combines the fSR and the bSR together, into robust face recognition, by proposing two alternative methods to these two key problems: learning bases and unknown faces help to enrich the bases set of the bSR. Thereby, we also propose two improved algorithms of the CoSR for robust face recognition. Our study shows that our CoSR algorithms obtain inspiring and competitive recognition rates, compared with other state-of-the-art algorithms. The bSR with the proposed methods enriching the bases set contributes the most to the robustness of our CoSR algorithm, and unknown faces works better than learned bases. Moreover, since our CoSR model is performed in a subspace with a very low dimensionality, it gains an overwhelming advantage on time consumption over the traditional RSR algorithm in image pixel space. In addition, our study also reveals that the sparsity plays an important role in our CoSR algorithm for face recognition. (C) 2016 Elsevier Ltd. All rights reserved.']}
{'论文标题':['Multi-layer incremental induction'],'论文作者':['Wu, XD', ';\xa0', 'Lo, WHW'],'论文摘要':['This paper describes a multi-layer incremental induction algorithm, MLII, which is linked to an existing nonincremental induction algorithm to learn incrementally from noisy data. MLII makes use of three operations: data partitioning, generalization and reduction. Generalization can either learn a set of rules from a (sub)set of examples, or refine a previous set of rules. The latter is achieved through a redescription operation called reduction: from a set of examples and a set of rules, Rie derive a new set of examples describing the behaviour of the rule set. New rules are extracted from these behavioral examples, and these rules can be seen as meta-rules, as they control previous rules in order to improve their predictive accuracy. Experimental results show that MLII achieves significant improvement on the existing nonincremental algorithm HCV used for experiments in this paper, in terms of rule accuracy.']}
{'论文标题':['Morphology-based realization of a rapid scoliosis correction simulation system'],'论文作者':['Shao, K', ';\xa0', 'Wang, H', ';\xa0', 'Huo, X'],'论文摘要':['Objective: Scoliosis is a complex spinal deformity in 3D space that commonly occurs in teenagers, especially teenage girls, and judging the actual deformed spine situation using only CT images is difficult. However, using 3D finite element models to help doctors analyse the deformed spine is also time-consuming and laborious. Therefore, software that can quickly and easily perform scoliosis correction analysis is needed. To achieve rapid preoperative simulation of scoliosis correction in 3D space and help doctors construct surgical programmes faster, a morphology-based system was developed for simulating scoliosis correction performance.', 'Methods: The simulation system first takes advantage of the centre point of each vertebra on the entire spine model to fit a space curve. Then the system obtains information from the models and the space curve, and finally, uses the information to simulate scoliosis correction. The deformed spine model in the system can be corrected to a better state.', 'Results: During the simulation process, doctors can easily and clearly see how the vertebral models move, and the deformed spine parameters are also updated and shown. Using this system, doctors can easily simulate scoliosis correction according to their experience and quickly construct a surgical programme.', "Conclusions: The experimental results show that this system is capable of simulating scoliosis correction according to a doctor's own experience to speed up the operation and provides a scientific basis for the development of surgical programmes."]}
{'论文标题':['Tensor deletion model based vehicle network data multiple estimation method, involves interpolating incomplete random tensor data set, and obtaining repaired data closer to original data to call bubble sorting mechanism algorithm'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Wasserstein GAN based on Autoencoder with back-translation for cross-lingual embedding mappings'],'论文作者':['Zhang, YH', ';\xa0', 'Li, YL', ';\xa0', 'Hu, XG'],'论文摘要':['Recent works about learning cross-lingual word mappings (CWMs) focus on relaxing the requirement of bilingual signals through generative adversarial networks (GANs). GANs based models intend to enforce source embedding space to align target embedding space. However, existing GANs based models cannot exploit the underlying information of target-side for an alignment standard in the training, which may lead to some suboptimal results of CWMs. To address this problem, we propose a novel method, named Wasserstein GAN based on autoencoder with back-translation (ABWGAN) that can effectively exploit the target-side information and improve the performance of GANs based models. ABWGAN is an innovative combination of preliminary mappings learning and back-translation with target-side (BT-TS). In the proposed BT-TS, we back-translate target-side embeddings with preliminary CWMs to learn the final cross-lingual mappings, which enables to improve the quality of the preliminary mappings by reusing the target-side samples. Experimental results on three language pairs demonstrate the effectiveness of the proposed ABWGAN. (C) 2019 Elsevier B.V. All rights reserved.']}
{'论文标题':['A relation extraction method of Chinese named entities based on location and semantic features'],'论文作者':['Li, HG', ';\xa0', 'Wu, XD', ';\xa0', 'Wu, GQ'],'论文摘要':['Named entity relations are a foundation of semantic networks, ontology and the semantic Web, and are widely used in information retrieval and machine translation, as well as automatic question and answering systems. In named entity relations, relational feature selection and extraction are two key issues. The location features possess excellent computability and operability, while the semantic features have strong intelligibility and reality. Currently, relation extraction of Chinese named entities mainly adopts the Vector Space Model (VSM), a traditional semantic computing or the classification method, and these three methods use either the location features or the semantic features alone, resulting in unsatisfactory extraction. A relation extraction method of Chinese named entities called LaSE is proposed to combine the information gain of the positions of words and semantic computing based on HowNet. LaSE is scalable, semi-supervised and domain independent. Extensive experiments show that LaSE is superior, with an F-score of 0.879, which is at least 0.113 better than existing extraction methods that use either the location features or the semantic features alone.']}
{'论文标题':['An Evaluation of Big Data Analytics in Feature Selection for Long-lead Extreme Floods Forecasting'],'论文作者':['Zhuang, Y', ';\xa0', 'Kui, Y', ';\xa0', 'Ding, W'],'论文摘要':[]}
{'论文标题':['A framework for semantic connection based topic evolution with DeepWalk'],'论文作者':['Wang, JM', ';\xa0', 'Wu, XD', ';\xa0', 'Li, L'],'论文摘要':['One of the prevalent studies on Topic Detection and Tracking (TDT) is topic evolution. With the emergence of Internet data, there is a clear need for intuitive and adaptive methods to analyze a series of evolutions. Most approaches completely depend on topic models and only focus on whether topics are changed while ignoring the degree of changes, resulting in poor quality topics and insensitivity of changes. In this paper, we propose a framework of topic evolution based on semantic connections which not only indicates the content similarity between documents but also shows the time decay for an adaptive number of topics and rapid responses to the changes of contents. Additionally, semantic connection features can be used to visualize topic evolution, which makes the analyses much easier. For empirical studies, three data sets in real applications are chosen to prove the effectiveness of our method, and the results show that our method has a better performance in reducing redundant topics, avoiding topic suppression, and discerning the vanishment of old topics and the appearance of new topics.']}
{'论文标题':['Balanced Tree Partitioning with Succinct Logic'],'论文作者':['Wu, XD', ';\xa0', 'Sheng, SJ', ';\xa0', 'Zhou, P'],'论文摘要':['As a widely used data structure, graphs are good at characterizing data with internal associations, such as social and biological data. Tree structured data are special and are widely used in many real-world applications, such as organizational structure analysis and genealogical knowledge graph reasoning. For example, in kinship knowledge graph analysis, when a genealogical tree is particularly large (more than 25 levels and 45,000 nodes), it is a great challenge to partition this large tree into a specified number of subtrees with succinct logic and a balanced number of nodes. Therefore, in this paper, we propose the TPA (tree partitioning algorithm) algorithm to achieve a balanced and succinct logic partition of large-scale tree structured data. TPA first extracts all related nodes from a massive graph database and then constructs the convergent subgraph into a complete tree with a specified root node. Specifically, several virtual nodes are supplemented for generation-skipping connected nodes to achieve correct node numbering and partitioning. Finally, a graph partitioning algorithm is executed on the complete tree to obtain a specified number of subtrees with succinct logic and balanced node scales. Experiments conducted on four real-world datasets verify the effectiveness of our TPA algorithm.']}
{'论文标题':['An efficient and fast algorithm for community detection based on node role analysis'],'论文作者':['Hu, XG', ' (', 'Hu, Xuegang', ') ', ';\xa0', 'He, W', ' (', 'He, Wei', ') ', ';\xa0', 'Li, L', ' (', 'Li, Lei', ') ', ';\xa0', 'Lin, YJ', ' (', 'Lin, Yaojin', ') ', ';\xa0', 'Li, HZ', ' (', 'Li, Huizong', ') ', ';\xa0', 'Pan, JH', ' (', 'Pan, Jianhan', ') '],'论文摘要':['The community structure of networks provides a comprehensive insight into their organizational structures and functional behaviors. Label propagation is one of the most commonly adopted community detection algorithm with nearly linear time complexity. It ignores the difference between nodes when breaking ties, leading to poor stability and the occurrence of the monster community. We note that different community-oriented node roles impact the label propagation in different ways. In this paper, we propose a role-based label propagation algorithm (roLPA), in which the heuristics with regard to community-oriented node role were used. We have evaluated the proposed algorithm on both real and artificial networks. The result shows that roLPA outperforms other state-of-the-art community detection algorithms.']}
{'论文标题':['Predicting Long-Term Trajectories of Connected Vehicles via the Prefix-Projection Technique'],'论文作者':['Qiao, SJ', ';\xa0', 'Han, N', ';\xa0', 'Wu, XD'],'论文摘要':['The vehicle location prediction based on their spatial and temporal information is an important and difficult task in many applications. In the last few years, devices, such as connected vehicles, smart phones, GPS navigation systems, and smart home appliances, have amassed the large stores of geographic data. The task of leveraging this data by employing moving objects database techniques to predict spatio-temporal locations in an accurate and efficient fashion, comprising a complete trajectory remains an actively researched area. Existing methods for frequent sequential pattern mining tend to be limited to predicting short-term partial trajectories, at extremely high computational costs. In order to address these limitations, we designed a prefix-projection-based trajectory prediction algorithm called PrefixTP, which contains three essential phases. First, data collection, connected vehicles equipped with sensors comprise a vehicle grid and generate copious amounts of spatio-temporal data, in order to communicate and share traffic information. Second, model training, examining only the prefix subsequences, and projecting only their corresponding postfix subsequences into projected sets. Finally, trajectory matching, recursively finding postfix sequences meeting the requirement of minimum support count, and outputting the mast frequent sequential pattern as the most probable trajectory. Fundamentally, PrefixTP supports three trajectory matching strategies which encompass all possibilities of prediction. Extensive experiments were conducted using real world GPS data sets, and the results show, when comparing predicted complete trajectories against partial shortterm trajectories with a guarantee of real-time forecasting, that PrefixTP outperforms first-order, second-order Markov models, and Apriori-based trajectory prediction algorithm.']}
{'论文标题':['A Unified View of Causal and Non-causal Feature Selection'],'论文作者':['Yu, K', ';\xa0', 'Liu, L', ';\xa0', 'Li, JY'],'论文摘要':['In this article, we aim to develop a unified view of causal and non-causal feature selection methods. The unified view will fill in the gap in the research of the relation between the two types of methods. Based on the Bayesian network framework and information theory, we first show that causal and non-causal feature selection methods share the same objective. That is to find the Markov blanket of a class attribute, the theoretically optimal feature set for classification. We then examine the assumptions made by causal and non-causal feature selection methods when searching for the optimal feature set, and unify the assumptions by mapping them to the restrictions on the structure of the Bayesian network model of the studied problem. We further analyze in detail how the structural assumptions lead to the different levels of approximations employed by the methods in their search, which then result in the approximations in the feature sets found by the methods with respect to the optimal feature set. With the unified view, we can interpret the output of non-causal methods from a causal perspective and derive the error bounds of both types of methods. Finally, we present practical understanding of the relation between causal and non-causal methods using extensive experiments with synthetic data and various types of real-world data.']}
{'论文标题':['A structurally motivated framework for discriminant analysis'],'论文作者':['Yang, B', ';\xa0', 'Chen, SC', ';\xa0', 'Wu, XD'],'论文摘要':['Over the last few years, a lot of algorithms for discriminant analysis (DA) have been developed. Although having different motivations, they all inject structure information in data into their own within- and between-class scatters. However, to our best knowledge, there has not been yet a systematical examination about (1) which structure granularities lurk in data; (2) which structure granularities are utilized in scatters of a DA algorithm; (3) whether new DA algorithms can be developed based on existing structure granularities. In this paper, the established so-called structurally motivated (SM) framework for DA and its unified mathematical formulation of the ratio trace exactly answers them. It categorizes these DA algorithms from the viewpoint of constructing scatters based on different-granularity structures in data, identifies their applicable scenarios for different structure types, and provides insights into developing new DA algorithms. Inspired by the insight, we find that cluster granularity lying in the middle of granularity spectrum in SM framework can still be further utilized and exploited. As a result, the three DA algorithms based on the cluster granularity are derived from the SM framework and from the injection of the cluster structure information into the respective within-class, between-class and joint both scatter matrices for the classical MDA, and these corresponding algorithms are, respectively, called as SWDA, SBDA and SWBDA. The injection of cluster structure information makes the proposed three algorithms able to fit relatively complicated data not only more effectively, but also with the regularization technique obtain more projections than the classical MDA, which is very helpful for more effective DA. Moreover, MDA becomes their special case when the cluster numbers of all classes are set to 1. Our experiments on the benchmarks (face and UCI databases) here show that the proposed algorithms yield encouraging results.']}
{'论文标题':['Authorship identification from unstructured texts'],'论文作者':['Zhang, CX', ';\xa0', 'Wu, XD', ';\xa0', 'Ding, W'],'论文摘要':['Authorship identification is a task of identifying authors of anonymous texts given examples of the writing of authors. The increasingly large volumes of anonymous texts on the Internet enhance the great yet urgent necessity for authorship identification. It has been applied to more and more practical applications including literary works, intelligence, criminal law, civil law, and computer forensics. In this paper, we propose a semantic association model about voice, word dependency relations, and non-subject stylistic words to represent the writing style of unstructured texts of various authors, design an unsupervised approach to extract stylistic features, and employ principal components analysis and linear discriminant analysis to identify authorship of texts. This paper provides a uniform quantified method to capture syntactic and semantic stylistic characteristics of and between words and phrases, and this approach can solve the problem of the independence of different dimensions to some extent. Experimental results on two English text corpora show that our approach significantly improves the overall performance over authorship identification. (C) 2014 Elsevier B.V. All rights reserved.']}
{'论文标题':['Hierarchical features-based targeted aspect extraction from online reviews'],'论文作者':['He, J', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['With the prevalence of online review websites, large-scale data promote the necessity of focused analysis. This task aims to capture the information that is highly relevant to a specific aspect. However, the broad scope of the aspects of the various products makes this task overarching but challenging. A commonly used solution is to modify the topic models with additional information to capture the features for a specific aspect (referred to as a targeted aspect). However, the existing topic models, either perform the full analysis to capture features as many as possible or estimate the similarity to capture features as coherent as possible, overlook the fine-grained semantic relations between the features, resulting in the captured features coarse and confusing. In this paper, we propose a novel Hierarchical Features-based Topic Model (HFTM) to extract targeted aspects from online reviews, then to capture the aspect-specific features. Specifically, our model can not only capture the direct features posing target-to-feature semantics but also capture the latent features posing feature-to-feature semantics. The experiments conducted on real-world datasets demonstrate that HFTM1 outperforms the state-of-the-art baselines in terms of both aspect extraction and document classification.']}
{'论文标题':['TOP-10 DATA MINING CASE STUDIES'],'论文作者':['Melli, G', ';\xa0', 'Wu, XD', ';\xa0', 'Zaiane, O'],'论文摘要':['We report on the panel discussion held at the ICDM\'10 conference on the top 10 data mining case studies in order to provide a snapshot of where and how data mining techniques have made significant real-world impact. The tasks covered by 10 case studies range from the detection of anomalies such as cancer, fraud, and system failures to the optimization of organizational operations, and include the automated extraction of information from unstructured sources. From the 10 cases we find that supervised methods prevail while unsupervised techniques play a supporting role. Further, significant domain knowledge is generally required to achieve a completed solution. Finally, we find that successful applications are more commonly associated with continual improvement rather than by single "aha moments" of knowledge ("nugget") discovery.']}
{'论文标题':['Multi-Target Core Network-based Networked Multi-label Classification'],'论文作者':['Li, L', ';\xa0', 'Zhang, F', ';\xa0', 'Hu, XG'],'论文摘要':['As the increasing popularity of label classification, networked multi-label classification is becoming a hot topic in the field of data mining, where the networked multi-label means that each entity has more than one label during classification in network environments. In the existing works on networked multi label classification, although only the labels of certain nodes are required to be determined, the labels of all nodes in the network have to be inferred. This works well for small networks, but not for large networks, especially not for large-scale networks with big data, as a plenty of time has been spent to compute a lot of unrequired labels. In this paper, we introduce a core network which is composed of the shortest paths that link some sources (i.e., some nodes with known labels) and some targets (i.e., some nodes with unknown labels required to be determined), as these paths have the most significant directly influence on label classification. Then we propose a novel heuristic MultI-TargeT corE Network discovery algorithm MITTEN to discover a core network, which aims to achieve the relatively accuracy of predicted labels with a relatively short time. Compared with existing networked multi-label classification approaches, the experimental results executed on real networks show that our proposed MITTEN can predict labels in network environments more precisely and more efficiently.']}
{'论文标题':['Dynamic Community Evolution Analysis Framework for Large-Scale Complex Networks Based on Strong and Weak Events'],'论文作者':['Qiao, SJ', ';\xa0', 'Han, N', ';\xa0', 'Wu, XD'],'论文摘要':['Community evolution remains a heavily researched and challenging area in the analysis of dynamic complex network structures. Currently, the primary limitation of traditional event-based approaches for community evolution analysis is the lack of strict constraint conditions for distinguishing evolutionary events, which entails that as the cardinality of discovered events increases, so does the number of redundant events. Another limitation of existing approaches is the lack of consideration for weak events. Weak events can be generated by small changes in communities, which are empirically prevalent, and are typically not captured by traditional events. To manage these two aforementioned limitations, this research aims to formalize a weak and strong events-based framework, which includes the following newly discovered events: "weak shrink," "weak expand," "weak merge," and "weak splity" predicated on the community overlapping degree and community degree membership, this article refines these traditional strong events, as well as new constraints for weak events. In addition, a community evolution mining framework, which is based on both strong and weak events, is proposed and denoted by a weak-event-based community evolution method (WECEM). The framework can be summarized by the following: 1) communities in complex networks with adjacent time-stamps are compared to determine the community overlapping degree and community membership degree; 2) the values of the community overlapping degree and membership degree meet the definition of events; and 3) weak events are effectively identified. Extensive experimental results, on real and synthetic data sets consisting of dynamic complex networks and online social networks, demonstrate that WECEM is able to identify weak events more effectively than traditional frameworks. Specifically, WECEM outperforms traditional frameworks by 22.9% in the number of discovered strong events. The detection accuracy of evolutionary events is approximately 12.2% higher than that of traditional event-based frameworks. It is also worth noting that, as the cardinality of the data grows, the proposed framework, when compared with traditional frameworks, can more effectively, and efficiently, mine large-scale complex networks.']}
{'论文标题':['Approximate pattern matching with gap constraints'],'论文作者':['Wu, YX', ';\xa0', 'Tang, ZQ', ';\xa0', 'Wu, XD'],'论文摘要':['Pattern matching is a key issue in sequential pattern mining. Many researchers now focus on pattern matching with gap constraints. However, most of these studies involve exact pattern matching problems, a special case of approximate pattern matching and a more challenging task. In this study, we introduce an approximate pattern matching problem with Hamming distance. Its objective is to compute the number of approximate occurrences of pattern P with gap constraints in sequence S under similarity constraint d. We propose an efficient algorithm named Single-rOot Nettree for approximate pattern matchinG with gap constraints (SONG) based on a new non-linear data structure Single-root Nettree to effectively solve the problem. Theoretical analysis and experiments demonstrate an interesting law that the ratio M(P,S,d)/N(P,S,m) approximately follows a binomial distribution, where M(P,S,d) and N(P,S,m) are the numbers of the approximate occurrences whose distances to pattern P are d (0dm) and no more than m (the length of pattern P), respectively. Experimental results for real biological data validate the efficiency and effectiveness of SONG.']}
{'论文标题':['Error Tree: A Tree Structure for Hamming & Edit Distances & Wildcards Matching [arXiv]'],'论文作者':['Al-Okaily, A.'],'论文摘要':['Error Tree is a novel tree structure that is mainly oriented to solve the approximate pattern matching problems, Hamming and edit distances, as well as the wildcards matching problem. The input is a text of length n over a fixed alphabet of length Sigma, a pattern of length m, and k. The output is to find all positions that have les k Hamming distance, edit distance, or wildcards matching with P. The algorithm proposes for Hamming distance and wildcards matching a tree structure that needs O(n/log', 'n/k!) words and takes O(m', '/k! + occ)(O(m + log', 'n/k! + occ) in the average case) of query time for any online/offline pattern, where occ is the number of outputs. As well, a tree structure of O(2', 'nlog', 'n/k!) words and O(m', '/k! + 3', 'occ) in the average case) query time for edit distance for any online/offline pattern.']}
{'论文标题':['Method for distributing editorial content on web page, involves rendering editorial content with advisement, when target information of advertisement is compared with target information of editorial content'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Combining context-relevant features with multi-stage attention network for short text classification'],'论文作者':['Liu, YY', ';\xa0', 'Li, PP', ';\xa0', 'Hu, XG'],'论文摘要':['Short text classification is a challenging task in natural language processing. Existing traditional methods using external knowledge to deal with the sparsity and ambiguity of short texts have achieved good results, but accuracy still needs to be improved because they ignore the context-relevant features. Deep learning methods based on RNN or CNN are hence becoming more and more popular in short text classification. However, RNN based methods cannot perform well in the parallelization which causes the lower efficiency, while CNN based methods ignore sequences and relationships between words, which causes the poorer effectiveness. Motivated by this, we propose a novel short text classification approach combining Context-Relevant Features with multi- stage Attention model based on Temporal Convolutional Network (TCN) and CNN, called CRFA. In our approach, we firstly use Probase as external knowledge to enrich the semantic representation for the solution to the data sparsity and ambiguity of short texts. Secondly, we design a multi-stage attention model based on TCN and CNN, where TCN is introduced to improve the parallelization of the proposed model for higher efficiency, and discriminative features are obtained at each stage through the fusion of attention and different-level CNN for a higher accuracy. Specifically, TCN is adopted to capture context-related features at word and concept levels, and meanwhile, in order to measure the importance of features, Word-level TCN (WTCN) based attention, Concept-level TCN (CTCN) based attention and different-level CNN are used at each stage to focus on the information of more important features. Finally, experimental studies demonstrate the effectiveness and efficiency of our approach in the short text classification compared to several well-known short text classification approaches based on CNN and RNN.']}
{'论文标题':['Fundamentals of association rules in data mining and knowledge discovery'],'论文作者':['Zhang, SC', ';\xa0', 'Wu, XD'],'论文摘要':['Association rule mining is one of the fundamental research topics in data mining and knowledge discovery that identifies interesting relationships between item-sets in datasets and predicts the associative and correlative behaviors for new data. Rooted in market basket analysis, there are a great number of techniques developed for association rule mining. They include frequent pattern discovery, interestingness, complex associations, and multiple data source mining. This paper introduces the up-to-date prevailing association rule mining methods and advocates the mining of complete association rules, including both positive and negative association rules. (C) 2011 John Wiley& Sons, Inc. WIREs DataMining Knowl Discov 2011 1 97-116 DOI:10.1002/widm.10']}
{'论文标题':['Strict pattern matching with general gaps and one-off condition'],'论文作者':['Chai Xin', ';\xa0', 'Jia Xiao-Fei', ';\xa0', 'Wu Xin-Dong'],'论文摘要':["Pattern matching with gap constraints is one of the key issues of sequential pattern mining. One-off condition which is always used in sequential pattern mining tasks means that the character of each position in the sequence can be used at most once for pattern matching. Recently, most researches focus on pattern matching with non-negative gaps, but the rule of non-negative gaps implies the character's order in the sequence may limit the flexibility of matching. For these reasons, this article proposes a strict pattern matching with general gaps and one-off condition and shows that this problem is NP-hard. To tackle this issue, a heuristic algorithm, named dynamically changing node property (DCNP), is designed based on nettree which dynamically updates the properties of each node such as the numbers of root paths, leaf paths and root-leaf paths, and thus can get a better occurrence. The above process is then iterated. To effectively improve the speed of DCNP and avoid dynamically updating information of nodes on a large scale, a checking mechanism is applied to allow DCNP update information of nodes only when the occurrence may have repetition. The space and time complexities of DCNP are also analyzed. Experimental results show that DCNP has better performance than other competitive algorithms.", '具有间隙约束的模式匹配是序列模式挖掘的关键问题之一.一次性条件约束是要求序列中每个位置的字符最多只能使用一次,在序列模式挖掘中采用一次性条件约束更加合理.但是目前,间隙约束多为非负间隙,非负间隙对字符串中每个字符的出现顺序具有严格的约束,一定程度上限定了匹配的灵活性.为此,提出了一般间隙及一次性条件的严格模式匹配问题;之后,理论证明了该问题的计算复杂性为NP-Hard问题.为了对该问题进行有效求解,在网树结构上构建了动态更新结点信息的启发式求解算法(dynamically changing node property,简称DCNP).该算法动态地更新各个结点的树根路径数、叶子路径数和树根-叶子路径数等,进而每次可以获得一个较优的出现;之后,迭代这一过程.为了有效地提高DCNP算法速度,避免动态更新大量的结点信息,提出了Checking机制,使得DCNP算法仅在可能产生内部重复出现的时候才进行动态更新.理论分析了DCNP算法的时间复杂度和空间复杂度.大量实验结果验证了DCNP算法具有良好的求解性能.']}
{'论文标题':['Multi-source Multi-label Causal Feature Selection'],'论文作者':['Yun-an Wang', ';\xa0', 'Yaojin Lin', ';\xa0', 'Shaozi Li'],'论文摘要':['Multi-label causal feature selection plays a vital role in dealing with high-dimensional multi-label data, since the selected causal features signify the causal mechanism with respect to the class attribute. At present, the descriptive information of data is collected from different data sources in many practical applications. Therefore, there is a need of developing multi-source multi-label causal feature selection algorithms. In addition, existing multi-label causal feature selection algorithms cannot directly address multi-source heterogeneous distribution feature spaces. To address these problems, in this paper, we utilize the concept of causal invariance in causal inference. Firstly, we formulate the multi-label causal feature selection problem for multiple data sources as an invariant set search problem across the data sources. Secondly, we present the upper and lower bounds of the causal invariant set. Finally, we design a novel Multi-source Multi-label Causal Feature Selection (MMCFS) algorithm. Extensive experiments are conducted to verify the effectiveness of the proposed algorithm by comparing it with state-of-the-art methods on synthetic datasets.']}
{'论文标题':['Causal Feature Selection with Missing Data'],'论文作者':['Yu, K', ';\xa0', 'Yang, YJ', ';\xa0', 'Ding, W'],'论文摘要':['Causal feature selection aims at learning the Markov blanket (MB) of a class variable for feature selection. The MB of a class variable implies the local causal structure among the class variable and its MB and all other features are probabilistically independent of the class variable conditioning on its MB, this enables causal feature selection to identify potential causal features for feature selection for building robust and physically meaningful prediction models. Missing data, ubiquitous in many real-world applications, remain an open research problem in causal feature selection due to its technical complexity. In this article, we discuss a novel multiple imputation MB (MimMB) framework for causal feature selection with missing data. MimMB integrates Data Imputation with MB Learning in a unified framework to enable the two key components to engage with each other. MB Learning enables Data Imputation in a potentially causal feature space for achieving accurate data imputation, while accurate Data Imputation helps MB Learning identify a reliable MB of the class variable in turn. Then, we further design an enhanced kNN estimator for imputing missing values and instantiate the MimMB. In our comprehensively experimental evaluation, our new approach can effectively learn the MB of a given variable in a Bayesian network and outperforms other rival algorithms using synthetic and real-world datasets.']}
{'论文标题':['Computing Term Similarity by Large Probabilistic isA Knowledge'],'论文作者':['Li, PP', ';\xa0', 'Wang, HX', ';\xa0', 'Wu, XD'],'论文摘要':['Computing semantic similarity between two terms is essential for a variety of text analytics and understanding applications. However, existing approaches are more suitable for semantic similarity between words rather than the more general multi-word expressions (MWEs), and they do not scale very well. Therefore, we propose a lightweight and effective approach for semantic similarity using a large scale semantic network automatically acquired from billions of web documents. Given two terms, we map them into the concept space, and compare their similarity there. Furthermore, we introduce a clustering approach to orthogonalize the concept space in order to improve the accuracy of the similarity measure. Extensive studies demonstrate that our approach can accurately compute the semantic similarity between terms with MWEs and ambiguity, and significantly outperforms 12 competing methods.']}
{'论文标题':['Multi-Class Ground Truth Inference in Crowdsourcing with Clustering'],'论文作者':['Zhang, J', ';\xa0', 'Sheng, VS', ';\xa0', 'Wu, XD'],'论文摘要':["Due to low quality of crowdsourced labelers, the integrated label of each example is usually inferred from its multiple noisy labels provided by different labelers. This paper proposes a novel algorithm, Ground Truth Inference using Clustering (GTIC), to improve the quality of integrated labels for multi-class labeling. For a K labeling case, GTIC utilizes the multiple noisy label sets of examples to generate features. Then, it uses a K-Means algorithm to cluster all examples into K different groups, each of which is mapped to a specific class. Examples in the same cluster are assigned a corresponding class label. We compare GTIC with four existing multi-class ground truth inference algorithms, majority voting (MV), Dawid & Skene's (DS), ZenCrowd (ZC) and Spectral DS (SDS), on one synthetic and eight real-world datasets. Experimental results show that the performance of GTIC is significantly superior to the others in terms of both accuracy and M-AUC. Besides, the running time of GTIC is about twenty times faster than EM-based complicated inference algorithms."]}
{'论文标题':['Multimodal Graph-Based Reranking for Web Image Search'],'论文作者':['Wang, M', ';\xa0', 'Li, H', ';\xa0', 'Wu, XD'],'论文摘要':['This paper introduces a web image search reranking approach that explores multiple modalities in a graph-based learning scheme. Different from the conventional methods that usually adopt a single modality or integrate multiple modalities into a long feature vector, our approach can effectively integrate the learning of relevance scores, weights of modalities, and the distance metric and its scaling for each modality into a unified scheme. In this way, the effects of different modalities can be adaptively modulated and better reranking performance can be achieved. We conduct experiments on a large dataset that contains more than 1000 queries and 1 million images to evaluate our approach. Experimental results demonstrate that the proposed reranking approach is more robust than using each individual modality, and it also performs better than many existing methods.']}
{'论文标题':['Trust Agent-Based Behavior Induction in Social Networks'],'论文作者':['Li, L', ';\xa0', 'He, JP', ';\xa0', 'Wu, XD'],'论文摘要':[]}
{'论文标题':['Incremental Subgraph Feature Selection for Graph Classification'],'论文作者':['Wang, HS', ';\xa0', 'Zhang, P', ';\xa0', 'Wu, XD'],'论文摘要':['Graph classification is an important tool for analyzing data with structure dependency, where subgraphs are often used as features for learning. In reality, the dimension of the subgraphs crucially depends on the threshold setting of the frequency support parameter, and the number may become extremely large. As a result, subgraphs may be incrementally discovered to form a feature stream and require the underlying graph classifier to effectively discover representative subgraph features from the subgraph feature stream. In this paper, we propose a primal-dual incremental subgraph feature selection algorithm (ISF) based on a max-margin graph classifier. The ISF algorithm constructs a sequence of solutions that are both primal and dual feasible. Each primal-dual pair shrinks the dual gap and renders a better solution for the optimal subgraph feature set. To avoid bias of ISF algorithm on short-pattern subgraph features, we present a new incremental subgraph join feature selection algorithm (ISJF) by forcing graph classifiers to join short-pattern subgraphs and generate long-pattern subgraph features. We evaluate the performance of the proposed models on both synthetic networks and real-world social network data sets. Experimental results demonstrate the effectiveness of the proposed methods.']}
{'论文标题':['Semi-supervised classification on data streams with recurring concept drift and concept evolution'],'论文作者':['Zheng, XL', ';\xa0', 'Li, PP', ';\xa0', 'Yu, K'],'论文摘要':['Mining non-stationary stream is a challenging task due to its unique property of infinite length and dynamic characteristics let alone the issues of concept drift, concept evolution and limited labeled data. Although more attention has been attracted on the issues of concept drift and evolution in data streams, however, most of existing methods are supervised in nature, which probably result in a worse classification performance and lower efficiency in the case with scarcity of labeled data. Thus, in this paper, we proposed a semi-supervised framework with recurring concept drift and novel class detection called ESCR, which aims to detect recurring concept drift and concept evolution in data streams with partially labeled data. It is firstly built on an ensemble model consisted of several clustering-based classifiers. In terms of this framework, we adopt Jensen-Shannon divergence based change detection technique on classifier confidence score instead of classification error rate to detect recurring concept drifts. Meanwhile, we take concept evolution into consideration by monitoring the outliers with strong cohesion. Moreover, we further improve the execution efficiency of our framework by exploiting the recursive function and dynamic programming. Finally, extensive experiments conducted on both benchmark and synthetic data sets demonstrate the effectiveness and efficiency of our proposed semi-supervised framework in the handling of data streams with recurring concept drifts and concept evolution, as compared to several well-known semi-supervised data stream classification methods. (C) 2021 Elsevier B.V. All rights reserved.']}
{'论文标题':['Method for associating online publication i.e. poster, with print publication, involves providing data to publisher, receiving string of characters from user and presenting online publication to user based on string of characters'],'论文作者':[],'论文摘要':[]}
{'论文标题':['A logical framework for identifying quality knowledge from different data sources'],'论文作者':['Su, K', ';\xa0', 'Huang, HJ', ';\xa0', 'Zhang, SC'],'论文摘要':['As the Web has emerged as a large distributed data repository, individuals and organizations have been able to utilize the low-cost information and knowledge on the Internet when making business decisions. Because data in different data sources may beconflictive or untrue, researchers and practitioners must intensify efforts to develop appropriate techniques for its efficient use and management. In this paper, a logical framework is designed for identifying quality knowledge from different data sources, thus working towards the development of an agreed ontology. Our experimental results have demonstrated that the approach is promising, and that a minor data enhancement adjustment could bring higher effectiveness. (c) 2006 Elsevier B.V. All rights reserved.']}
{'论文标题':['Logistic Regression for Transductive Transfer Learning from Multiple Sources'],'论文作者':['Zhang, YH', ' (', 'Zhang, Yuhong', ') ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') ', ';\xa0', 'Fang, YC', ' (', 'Fang, Yucheng', ') '],'论文摘要':['Recent years have witnessed the increasing interest in transfer learning. And transdactive transfer learning from multiple source domains is one of the important topics in transfer learning. In this paper, we also address this issue. However, a new method, namely TTLRM (Transductive Transfer based on Logistic Regression from Multi-sources) is proposed to address transductive transfer learning from multiple sources to one target domain. In term of logistic regression, TTLRM estimates the data distribution difference in different domains to adjust the weights of instances, and then builds a model using these re-weighted data. This is beneficial to adapt to the target domain. Experimental results demonstrate that our method outperforms the traditional supervised learning methods and some transfer learning methods.']}
{'论文标题':['Online Feature Selection with Group Structure Analysis'],'论文作者':['Wang, J', ';\xa0', 'Wang, M', ';\xa0', 'Wu, XD'],'论文摘要':['Online selection of dynamic features has attracted intensive interest in recent years. However, existing online feature selection methods evaluate features individually and ignore the underlying structure of a feature stream. For instance, in image analysis, features are generated in groups which represent color, texture, and other visual information. Simply breaking the group structure in feature selection may degrade performance. Motivated by this observation, we formulate the problem as an online group feature selection. The problem assumes that features are generated individually but there are group structures in the feature stream. To the best of our knowledge, this is the first time that the correlation among streaming features has been considered in the online feature selection process. To solve this problem, we develop a novel online group feature selection method named OGFS. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we design a criterion based on spectral analysis to select discriminative features in each group. In the inter-group selection, we utilize a linear regression model to select an optimal subset. This two-stage procedure continues until there are no more features arriving or some predefined stopping conditions are met. Finally, we apply our method to multiple tasks including image classification and face verification. Extensive empirical studies performed on real-world and benchmark data sets demonstrate that our method outperforms other state-of-the-art online feature selection methods.']}
{'论文标题':['HGR: a new rule induction algorithm based on extension matrices'],'论文作者':['Weijun Chen', ';\xa0', 'Fuzong Lin', ';\xa0', 'Bo Zhang'],'论文摘要':['The paper proposes a heuristic, attribute-based data mining algorithm, HGR (heuristic grouping), based on the newly-developed extension matrix approach. First, the GROUP module is used to partition the positive examples of a specific class in a given example set into different groups, and then the COMP module is used to find a conjunctive complex for each group. The empirical comparison shows that the HGR predicative accuracy is competitive with its immediate predecessor, the HCV algorithm, and the famous ID3-like algorithm C4.5.']}
{'论文标题':['Synthesizing high-frequency rules from different data sources'],'论文作者':['Wu, XD', ';\xa0', 'Zhang, SC'],'论文摘要':['Many large organizations have multiple data sources, such as different branches of an interstate company. While putting all data together from different sources might amass a huge database for centralized processing, mining association rules at different data sources and forwarding the rules (rather than the original raw data) to the centralized company headquarter provides a feasible way to deal with multiple data source problems. In the meanwhile, the association rules at each data source may be required for that data source in the first instance, so association analysis at each data source is also important and useful. However, the forwarded rules from different data sources may be too many for the centralized company headquarter to use. This paper presents a weighting model for synthesizing high-frequency association rules from different data sources. There are two reasons to focus on high-frequency rules. First, a centralized company headquarter is interested in high-frequency rules because they are supported by most of its branches for corporate profitability, Second, high-frequency rules have larger chances to become valid rules in the union of all data sources. In order to extract high-frequency rules efficiently, a procedure of rule selection is also constructed to enhance the weighting model by coping with low-frequency rules. Experimental results show that our proposed weighting model is efficient and effective.']}
{'论文标题':['A Data-Aware Latent Factor Model for Web Service QoS Prediction'],'论文作者':['Wu, D', ';\xa0', 'Luo, X', ';\xa0', 'Wu, XD'],'论文摘要':["Accurately predicting unknown quality-of-service (QoS) data based on historical QoS records is vital in web service recommendation or selection. Recently, latent factor (LF) model has been widely and successfully applied to QoS prediction because it is accurate and scalable under many circumstances. Hence, state-of-the-art methods in QoS prediction are primarily based on LF. They improve the basic LF-based models by identifying the neighborhoods of QoS data based on some additional geographical information. However, the additional geographical information may be difficult to collect in considering information security, identity privacy, and commercial interests in real-world applications. Besides, they ignore the reliability of QoS data while unreliable ones are often mixed in. To address these issues, this paper proposes a data-aware latent factor (DALF) model to achieve highly accurate QoS prediction, where 'data-aware' means DALF can easily implement the predictions according to the characteristics of QoS data. The main idea is to incorporate a density peaks based clustering method into an LF model to discover the neighborhoods and unreliable ones of QoS data. Experimental results on two benchmark real-world web service QoS datasets demonstrate that DALF has better performance than the state-of-the-art models."]}
{'论文标题':['Short text clustering based on Pitman-Yor process mixture model'],'论文作者':['Qiang, JP', ';\xa0', 'Li, Y', ';\xa0', 'Wu, XD'],'论文摘要':['For finding the appropriate number of clusters in short text clustering, models based on Dirichlet Multinomial Mixture (DMM) require the maximum possible cluster number before inferring the real number of clusters. However, it is difficult to choose a proper number as we do not know the true number of clusters in short texts beforehand. The cluster distribution in DMM based on Dirichlet process as prior goes down exponentially as the number of clusters increases. Therefore, we propose a novel model based on Pitman-Yor Process to capture the power-law phenomenon of the cluster distribution in the paper. Specifically, each text chooses one of the active clusters or a new cluster with probabilities derived from the Pitman-Yor Process Mixture model (PYPM). Discriminative words and nondiscriminative words are identified automatically to help enhance text clustering. Parameters are estimated efficiently by collapsed Gibbs sampling and experimental results show PYPM is robust and effective comparing with the state-of-the-art models.']}
{'论文标题':['Assessing Sparse Information Extraction using Semantic Contexts'],'论文作者':['Li, PP', ';\xa0', 'Wang, HX', ';\xa0', 'Wu, XD'],'论文摘要':['One important assumption of information extraction is that extractions occurring more frequently are more likely to be correct. Sparse information extraction is challenging because no matter how big a corpus is, there are extractions supported by only a small amount of evidence in the corpus. A pioneering work known as REALM learns HMMs to model the context of a semantic relationship for assessing the extractions. This is quite costly and the semantics revealed for the context are not explicit. In this work, we introduce a lightweight, explicit semantic approach for sparse information extraction. We use a large semantic network consisting of millions of concepts, entities, and attributes to explicitly model the context of semantic relationships. Experiments show that our approach improves the F-score of extraction by at least 11.2% over state-of-the-art, HMM based approaches while maintaining more efficiency.']}
{'论文标题':['A Fast Parallel Community Discovery Model on Complex Networks Through Approximate Optimization'],'论文作者':['Qiao, SJ', ';\xa0', 'Han, N', ';\xa0', 'Wu, XD'],'论文摘要':['Community discovery plays an essential role in the analysis of the structural features of complex networks. Since online networks grow increasingly large and complex over time, the methods traditionally used for community discovery cannot efficiently handle large-scale network data. This introduces the important problem of how to effectively and efficiently discover large communities from complex networks. In this study, we propose a fast parallel community discovery model called picaso (a parallel community discovery algorithm based on approximate optimization), which integrates two new techniques: (1) Mountain model, which works by utilizing graph theory to approximate the selection of nodes needed for merging, and (2) Landslide algorithm, which is used to update the modularity increment based on the approximated optimization. In addition, the GraphX distribution computing framework is employed in order to achieve parallel community detection over complex networks. In the proposed model, clustering on modularity is used to initialize the Mountain model as well as to compute the weight of each edge in the networks. The relationships among the communities are then simplified by applying the Landslide algorithm, which allows us to obtain the community structures of the complex networks. Extensive experiments were conducted on real and synthetic complex network datasets, and the results demonstrate that the proposed algorithm can outperform the state of the art methods, in effectiveness and efficiency, when working to solve the problem of community detection. Moreover, we demonstratively prove that overall time performance approximates to four times faster than similar approaches. Effectively our results suggest a new paradigm for large-scale community discovery of complex networks.']}
{'论文标题':['A Survey on Multi-Label Data Stream Classification'],'论文作者':['Zheng, XL', ';\xa0', 'Li, PP', ';\xa0', 'Hu, XG'],'论文摘要':['Nowadays, many real-world applications of our daily life generate massive volume of streaming data at a higher speed than ever before, to name a few, Web clicking data streams, sensor network data and credit transaction streams. Contrary to traditional data mining using static datasets, there are several challenges for data stream mining, for instance, finite memory, one-pass and timely reaction. In this survey, we provide a comprehensive review of existing multi-label streams mining algorithms and categorize these methods based on different perspectives, which mainly focus on the multi-label data stream classification. We first briefly summarize existing multi-label and data stream classification algorithms and discuss their merits and demerits. Secondly, we identify mining constraints on classification for multi-label streaming data, and present a comprehensive study in algorithms for multi-label data stream classification. Finally, several challenges and open issues in multi-label data stream classification are discussed, which are worthwhile to be pursued by the researchers in the future.']}
{'论文标题':['Scalable Active Learning by Approximated Error Reduction'],'论文作者':['Fu, WJ', ' (', 'Fu, Weijie', ') ', ';\xa0', 'Wang, M', ' (', 'Wang, Meng', ') ', ';\xa0', 'Hao, SJ', ' (', 'Hao, Shijie', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':['We study the problem of active learning for multi-class classification on large-scale datasets. In this setting, the existing active learning approaches built upon uncertainty measures are ineffective for discovering unknown regions, and those based on expected error reduction are inefficient owing to their huge time costs. To overcome the above issues, this paper proposes a novel query selection criterion called approximated error reduction (AER). In AER, the error reduction of each candidate is estimated based on an expected impact over all datapoints and an approximated ratio between the error reduction and the impact over its nearby datapoints. In particular, we utilize hierarchical anchor graphs to construct the candidate set as well as the nearby datapoint sets of these candidates. The benefit of this strategy is that it enables a hierarchical expansion of candidates with the increase of labels, and allows us to further accelerate the AER estimation. We finally introduce AER into an efficient semi-supervised classifier for scalable active learning. Experiments on publicly available datasets with the sizes varying from thousands to millions demonstrate the effectiveness of our approach.']}
{'论文标题':['Variable-precision-dominance-based rough set approach to interval-valued information systems'],'论文作者':['Zhang, HY', ' (', 'Zhang, Hong-Ying', ') ', ', ', ';\xa0', 'Leung, Y', ' (', 'Leung, Yee', ') ', ';\xa0', 'Zhou, L', ' (', 'Zhou, Lei', ') '],'论文摘要':['This paper proposes a general framework for the study of interval-valued information systems by integrating the variable-precision-dominance-based rough set theory with inclusion measure theory. By introducing a a-dominance relation based on inclusion measures between two interval numbers, we propose a variable-precision-dominance-based rough set approach based on the substitution of indiscernibility relation by the a-dominance relation. The knowledge discovery framework is formulated for interval-valued information systems. Furthermore, knowledge reduction of interval-valued decision systems based on the variable-precision-dominance-based rough set model is postulated. Relationships between these reducts and discemibility matrices are also established to substantiate knowledge reduction in the variable-precision-dominance-based rough set model. (C) 2013 Elsevier Inc. All rights reserved.']}
{'论文标题':['Relation Representation Learning via Signed Graph Mutual Information Maximization for Trust Prediction'],'论文作者':['Jing, YJ', ';\xa0', 'Wang, H', ';\xa0', 'Huo, X'],'论文摘要':['Trust prediction is essential to enhancing reliability and reducing risk from the unreliable node, especially for online applications in open network environments. An essential fact in trust prediction is to measure the relation of both the interacting entities accurately. However, most of the existing methods infer the trust relation between interacting entities usually rely on modeling the similarity between nodes on a graph and ignore semantic relation and the influence of negative links (e.g., distrust relation). In this paper, we proposed a relation representation learning via signed graph mutual information maximization (called SGMIM). In SGMIM, we incorporate a translation model and positive point-wise mutual information to enhance the relation representations and adopt Mutual Information Maximization to align the entity and relation semantic spaces. Moreover, we further develop a sign prediction model for making accurate trust predictions. We conduct link sign prediction in trust networks based on learned the relation representation. Extensive experimental results in four real-world datasets on trust prediction task show that SGMIM significantly outperforms state-of-the-art baseline methods.']}
{'论文标题':['Unsupervised Cross-Lingual Word Embeddings Learning with Adversarial Training'],'论文作者':['Yuling Li', ';\xa0', 'Yuhong Zhang', ';\xa0', 'Xuegang Hu'],'论文摘要':['Recent works have managed to learn cross-lingual word embeddings (CLWEs) in an unsupervised manner. As a prominent unsupervised model, generative adversarial networks (GANs) have been heavily studied for unsupervised CLWEs learning by aligning the embedding spaces of different languages. Due to disturbing the embedding distribution, the embeddings of low-frequency words (LFEs) are usually treated as noises in the alignment process. To alleviate the impact of LFEs, existing GANs based models utilized a heuristic rule to aggressively sample the embeddings of high-frequency words (HFEs). However, such sampling rule lacks of theoretical support. In this paper, we propose a novel GANs based model to learn cross-lingual word embeddings without any parallel resource. To address the noise problem caused by the LFEs, some perturbations are injected into the LFEs for offsetting the distribution disturbance. In addition, a modified framework based on Cramer GAN is designed to train the perturbed LFEs and the HFEs jointly. Empirical evaluation on bilingual lexicon induction demonstrates that the proposed model outperforms the state-of-the-art GANs based model in several language pairs.']}
{'论文标题':['Active Learning with Multi-Granular Graph Auto-Encoder'],'论文作者':['He, Y', ';\xa0', 'Yuan, X', ';\xa0', 'Wu, XD'],'论文摘要':['Predictive modeling of networked data finds many real-world applications, such as fraud detection in social networks, drug discovery in biomedical networks, paper topic classification in citation networks, and so forth. Although the advanced machine learning approaches can help build reasonably accurate predictive models, their applicability is immensely hindered by the data labeling tasks, which are onerous, time-consuming, and error-prone. In this paper, we propose a novel active learning paradigm for networked data, named topology-and-content-aware (TACA) active learning, aiming to minimize the number of labels while achieving a desirable level of model accuracy. Overall, TACA advances existing works from two aspects: (1) TACA makes no assumption on the network property, whereas most existing works only perform effectively on a locally consistent network in which linked nodes are expected to share the same labels and (2) TACA generates queries without relying on model performance, thereby enjoying robust predictive results even when noises exist in the queried labels. Both theoretical and empirical evidences are presented, substantiating the effectiveness of and optimism our approach.']}
{'论文标题':['On the Feasibility of Distributed Kernel Regression for Big Data'],'论文作者':['Xu, C', ';\xa0', 'Zhang, YQ', ';\xa0', 'Wu, XD'],'论文摘要':['In Big Data applications, massive datasets with huge numbers of observations are frequently encountered. To deal with such massive datasets, a divide-and-conquer scheme (e.g., MapReduce) is often used for the analysis of Big Data. With such a strategy, a large dataset (e.g., a centralized real database or a virtual database with distributed data sources) is first divided into smaller manageable segments; the final output is then aggregated from the individual outputs of the segments. Despite its popularity in practice, it remains largely unknown whether such a distributive strategy provides valid theoretical inferences to the original data. In this paper, we address this fundamental issue for the distributed kernel regression (DKR) problem, where the algorithmic feasibility is measured by the generalization performance of the resulting estimator. To justify DKR, a uniform convergence rate is needed for bounding the generalization error over the individual outputs, which brings new and challenging issues in the Big Data setup. Using a sample dependent kernel dictionary, we show that, with proper data segmentation, DKR leads to an estimator that is generalization consistent to the unknown regression function. This result theoretically justifies DKR and sheds light on more advanced distributive algorithms for processing Big Data. The promising performance of the method is supported by both simulation and real data examples.']}
{'论文标题':['Inferring Functional Groups from Microbial Gene Catalogue with Probabilistic Topic Models'],'论文作者':['Chen, X', ';\xa0', 'He, TT', ';\xa0', 'Wu, XD'],'论文摘要':["In this paper, based on the functional elements derived from non-redundant CDs catalogue, we show that the configuration of functional groups in meta-genome samples can be inferred by probabilistic topic modeling. The probabilistic topic modeling is a Bayesian method that is able to extract useful topical information from unlabeled data. When used to study microbial samples (assuming that relative abundance of functional elements is already obtained by a homology-based approach), each sample can be considered as a 'document', which has a mixture of functional groups, while each functional group (also known as a 'latent topic') is a weight mixture of functional elements (including taxonomic levels, and indicators of gene orthologous groups and KEGG pathway mappings). The functional elements bear an analogy with 'words'. Estimating the probabilistic topic model can uncover the configuration of functional groups (the latent topic) in each sample. The experimental results demonstrate the effectiveness of our proposed method."]}
{'论文标题':['System for facilitating online and continuous recognition of handwriting on e.g. mobile phone, has machine learning model configured to deliver as output sequence of characters written by user with handwriting instrument'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Online early terminated streaming feature selection based on Rough Set theory'],'论文作者':['Zhou, P', ';\xa0', 'Li, PP', ';\xa0', 'Zhang, YP'],'论文摘要':['Feature selection is a vital dimensionality reduction technology for machine learning and data mining that aims to select a minimal subset from the original feature space. Traditional feature selection methods assume that all features can be required before learning, while features may exist in a stream mode for some real-world applications. Therefore, online streaming feature selection was proposed to handle streaming features on the fly. When the feature dimension is extraordinarily high or even infinite, it is time-consuming or impractical to wait for all the streaming features to arrive. Motivated by this, we study and solve the exciting issue of whether we can terminate the online streaming feature selection early for efficiency while maintaining satisfactory performance for the first time. Specifically, we first formally define the problem of online early terminated streaming feature selection and summary two properties that the early terminated mapping function should satisfy. Then we choose the dependency degree function in Rough Set theory as our early terminated mapping function and demonstrate that it satisfies the two properties. Based on this, we propose a novel Early Terminated Online Streaming Feature Selection framework, named OSFS-ET, which could terminate the streaming feature selection early before the end of streaming features and guarantee a competing performance with the currently selected features. Extensive experiments on twelve real-world datasets demonstrate that OSFS-ET can be far faster than state-of-the-art streaming feature selection methods while maintaining excellent performance on predictive accuracy. (C) 2021 Elsevier B.V. All rights reserved.']}
{'论文标题':['Subject Event Extraction from Chinese Court Verdict Case via Frame-filling'],'论文作者':['Wu, GQ', ';\xa0', 'Hu, SJ', ';\xa0', 'Bao, XY'],'论文摘要':['At present, the query and acquisition of the fragmented knowledge in Chinese court verdicts mainly adopt the class case retrieval method based on the search engine and the rough extraction method for a part of the data in court verdicts. These traditional methods cannot structurally extract fragmented knowledge in Chinese court verdicts and meet the needs of people for the follow-up analysis of court verdicts. Thus, in this paper, we present a structured subject event extraction method (SEE) for Chinese court verdict cases combining with techniques of event extraction (EE) and attribute-value pair extraction (AVPE). Specifically, we provide a subject event representation frame for organizing fragmented knowledge in Chinese court verdict cases. Then, we extract subject events from the unstructured cases based on the trained sequence labeling models and constructed heuristic rules, and fill them into the subject event representation frame in the form of attribute-value pairs (AVPs). The experimental results show that SEE can efficiently and automatically extract subject events from Chinese court verdict cases and visually display them via frame-filling, which promotes the efficiency of people in searching for legal materials and facilitates further research and analysis.']}
{'论文标题':['Hierarchical temporal association mining for video event detection in video databases'],'论文作者':['Min Chen', ';\xa0', 'Shu-Ching Chen', ';\xa0', 'Mei-Ling Shyu'],'论文摘要':['With the proliferation of multimedia data and ever growing requests for multimedia applications, new challenges emerged for efficient and effective managing and accessing large audio-visual collections. In this paper, we present a novel framework for video event detection, which plays an essential role in high-level video indexing and retrieval. Especially, since temporal information in a video sequence is critical in conveying video content, a hierarchical temporal association mining approach is developed to systematically capture the characteristic temporal patterns with respect to the events of interest. In this process, the unique challenges caused by the loose video structure and skewed data distribution issues are effectively tackled. In addition, an adaptive mechanism is proposed to determine the essential thresholds which are generally defined manually in the traditional association rule mining (ARM) approach. This framework thus largely relaxes the dependence on the domain knowledge and contributes to the ultimate goal of automatic video content analysis.']}
{'论文标题':['Keyphrase Extraction Based On Semantic Relatedness'],'论文作者':['Fei Xie', ';\xa0', 'Xindong Wu', ';\xa0', 'Xuegang Hu'],'论文摘要':['Keyphrase extraction is a fundamental research task in natural language processing and text mining. A limitation of previous keyphrase extraction methods based on semantic analysis is that the acquisition of the semantic features within phrases is restricted by the constructed thesaurus and language. An approach to the acquisition of the semantic features within phrases from a single document is proposed in this paper, which is used to extract document keyphrases. Semantic relatedness degrees between phrases are computed using word co-occurrence information in the document, and the document is represented as a relatedness graph. Keyphrases are extracted based on the semantic relatedness features acquired from the graph. Our experiments demonstrate that the proposed keyphrase extraction method always outperforms the baseline methods TFIDF and Kea. Furthermore, our approach is not domain-specific and the method generalizes well when it is trained on one domain (journal articles) and tested on another (news web pages).']}
{'论文标题':['Searching for Recent Celebrity Images in Microblog Platform'],'论文作者':['Zhao, N', ';\xa0', 'Hong, RC', ';\xa0', 'Chua, TS'],'论文摘要':['With the explosive growth and widespread accessibility of image content in social media, many users are eagerly searching for most recent and relevant images on topics of their interests. However, most current microblog platforms merely make use of textual information, specifically, keywords, for image search which cannot achieve satisfactory results since in most cases image content is inconsistent with textual content. In this paper we tackle this problem under the application of searching for celebrity image. The proposed method is based on the idea of refining the initial text-based search results by utilizing multimedia plus social information. Given a text search query, we first obtain an initial text-based result. Next, we extract a seed tweet set whose images contain faces recognized as celebrities and texts contain the expanded keywords. Third, we extend the seed set based on visual and user information. Lastly, we employ a multi-modal graph based learning method to properly rank the obtained tweets by integrating social and visual information. Extensive experiments on data collected from Tencent Weibo demonstrate that our proposed method could approximately achieve 3-fold improvement in results as compared to the text baseline, typically used in microblog search service.']}
{'论文标题':['Employing Semantic Context for Sparse Information Extraction Assessment'],'论文作者':['Li, PP', ';\xa0', 'Wang, HX', ';\xa0', 'Wu, XD'],'论文摘要':['A huge amount of texts available on the World Wide Web presents an unprecedented opportunity for information extraction (IE). One important assumption in IE is that frequent extractions are more likely to be correct. Sparse IE is hence a challenging task because no matter how big a corpus is, there are extractions supported by only a small amount of evidence in the corpus. However, there is limited research on sparse IF., especially in the assessment of the validity of sparse IEs. Motivated by this, we introduce a lightweight, explicit semantic approach for assessing sparse IE.(1) We first use a large semantic network consisting of millions of concepts, entities, and attributes to explicitly model the context of any semantic relationship. Second, we learn from three semantic contexts using different base classifiers to select an optimal classification model for assessing sparse extractions. Finally, experiments show that as compared with several state-of-the-art approaches, our approach can significantly improve the F-score in the assessment of sparse extractions while maintaining the efficiency.']}
{'论文标题':['A decremental algorithm for maintaining frequent itemsets in dynamic databases'],'论文作者':['Zhang, SC', ';\xa0', 'Wu, XD', ';\xa0', 'Zhang, CQ'],'论文摘要':['Data mining and machine learning must confront the problem of pattern maintenance because data updating is a fundamental operation in data management. Most existing data-mining algorithms assume that the database is static, and a database update requires rediscovering all the patterns by scanning the entire old and new data. While there are many efficient mining techniques for data additions to databases, in this paper, we propose a decremental algorithm for pattern discovery when data is being deleted from databases. We conduct extensive experiments for evaluating this approach, and illustrate that the proposed algorithm can well model and capture useful interactions within data when the data is decreasing.']}
{'论文标题':['Ubiquitous Mining with Interactive Data Mining Agents'],'论文作者':['Wu, XD', ';\xa0', 'Zhu, XQ', ';\xa0', 'Wang, FY'],'论文摘要':['Due to the increasing availability and sophistication of data recording techniques, multiple information sources and distributed computing are becoming the important trends of modern information systems. Many applications such as security informatics and social computing require a ubiquitous data analysis platform so that decisions can be made rapidly under distributed and dynamic system environments. Although data mining has now been popularly used to achieve such goals, building a data mining system is, however, a nontrivial task, which may require a complete understanding on numerous data mining techniques as well as solid programming skills. Employing agent techniques for data analysis thus becomes increasingly important, especially for users not familiar with engineering and computational sciences, to implement an effective ubiquitous mining platform. Such data mining agents should, in practice, be intelligent, complete, and compact. In this paper, we present an interactive data mining agent - OIDM (online interactive data mining), which provides three categories (classification, association analysis, and clustering) of data mining tools, and interacts with the user to facilitate the mining process. The interactive mining is accomplished through interviewing the user about the data mining task to gain efficient and intelligent data mining control. OIDM can help users find appropriate mining algorithms, refine and compare the mining process, and finally achieve the best mining results. Such interactive data mining agent techniques provide alternative solutions to rapidly deploy data mining techniques to broader areas of data intelligence and knowledge informatics.']}
{'论文标题':['An efficient and fast algorithm for community detection based on node role analysis'],'论文作者':['Hu, XG', ';\xa0', 'He, W', ';\xa0', 'Pan, JH'],'论文摘要':['The community structure of networks provides a comprehensive insight into their organizational structures and functional behaviors. Label propagation is one of the most commonly adopted community detection algorithm with nearly linear time complexity. It ignores the difference between nodes when breaking ties, leading to poor stability and the occurrence of the monster community. We note that different community-oriented node roles impact the label propagation in different ways. In this paper, we propose a role-based label propagation algorithm (roLPA), in which the heuristics with regard to community-oriented node role were used. We have evaluated the proposed algorithm on both real and artificial networks. The result shows that roLPA outperforms other state-of-the-art community detection algorithms.']}
{'论文标题':['Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval'],'论文作者':['Tao, DC', ';\xa0', 'Tang, X', ';\xa0', 'Wu, XD'],'论文摘要':["Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM's optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM(AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random subspace SVM (RS-SVM). Finally, by integrating AB-SVM and RS-SVM, an asymmetric bagging and random subspace SVM (ABRS-SVM) is built to solve these three problems and further improve the relevance feedback performance."]}
{'论文标题':['Noise Modeling with associative corruption rules'],'论文作者':['Zhang, Y', ';\xa0', 'Wu, XD'],'论文摘要':['This paper presents an active learning approach to the problem of systematic noise inference and noise elimination, specifically the inference of Associated Corruption (AC) rules. AC rules are defined to simulate a common noise formation process in real-world data, in which the occurrence of an error on one attribute is dependent on several other attribute values. Our approach consists of two algorithms, Associative Corruption Forward (ACF) and Associative Corruption Backward (ACB). Algorithm ACF is proposed for noise inference, and ACB is designed for noise elimination. The experimental results show that the ACF algorithm can infer the noise formation correctly, and ACB indeed enhances the data quality for supervised learning.']}
{'论文标题':['Weighted partial order oriented three-way decisions under score-based common voting rules'],'论文作者':['Li, L', ';\xa0', 'Wu, XD', ';\xa0', 'Jiang, YK'],'论文摘要':['Recently, with the trisecting and acting models of human cognitive behaviors, three-way decisions are introduced to deal with uncertain decisions in many applications. To define and conduct the three-way decisions on the possible or necessary loser or co-loser and/or winner or co-winner, we first define the score-based common voting rules, where a score-based function is introduced to evaluate certain utility, including positional scoring rules, maximin, Bucklin, and so on. Then on one hand, it has been proved that under certain conditions three-way decisions on the possible loser or co-loser and/or winner or co-winner are NP-complete with positional scoring rules, maximin, and Bucklin, respectively. On the other hand, we have presented algorithms whose time complexity is polynomial to conduct three-way decisions on the necessary winner or co-winner and/or necessary loser or co-loser with positional scoring rules, maximin, and Bucklin in time O (nm(2)), O (nm(3)), and O (nm(2)), respectively, which has been validated experimentally. (C) 2020 Elsevier Inc. All rights reserved.']}
{'论文标题':['Flexible Pattern Matching with Gap-length and One-off Conditions'],'论文作者':['Guo, D', ';\xa0', 'Xiang, TN', ';\xa0', 'Wu, XD'],'论文摘要':['This paper focuses on pattern matching with wildcard, gap-length and one-off conditions. It is difficult to achieve optimal solutions. We propose an FNP algorithm based on Free-Node Optimum Pruning. Each Free-Node set is a set of nodes labeled by the same number which appear on different layers in a directed graph structure WON-Net. Compared on biological data and artificial data, experimental results show that (1) FNP has a significant advantage with its solutions, winning above 95% of biological data among similar algorithms. There are theorems on obtaining optimal solutions by FNP. (2)FNP demonstrates an evident advantage on running time, when (vertical bar Net vertical bar is large and k is small. vertical bar Net vertical bar denotes the number of independent substructures without losing solutions in WON-Net and k is the number of Free-Node sets.)']}
{'论文标题':['A classification algorithm for noisy data streams'],'论文作者':['Yan Li', ';\xa0', 'Yuhong Zhang', ';\xa0', 'Li Peipei'],'论文摘要':['Classification on noisy data streams has recently become one of the most important topics in streaming data mining. In this paper, a Classification algorithm for mining Data Streams based on Mixture Models of C4.5 and NB is proposed called CDSMM. In this algorithm, C4.5 is used as the base classifiers, the hypothesis testing method is introduced for the detection of concept drifts, and a Naive Bayes classifier is adopted to filter noise. Extensive experiments demonstrate that CDSMM has substantial advantages over similar existing algorithms in the predictive accuracy on noisy data streams with concept drifts.']}
{'论文标题':['Single Image Super Resolution with Neighbor Embedding and In-place Patch Matching'],'论文作者':['Zhao, ZQ', ';\xa0', 'Hao, ZW', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we present a novel image super-resolution framework based on neighbor embedding, which belongs to the family of learning-based super-resolution methods. Instead of relying on extrinsic set of training images, image pairs are generated by learning self-similarities from the low-resolution input image itself. Furthermore, to improve the efficiency of image reconstruction, the in-place matching is introduced to the process of similar patches searching. The gradual magnification scheme is adopted to upscale the low-resolution image, and iterative back projection is used to reduce the reconstruction error at each step. Experimental results show that our method achieves satisfactory performance not only on reconstruction quality but also on time efficiency, as compared with other super-resolution methods.']}
{'论文标题':['Quality of information-based source assessment and selection'],'论文作者':['Lin, YJ', ';\xa0', 'Hu, XG', ';\xa0', 'Wu, XD'],'论文摘要':['Multiple information sources for the same set of objects can provide different representations, and combining their advantages may improve the predictive power for a given task. However, it is noticeable that some sources might be irrelevant or redundant. Thus, it is meaningful to select a set of good information sources that could help improve the learning performance, and very little work has been reported on this topic. In this paper, we first identify the two aspects of quality of information, source significance and source redundancy. In particular, significance represents the degree to which an information source contributes to the classification, and redundancy implies the information overlap among different information sources. We then propose a metric that combines neighborhood mutual information with a Max-Significance-Min-Redundancy algorithm, allowing us to select a compact set of superior information sources for classification learning. Extensive experiments show that the metric is very helpful in finding good information sources, and that the proposed method outperforms many other methods. (C) 2014 Elsevier B.V. All rights reserved.']}
{'论文标题':['Online Feature Selection for Streaming Features with High Redundancy Using Sliding-window Sampling'],'论文作者':['You, DL', ';\xa0', 'Wu, XD', ';\xa0', 'Deng, S'],'论文摘要':['In recent years, online feature selection has received much attention in data mining with the aim to reduce dimensionality of streaming features by removing irrelevant and redundant features in a real time manner. The existing works, such as Alpha-investing, OSFS, and SAOLA have been proposed to serve this purpose but have drawbacks e.g. low predication accuracy, and more numbers of selected features, streaming features can overflow when the streaming features they have high relevance to each other. In this paper, we propose an online learning algorithm, named OSFSW, with a sliding-window strategy to real-time sample streaming features, by the analysis of conditional independence to discard irrelevant and redundant features with the aim to overcome such drawbacks. Through OSFSW, we can get an approximate Markov blanket in a smaller number of selected features with high prediction accuracy. To validate the efficiency, we implement the proposed algorithm and test its performance on a prevalent dataset, i.e., NIPS 2003, and Causality Workbench. Through extensive experimental results, we demonstrate that OSFSW has a significant performance improvement on prediction accuracy and smaller numbers of selected features when comparing to Alpha-investing, OSFS and SAOLA.']}
{'论文标题':['Subkilometer Crater Discovery with Boosting and Transfer Learning'],'论文作者':['Ding, W', ';\xa0', 'Stepinski, TF', ';\xa0', 'Wu, XD'],'论文摘要':['Counting craters in remotely sensed images is the only tool that provides relative dating of remote planetary surfaces. Surveying craters requires counting a large amount of small subkilometer craters, which calls for highly efficient automatic crater detection. In this article, we present an integrated framework on autodetection of subkilometer craters with boosting and transfer learning. The framework contains three key components. First, we utilize mathematical morphology to efficiently identify crater candidates, the regions of an image that can potentially contain craters. Only those regions occupying relatively small portions of the original image are the subjects of further processing. Second, we extract and select image texture features, in combination with supervised boosting ensemble learning algorithms, to accurately classify crater candidates into craters and noncraters. Third, we integrate transfer learning into boosting, to enhance detection performance in the regions where surface morphology differs from what is characterized by the training set. Our framework is evaluated on a large test image of 37, 500 x 56, 250 m(2) on Mars, which exhibits a heavily cratered Martian terrain characterized by nonuniform surface morphology. Empirical studies demonstrate that the proposed crater detection framework can achieve an F1 score above 0.85, a significant improvement over the other crater detection algorithms.']}
{'论文标题':['Challenges of Big Data to Big Data Mining with their Processing Framework'],'论文作者':['Pandey, K.K.', ';\xa0', 'Shukla, D.'],'论文摘要':['Big data is an emerging trend and need of industries, sciences, and engineering area because all areas are having a lot of data and these data have given a result for a particular problem. This result is very helpful for taking a decision for growing to own area and this decision is taken by big data analysis and big data mining process. When any organization is performing big data mining process then it faces a lot of challenges and these challenges have given a lot of risks to take a decision. This paper presents big data and data mining concept with big data mining challenges which are related to data, process, and management. Data challenges are related to big data, Process challenges are related to data mining and Management challenges are related to the common factor of big data and data mining. In the last section, this paper proposed a conceptual big data mining process framework which is helpful for handling challenges and design big data mining algorithm.']}
{'论文标题':['Collective semantic behavior extraction in social networks'],'论文作者':['Li, L', ';\xa0', 'Zhou, C', ';\xa0', 'Wu, XD'],'论文摘要':['As a media for sharing knowledge, forming communities with similar hobbies and interacting with friends, social networks are booming dramatically recently. The study on collective behaviors in social networks is a key to analyze community dynamics and network functionalities. Hence, it is significant and necessary to accurately extract and analyze these collective behaviors. At present, the semantic behaviors of any individual in semantic social networks can be extracted conveniently. However, how to automatically extract collective semantic behaviors, to a large scale, in social networks is still an open question.', 'Our proposed collective semantic behavior extraction process works as follows: Firstly, as for the popular semantic social networks, such as Facebook, Twitter and QQ it is convenient to extract semantic behaviors from any semantic information. Secondly, as similar behaviors will form a community spontaneously, the communities with similar extracted semantic behaviors can be determined with DeepWalk. Hence, as for a determined community, our proposed collective semantic behavior extraction approach can properly extract the collective semantic behaviors in social networks. The experimental results of our proposed approach executed on real semantic information show that our proposed approach can automatically extract collective semantic behaviors accurately. (C) 2017 Elsevier B.V. All rights reserved.']}
{'论文标题':['Hybrid Consensus Pruning of Ensemble Classifiers for Big Data Malware Detection'],'论文作者':['Abawajy, JH', ';\xa0', 'Chowdhury, M', ';\xa0', 'Kelarev, A'],'论文摘要':['One of the major challenges for safeguarding the security of big data in the cloud is how to detect and prevent malicious software (malware). Despite of the fact that security and privacy are critical issues in big data, more research needs to be done in this area. As malware can affect the reliability of the data and subsequently the reputation of the system, it is critical to detect and remove malware from a system as early as possible. Recently, ensembles that combine a set of classifiers have been proposed as an efficient approach for malware detection. Unfortunately, the size, memory and processing requirements as well as the high cost of data transfer during training and operation make large ensemble classifiers unsuitable for big data in the cloud. To address this problem, we propose a new advanced ensemble pruning method, Hybrid Consensus Pruning (HCP), which is the first pruning algorithm that employs a fast consensus function to combine several classifier classes into one scheme. To test the effectiveness of the HCP method, we conducted experiments comparing its performance with Ensemble Pruning via Individual Contribution ordering (EPIC), Directed Hill Climbing Ensemble Pruning (DHCEP) and K-Means Pruning approaches for pruning very large ensemble classifiers for malware detection. The results of the experiments show that HCP achieved better results by producing better ensemble classifiers as compared to those created by EPIC, DHCEP and K-Means Pruning.']}
{'论文标题':['A Real-Time Head Pose Estimation Using Adaptive POSIT Based on Modified Supervised Descent Method'],'论文作者':['Zhao, ZQ', ';\xa0', 'Cheng, KW', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we proposed a real-time head pose estimation algorithm by extending Pose from Orthography and Scaling with Iterations (POSIT) (named Adaptive POSIT) method and modifying the Supervised Descent Method (SDM). Specifically, we used the modified SDM for facial landmarks detection and tracking, and adopted adaptive POSIT to estimate head pose. In the feature selection stage, we extracted different features in neighboring facial landmarks instead of a single feature. In the facial landmarks selection stage, we used partial facial landmarks instead of the whole facial landmarks. The experiments show that our method can track facial landmarks robustly with tolerance to certain illumination changes and partial occlusion, and improves the accuracy of head pose estimation.']}
{'论文标题':['A Scheme for Kinship Reasoning based on Ontology'],'论文作者':['Ru Chen', ';\xa0', 'Guliu Liu', ';\xa0', 'Xindong Wu'],'论文摘要':['With the rapid development of artificial intelligence and semantic networks, knowledge graphs have received extensive attention in various application domains. As a domain knowledge graph, a genealogical knowledge graph has significant research value in the family blood, family culture, and medical genetic analysis. However, as the identical relationship often has different names and complex relationships such as divorce, remarriage, and polygamy, the reasoning based on the genealogical knowledge graph is a challenging task. In response to this problem, we propose a scheme for kinship reasoning in the genealogical knowledge graph. First, based on real genealogical revision experiences, a character ontology framework in the genealogical knowledge graph is defined, and basic kinship reasoning rules are designed. Then, given different definitions of kinship in different surnames, the solution of custom reasoning rules is integrated into the reasoning framework. In addition, aiming at complex relationships in family trees, such as multiple generations of ancestors and multiple wives, a series of inference optimization methods are proposed. Finally, we implement this scheme in the Huapu system, and the experimental results conducted on a real genealogical dataset demonstrate the effectiveness and practicality of our proposed scheme.']}
{'论文标题':['Self-Taught Active Learning from Crowds'],'论文作者':['Fang, M', ';\xa0', 'Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['The emergence of social tagging and crowdsourcing systems provides a unique platform where multiple weak labelers can form a crowd to fulfill a labeling task. Yet crowd labelers are often noisy, inaccurate, and have limited labeling knowledge, and worst of all, they act independently without seeking complementary knowledge from each other to improve labeling performance. In this paper, we propose a Self-Taught Active Learning (STAL) paradigm, where imperfect labelers are able to learn complementary knowledge from one another to expand their knowledge sets and benefit the underlying active learner. We employ a probabilistic model to characterize the knowledge of each labeler through which a weak labeler can learn complementary knowledge from a stronger peer. As a result, the self-taught active learning process eventually helps achieve high classification accuracy with minimized labeling costs and labeling errors.']}
{'论文标题':['Chinese Entity Relation Extraction Based on Syntactic Features'],'论文作者':['Jiang, YS', ';\xa0', 'Wu, GQ', ';\xa0', 'Hu, XG'],'论文摘要':['Entity Relation Extraction (ERE) is an important research topic in the field of information extraction However, to the best of our knowledge, only a few ERE works have been done for Chinese corpus. Because the syntactic features of Chinese sentences and English sentences are very different, existing algorithms for English corpus cannot be directly applied to Chinese corpus. Thus, in this paper, we propose a novel Chinese entity extraction system based on syntactic features (named SF-CERE). The basic idea of SF-CERE is given as follows. Firstly, we extract candidate relation triples based on verbs and verb nouns as relation keywords to avoid pre-defining relation types. Secondly, the triples are filtered using the positional constraints between relation keywords and entity pairs. Thirdly, we summarize four major Chinese syntactic features to expand the identified relation triples and improve accuracy. Finally, we use the method of relation transfer to mine and infer implicit relation triples. The experimental results on two real-world dataset (i.e., the encyclopedia dataset and the news dataset) show that SF-CERE effectively improves the quality of the relation triples and obtains good extraction performance.']}
{'论文标题':['One-class learning and concept summarization for data streams'],'论文作者':['Zhu, XQ', ';\xa0', 'Ding, W', ';\xa0', 'Zhang, CQ'],'论文摘要':['In this paper, we formulate a new research problem of concept learning and summarization for one-class data streams. The main objectives are to (1) allow users to label instance groups, instead of single instances, as positive samples for learning, and (2) summarize concepts labeled by users over the whole stream. The employment of the batch-labeling raises serious issues for stream-oriented concept learning and summarization, because a labeled instance group may contain non-positive samples and users may change their labeling interests at any time. As a result, so the positive samples labeled by users, over the whole stream, may be inconsistent and contain multiple concepts. To resolve these issues, we propose a one-class learning and summarization (OCLS) framework with two major components. In the first component, we propose a vague one-class learning (VOCL) module for concept learning from data streams using an ensemble of classifiers with instance level and classifier level weighting strategies. In the second component, we propose a one-class concept summarization (OCCS) module that uses clustering techniques and a Markov model to summarize concepts labeled by users, with only one scanning of the stream data. Experimental results on synthetic and real-world data streams demonstrate that the proposed VOCL module outperforms its peers for learning concepts from vaguely labeled stream data. The OCCS module is also able to rebuild a high-level summary for concepts marked by users over the stream.']}
{'论文标题':['Tensor Rank One Discriminant Analysis - A convergent method for discriminative multilinear subspace selection'],'论文作者':['Tao, DC', ';\xa0', 'Li, XL', ';\xa0', 'Maybank, S'],'论文摘要':['This paper proposes Tensor Rank One Discriminant Analysis (TR1DA) in which general tensors are input for pattern classification. TR1DA is based on Differential Scatter Discriminant Criterion (DSDC) and Tensor Rank One Analysis (TR1A). DSDC is a generalization of the Fisher discriminant criterion. It ensures convergence during training stage. TR1A is a method for adapting general tensors as input to DSDC. The benefits of TR1DA include: (1) a natural way of representing data without losing structure information, i.e., the information about the relative positions of pixels or regions; (2) a reduction in the small sample size problem which occurs in conventional discriminant learning because the number of training samples is much less than the dimensionality of the feature space; (3) a better convergence during the training procedure. We use a graph-embedding framework to generalize TR1DA in manifold learning-based feature selection algorithms, such as locally linear embedding, ISOMAP, and the Laplace eigenmap. We also kernelize TR1DA to nonlinear problems. TR1DA is then demonstrated to outperform traditional subspace methods, such as principal component analysis and linear discriminant analysis. (C) 2008 Elsevier B.V. All rights reserved.']}
{'论文标题':['K-Means Clustering with Bagging and MapReduce'],'论文作者':['Hai-Guang Li', ';\xa0', 'Gong-Qing Wu', ';\xa0', 'Xindong Wu'],'论文摘要':['Clustering is one of the most widely used techniques for exploratory data analysis. Across all disciplines, from social sciences over biology to computer science, people try to get a first intuition about their data by identifying meaningful groups among the data objects. K-means is one of the most famous clustering algorithms. Its simplicity and speed allow it to run on large data sets. However, it also has several drawbacks. First, this algorithm is instable and sensitive to outliers. Second, its performance will be inefficient when dealing with large data sets. In this paper, a method is proposed to solve those problems, which uses an ensemble learning method bagging to overcome the instability and sensitivity to outliers, while using a distributed computing framework MapReduce to solve the inefficiency problem in clustering on large data sets. Extensive experiments have been performed to show that our approach is efficient.']}
{'论文标题':['Distributed multi-dimensional hidden Markov models for image and trajectory-based video classifications'],'论文作者':['Ma, X', ';\xa0', 'Schonfeld, D', ';\xa0', 'Khokhar, A'],'论文摘要':['In this paper, we propose a novel multi-dimensional distributed hidden Markov model (DHMM) framework. We first extend the theory of 2D hidden Markov models (HMMs) to arbitrary causal multi-dimensional HMMs and provide the classification and training algorithms for this model. The proposed extension of causal multi-dimensional HMMs allows state transitions in arbitrary causal directions and neighbors. We subsequently generalize this framework further to non-causal models by distributing the non-causal models into multiple causal multi-dimensional HMMs. The proposed training and classification process consists of the extension of three fundamental algorithms to multi-dimensional causal systems, i.e. (1) Expectation-Maximization (EM) algorithm,(2) General Forward-Backward (GFB) algorithm; and (3) Viterbi algorithm. Simulation results performed using real-world images and videos demonstrate the superior performance, higher accuracy rate and promising applicability of the proposed DHMM framework.']}
{'论文标题':['Multi-pattern matching with variable-length wildcards using suffix tree'],'论文作者':['Liu, N', ';\xa0', 'Xie, F', ';\xa0', 'Wu, XD'],'论文摘要':['Multi-pattern matching with variable-length wildcards is an interesting and important problem in bioinformatics, information retrieval and other domains. Most of the previously developed multi-pattern matching methods, such as famous Aho-Corasick and Wu-Manber algorithms, aimed to solve some classical string matching problems. However, these algorithms are not efficient for patterns with flexible wildcards or do-not-care characters. In this paper, we propose two efficient algorithms for multi-pattern matching with variable-length wildcards based on suffix tree, called MMST-L and MMST-S, according to the length of exact characters in a pattern. Experimental results show that the two MMST algorithms, in most cases, outperform other various versions of comparing algorithms.']}
{'论文标题':['Design and Characterization of a Concentrated Winding based Two-Phase Brushless Exciter for Aircraft Wound-Rotor Synchronous Starter/Generator'],'论文作者':['Sun, CH', ';\xa0', 'Liu, WG', ';\xa0', 'Jiang, Y'],'论文摘要':["The paper proposes an alternative method to design two-phase exciter based on the existing single-phase exciter to solve the excitation problem for the main generator when the aircraft starter/generator starts up. Compared with distributed winding, concentrated winding has been adopted for the exciter's field winding for the advantages of shorter end-windings and available space for air duct in the stator which really meet the demand of aircraft machines on volume and heat dissipation. Initially a reconfiguration of the existing single-phase exciter into a two-phase exciter has been carried out to assess the potential of the existing exciter. The relationship between the number of the field winding's turns of single-phase exciter and two-phase exciter has been analyzed, on the basis of which electromagnetic design has been accomplished using FEA. Simulation results verify the reasonability of the analysis and confirm that the two-phase exciter is capable of generating the required current both in the start and generation modes. Finally, the characterization of the two-phase exciter has been studied to supply excitation control with theoretical support."]}
{'论文标题':['Method for analyzing psychological behavior based on memory rate and FP-growth, involves sorting association rules calculated, obtaining psychological behavior words associated with mental behavior words when mental behavior words appear'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Streaming feature-based causal structure learning algorithm with symmetrical uncertainty'],'论文作者':['Yang, J', ';\xa0', 'Guo, XX', ';\xa0', 'Yu, K'],'论文摘要':['Most existing causal structure learning algorithms must have access to the entire feature set of a dataset during the learning process. However, in many real-world applications, rather than having access to an entire feature set before learning begins, features are generated in an online manner. Learning and analyzing these dynamic features online is in high demand for effective decision-making. In this paper, by modeling these dynamic features as streaming features, we propose the CSSU algorithm, a streaming feature-based casual structure learning algorithm with symmetrical uncertainty. Specifically, the CSSU algorithm performs online updates of the candidate neighbor nodes of each feature seen so far using proposed definitions of the dependence relationship and pseudo-dependence relationship and adopts a constrained greedy search to obtain the final causal structure when no new features are available. The CSSU algorithm obtains candidate neighbors with symmetrical uncertainty to avoid subset searchs to execute the conditional independence (CI) test, which significantly reduces the time complexity. Using seven benchmark Bayesian networks, the experimental results show that the CSSU algorithm improves on other stateof-the-art causal structure leaning algorithms with regard to learning accuracy and time efficiency. (C) 2018 Published by Elsevier Inc.']}
{'论文标题':['Efficient runtime generation of association rules'],'论文作者':['Relue, R.', ';\xa0', 'Xindong Wu', ';\xa0', 'Hao Huang'],'论文摘要':['Mining frequent patterns in transaction databases has been a popular subject in data mining research. Common activities include finding patterns in database transactions, time series and exceptions. The Apriori algorithm is a widely accepted method of generating frequent patterns. The algorithm can require many scans of the database and can seriously tax resources. New methods of finding association rules, such as the Frequent Pattern Tree (FP-Tree) have improved performance, but still have problems when new data becomes available and require two scans of the database. We propose a new method, which requires only one scan of the database and supports update of patterns when new data becomes available. We design a new structure called Pattern Repository (PR), which stores all of the relevant information in a highly compact form and allows direct derivation of the FP-Tree and association rules quickly with a minimum of resources. In addition, it supports run-time generation of association rules by considering only those patterns that meet on-line data requirements.']}
{'论文标题':['Collaborative filtering with a deep adversarial and attention network for cross-domain recommendation'],'论文作者':['Liu, HT', ';\xa0', 'Guo, LL', ';\xa0', 'Wu, XD'],'论文摘要':['Cross-domain recommendation can alleviate the data sparsity problem in the target domain and has become a promising research area. Recently, various models have been proposed to provide recommendation across domains. Some models successfully embedded adversarial learning to reduce domain discrepancies between the source and target domains; however, these models only focuse on extracting the domain-shared features among multiple domains. In this paper, we devise a novel framework that considers both domain-shared and domain-specific knowledge across domains, namely, DAAN. Specifically, we tightly couple matrix factorization-based collaborative filtering with deep adversarial domain adaptation via an attention network. In this framework, we first learn the domain-specific representations for each user and each item from the source and target user-item interaction matrices. Then, we capture the domain-shared features between two domains with common user (or item) embeddings in a domain-adversarial paradigm. Additionally, an attention network is used to adjust the degree of importance between domain-shared and domain-specific knowledge. Extensive experiments on six tasks demonstrate that our method outperforms various baselines in terms of two ranking metrics. To the best of our knowledge, our proposed approach is the first deep model that considers both domain-shared and domain-specific knowledge across domains within a domain-adversarial training paradigm for cross-domain recommendation.', '(c) 2021 Elsevier Inc. All rights reserved.']}
{'论文标题':['Conceptual equivalence for contrast mining in classification learning'],'论文作者':['Yang, Y', ';\xa0', 'Wu, XD', ';\xa0', 'Zhu, XQ'],'论文摘要':["Learning often occurs through comparing. In classification learning, in order to compare data groups, most existing methods compare either raw instances or learned classification rules against each other. This paper takes a different approach, namely conceptual equivalence, that is, groups are equivalent if their underlying concepts are equivalent while their instance spaces do not necessarily overlap and their rule sets do not necessarily present the same appearance. A new methodology of comparing is proposed that learns a representation of each group's underlying concept and respectively cross-exams one group's instances by the other group's concept representation. The innovation is fivefold. First, it is able to quantify the degree of conceptual equivalence between two groups. Second, it is able to retrace the source of discrepancy at two levels: an abstract level of underlying concepts and a specific level of instances. Third, it applies to numeric data as well as categorical data. Fourth, it circumvents direct comparisons between (possibly a large number of) rules that demand substantial effort. Fifth, it reduces dependency on the accuracy of employed classification algorithms. Empirical evidence suggests that this new methodology is effective and yet simple to use in scenarios such as noise cleansing and concept-change learning. (c) 2008 Elsevier B.V. All rights reserved."]}
{'论文标题':['An Epidemic Model for News Spreading on Twitter'],'论文作者':['Abdullah, S', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we describe a novel approach to understand and explain news spreading dynamics on Twitter by using well-known epidemic models. Our underlying hypothesis is that the information diffusion on Twitter is analogous to the spread of a disease. As mathematical epidemiology has been extensively studied, being able to express news spreading as an epidemic model enables us to use a wide range of tools and procedures which have been proven to be both analytically rich and operationally useful. To further emphasize this point, we also show how we can readily use one of such tools - a procedure for detection of influenza epidemics, to detect change of trend dynamics on Twitter.']}
{'论文标题':['Hierarchical video content description and summarization using unified semantic and visual similarity'],'论文作者':['Zhu, XQ', ';\xa0', 'Fan, JP', ';\xa0', 'Wu, XD'],'论文摘要':['Video is increasingly the medium of choice for a variety of communication channels, resulting primarily from increased levels of networked multimedia systems. One way to keep our heads above the video sea is to provide summaries in a more tractable format. Many existing approaches are limited to exploring important low-level feature related units for summarization. Unfortunately, the semantics, content and structure of the video do not correspond to low-level features directly, even with closed-captions, scene detection, and audio signal processing. The drawbacks of existing methods are the following: (1) instead of unfolding semantics and structures within the video, low-level units usually address only the details, and (2) any important unit selection strategy based on low-level features cannot be applied to general videos. Providing users with an overview of the video content at various levels of summarization is essential for more efficient database retrieval and browsing. In this paper, we present a hierarchical video content description and summarization strategy supported by a novel joint semantic and visual similarity strategy. To describe the video content efficiently and accurately, a video content description ontology is adopted. Various video processing techniques are then utilized to construct a semi-automatic video annotation framework. By integrating acquired content description data, a hierarchical video content structure is constructed with group merging and clustering. Finally, a four layer video summary with different granularities is assembled to assist users in unfolding the video content in a progressive way. Experiments on real-word videos have validated the effectiveness of the proposed approach.']}
{'论文标题':['Fostering population-based cohort data discovery: The Maelstrom Research cataloguing toolkit'],'论文作者':['Bergeron, J', ' (', 'Bergeron, Julie', ') ', ';\xa0', 'Doiron, D', ' (', 'Doiron, Dany', ') ', ', ', ', ', ';\xa0', 'Marcon, Y', ' (', 'Marcon, Yannick', ') ', ';\xa0', 'Ferretti, V', ' (', 'Ferretti, Vincent', ') ', ';\xa0', 'Fortier, I', ' (', 'Fortier, Isabel', ') '],'论文摘要':['Background', 'The lack of accessible and structured documentation creates major barriers for investigators interested in understanding, properly interpreting and analyzing cohort data and biological samples. Providing the scientific community with open information is essential to optimize usage of these resources. A cataloguing toolkit is proposed by Maelstrom Research to answer these needs and support the creation of comprehensive and user-friendly study- and network-specific web-based metadata catalogues.', 'Methods', 'Development of the Maelstrom Research cataloguing toolkit was initiated in 2004. It was supported by the exploration of existing catalogues and standards, and guided by input from partner initiatives having used or pilot tested incremental versions of the toolkit.', 'Results', 'The cataloguing toolkit is built upon two main components: a metadata model and a suite of open-source software applications. The model sets out specific fields to describe study profiles; characteristics of the subpopulations of participants; timing and design of data collection events; and datasets/variables collected at each data collection event. It also includes the possibility to annotate variables with different classification schemes. When combined, the model and software support implementation of study and variable catalogues and provide a powerful search engine to facilitate data discovery.', 'Conclusions', 'The Maelstrom Research cataloguing toolkit already serves several national and international initiatives and the suite of software is available to new initiatives through the Maelstrom Research website. With the support of new and existing partners, we hope to ensure regular improvements of the toolkit.']}
{'论文标题':['Learning Kullback-Leibler Divergence-Based Gaussian Model for Multivariate Time Series Classification'],'论文作者':['Wu, GQ', ';\xa0', 'Zhang, HC', ';\xa0', 'Li, L'],'论文摘要':['The multivariate time series (MTS) classification is an important classification problem in which data has the temporal attribute. Because relationships between many variables of the MTS are complex and time-varying, existing methods perform not well in MTS classification with many attribute variables. Thus, in this paper, we propose a novel model-based classification method, called Kullback-Leibler Divergence-based Gaussian Model Classification (KLD-GMC), which converts the original MTS data into two important parameters of the multivariate Gaussian model: the mean vector and the inverse covariance matrix. The inverse covariance is the most important parameter, which can obtain the information between the variables. So that the more variables, the more information could be obtained by the inverse covariance, KLD-GMC can deal with the relationship between variables well in the MTS. Then the sparse inverse covariance of each subsequence is solved by Graphical Lasso. Furthermore, the Kullback-Leibler divergence is used as the similarity measurement to implement the classification of unlabeled subsequences, because it can effectively measure the similarity between different distributions. Experimental results on classical MTS datasets demonstrate that our method can improve the performance of multivariate time series classification and outperform the state-of-the-art methods.']}
{'论文标题':['A Key-phrase Extraction Method Based on Multi-size Convolution Windows for Scientific Literatures'],'论文作者':['Yuhong Zhang', ';\xa0', 'Yuxin Xie', ';\xa0', 'Xuegang Hu'],'论文摘要':['The key-phrase extraction is important for the downstream tasks in natural language process, and has attracted a lot of attention. Compared with other documents, scientific literatures contain many long phrases. Most existing methods perform poor on these literatures. To address this problem, a key-phrase extraction method based on multi-size convolution windows (KE-MCW) is proposed for scientific literatures in this paper. More specifically, in order to represent more contextual information, a convolutional neural network(CNN) with multi-size filters is introduced to map the documents into distributed feature vectors, then each vector can represent different size phrases. Next, in order to determine whether each word is a part of a keyphrase, a deep recurrent neural network is used to mark the role of each word. Finally, the attention mechanism is used to further judge the importance of each phrase. Experimental results show that our proposed method performs better than some competitive methods for technology literatures.']}
{'论文标题':['An object-oriented approach to manage multimedia documents'],'论文作者':['Papa, M.P.', ';\xa0', 'Giurleo, S.', ';\xa0', 'Molaro, G.'],'论文摘要':['In this paper we describe some aspects and issues related with the development of an Object-Oriented Multimedia Information System for an Office Environment which is being developed in the R&D laboratories of Bull HN Sud. The system provides integrated facilities for creating, filing, retrieving and browsing multimedia objects. This article is not an overview of the complete system but it makes a particular emphasis on some Object-Oriented features of the project. An Object-Oriented model of multimedia documents is presented which is based on concepts of Document Conceptual Structure and Document Type that both are very useful in the process of efficient information retrieval. Moreover, we discuss some design and implementation issues concerning the visual programming experience and the use of the Object-Oriented DBMS technology.']}
{'论文标题':['Computing and Pruning Method for Frequent Pattern Interestingness Based on Bayesian Networks'],'论文作者':['Hu Chun-Ling', ';\xa0', 'Wu Xin-Dong', ';\xa0', 'Hu Xue-Gang', ';\xa0', 'Yao Hong-Liang'],'论文摘要':['Based on background knowledge represented as a Bayesian network, this paper presents a BN-EJTR method that computes the interestingness of frequent items and frequent attributes, and prunes. BN-EJTR seeks to find inconsistent knowledge relative to background knowledge and to resolve the problems of un-interestingness and redundancy faced by frequent pattern mining. To deal with the demand of batch reasoning in Bayesian networks during computing interestingness, BN-EJTR provides a reasoning algorithm based on extended junction tree elimination for computing the support of a large number of items in a Bayesian network. In addition, BN-EJTR is equipped with a pruning mechanism based on a threshold for topological interestingness. Experimental results demonstrate that BN-EJTR has a good time performance compared with the same classified methods, and BN-EJTR also has effective pruning results. The analysis indicates that both the pruned frequent attributes and the pruned frequent items are un-interesting in respect to background knowledge.']}
{'论文标题':['An Optimization-based Truth Discovery Method with Claim Relation'],'论文作者':['Xia, JZ', ';\xa0', 'He, Y', ';\xa0', 'Wu, GQ'],'论文摘要':['With the advent of the era of big data, the information from multi-sources often conflicts due to that errors and fake information are inevitable. Therefore, how to obtain the most trustworthy or true information (i.e. truth) people need gradually becomes a troublesome problem. In order to meet this challenge, a novel hot technology named truth discovery that can infer the truth and estimate the reliability of the source without supervision has attracted more and more attention. However, most existing truth discovery methods only consider that the information is either same or different rather than the fine-grained relation between them, such as inclusion, support, mutual exclusion, etc. Actually, this situation frequently exists in real-world applications. To tackle the aforementioned issue, we propose a novel truth discovery method named OTDCR in this paper, which can handle the Erne-grained relation between the information and infer the truth more effectively through modeling the relation. In addition, a novel method of processing abnormal values is applied to the preprocessing of truth discovery, which is specially designed for categorical data with the relation Experiments in real dataset show our method is more effective than several outstanding methods.']}
{'论文标题':['Scalable inductive learning on partitioned data'],'论文作者':['Chen, QJ', ';\xa0', 'Wu, XD', ';\xa0', 'Zhu, XQ'],'论文摘要':['With the rapid advancement of information technology, scalability has become a necessity for learning algorithms to deal with large, real-world data repositories. In this paper, scalability is accomplished through a data reduction technique, which partitions a large data set into subsets, applies a learning algorithm on each subset sequentially or concurrently, and then integrates the learned results. Five strategies to achieve scalability (Rule-Example Conversion, Rule Weighting, Iteration, Good Rule Selection, and Data Dependent Rule Selection) are identified and seven corresponding scalable schemes are designed and developed. A substantial number of experiments have been performed to evaluate these schemes. Experimental results demonstrate that through data reduction some of our schemes can effectively generate accurate classifiers from weak classifiers generated from data subsets. Furthermore, our schemes require significantly less training time than that of generating a global classifier.']}
{'论文标题':['Welcoming two new Co-Editors-in-Chief for KAIS'],'论文作者':['Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':[]}
{'论文标题':['Multilayer change-point detection on stock order flows by wavelet transformation'],'论文作者':['Xiaoyan Liu', ';\xa0', 'Xindong Wu', ';\xa0', 'Yingfeng Wang'],'论文摘要':['In empirical finance, the increase or decrease in the number of stock buy/sell orders is aroused by the information asymmetry, which eventually affects the change of the stock price. To monitor the change in the stock order flow, we propose a multilayer change-point detection algorithm which makes use of the multi-resolution property of wavelet transformation. We first detect the change-points in the lower level resolutions of wavelet transforms and then map them back to the points in the original time series. Different weights are assigned to the different levels for computing the confidence of the mapped points to be the change-points in the original time series. The change-points obtained by our method are more reliable than the change-points detected only from the original time series. The experiments on both artificial Poisson sequences and real-world stock order flows from Shanghai Stock Exchange (SSE) show the effectiveness of our detection method.']}
{'论文标题':['Research on Joint Extraction of Triggers and Attribute-Value Pairs'],'论文作者':['Wang Yinghuan', ';\xa0', 'Xue Chan', ';\xa0', 'Wu Gongging'],'论文摘要':['Traditional attribute-value pair extraction methods are usually applied to short texts, and are limited to extract string attributes. In this work, a joint extraction of triggers and attribute-value pairs is proposed. The method not only can use triggers to obtain information sentences from long texts for identifying semantic attributes, but also can make full use of the interdependence among of trigger, attributes and values. Based on conditional random field a joint labeling model is Constructed to improve the extraction performance of string attribute-value pairs. Experimental results show that comparing with traditional methods, the proposed method can extract semantic attributes and improve the precision, recall and F-measure of string attributes by 15.3%, 15.5% and 15.5% respectively. At the same time, the average time of extraction is reduced by 76.29%.', '传统的属性值对抽取方法通常应用于短文本,且仅限于抽取字符串属性。提出一种触发词与属性值对的联合抽取方法,不仅能够通过识别触发词确定长文本中的信息语句,从而确定二元语义属性的取值,而且能够考虑触发词、字符串属性和属性值的相互依赖关系,基于条件随机场构建联合标记模型,提高字符串属性值对的抽取性能。实验结果显示,与传统方法相比,所提出的方法能够抽取二元语义属性值对,并且对字符串属性的抽取准确率、召回率和F值分别提高15.3%、15.5%和15.5%,同时抽取所用平均时间降低76.29%。']}
{'论文标题':['Incremental Mining of Across-streams Sequential Patterns in Multiple Data Streams'],'论文作者':['Shih-Yang Yang', ';\xa0', 'Ching-Ming Chao', ';\xa0', 'Po-Zung Chen', ';\xa0', 'Chu-Hao Sun'],'论文摘要':['Sequential pattern mining is the mining of data sequences for frequent sequential patterns with time sequence, which has a wide application. Data streams are streams of data that arrive at high speed. Due to the limitation of memory capacity and the need of real-time mining, the results of mining need to be updated in real time. Multiple data streams are the simultaneous arrival of a plurality of data streams, for which a much larger amount of data needs to be processed. Due to the inapplicability of traditional sequential pattern mining techniques, sequential pattern mining in multiple data streams has become an important research issue. Previous research can only handle a single item at a time and hence is incapable of coping with the changing environment of multiple data streams. In this paper, therefore, we propose the IAspam algorithm that not only can handle a set of items at a time but also can incrementally mine across-streams sequential patterns. In the process, stream data are converted into bitmap representation for mining. Experimental results show that the IAspam algorithm is effective in execution time when processing large amounts of stream data.']}
{'论文标题':['The representation and resolution of rough sets based on the extended concept lattice'],'论文作者':['Hu, XG', ';\xa0', 'Zhang, YH', ';\xa0', 'Wang, XY'],'论文摘要':['Rough set (RS) theory is a mathematics tool for handling uncertain problem. It is helpful for KDD, but expensive consumption of time and unclear expression of result are the main problem in practical application: The extended concept lattice (ECL) is a new form of concept lattice which is gotten by introducing equivalence intension into Galois concept lattice (GCL). The ECL is an efficient tool for data analysis and knowledge discovery in database (KDD). Both ECL and RS are based on equivalence class, so the relative between them exists. This paper describes the ECL first, then discusses the relation between the ECL and RS, and describes the implementation of rough set based on ECL.']}
{'论文标题':['A Bayesian discretizer for real-valued attributes'],'论文作者':['Wu, XD'],'论文摘要':['Discretization of real-valued attributes into nominal intervals has been an important area for symbolic induction systems because many real world classification tasks involve both symbolic and numerical attributes. Among various supervised and unsupervised discretization methods, the information gain-based methods have been widely used and cited. This paper designs a new discretization method, called the Bayesian discretizer, and compares its performance with the information gain methods implemented in C4.5 and HCV (Version 2.0). Over the seven datasets tested, the Bayesian discretizer has the best results of four of them in terms of predictive accuracy.']}
{'论文标题':['Sequential pattern mining with wildcards'],'论文作者':['Fei Xie', ';\xa0', 'Xindong Wu', ';\xa0', 'Ertian Hua'],'论文摘要':['Sequential pattern mining is an important research task in many domains, such as biological science. In this paper, we study the problem of mining frequent patterns from sequences with wildcards. The user can specify the gap constraints with flexibility. Given a subject sequence, a minimal support threshold and a gap constraint, we aim to find frequent patterns whose supports in the sequence are no less than the given support threshold. We design an efficient mining algorithm MAIL that utilizes the candidate occurrences of the prefix to compute the support of a pattern that avoids the rescanning of the sequence. We present two pruning strategies to improve the completeness and the time efficiency of MAIL. Experiments show that MAIL mines 2 times more patterns than one of its peers and the time performance is 12 times faster on average than its another peer.']}
{'论文标题':['Multi-view Visual Classification via a Mixed-norm Regularizer'],'论文作者':['Xiaofeng Zhu', ';\xa0', 'Zi Huang', ';\xa0', 'Xindong Wu'],'论文摘要':['In data mining and machine learning, we often represent instances by multiple views for better descriptions and effective learning. However, such comprehensive representations can introduce redundancy and noise. Learning with these multi-view data without any preprocessing may affect the effectiveness of visual classification. In this paper, we propose a novel mixed-norm joint sparse learning model to effectively eliminate the negative effect of redundant views and noisy attributes (or dimensions) for multi-view multi-label (MVML) classification. In particular, a mixed-norm regularizer, integrating a Frobenius norm and an l', '-norm, is embedded into the framework of joint sparse learning to achieve the design goals, which include selecting significant views, preserving the intrinsic view structure and removing noisy attributes from the selected views. Moreover, we devise an iterative algorithm to solve the derived objective function of the proposed mixed-norm joint sparse learning model. We theoretically prove that the objective function converges to its global optimum via the algorithm. Experimental results on challenging real-life datasets show the superiority of the proposed learning model over state-of-the-art methods.']}
{'论文标题':['Online Feature Selection for Streaming Features Using Self-Adaptation Sliding-Window Sampling'],'论文作者':['You, DL', ';\xa0', 'Wu, XD', ';\xa0', 'Lian, QS'],'论文摘要':['In recent years, online feature selection has been a research topic on streaming feature mining, as it can reduce the dimensionality of the streaming features by removing the irrelevant and redundant features in real time. There are many representative research efforts on the online feature selection with streaming features, i.e., alpha - investing, online streaming feature selection (OSFS), and scalable and accurate online approach (SAOLA) for feature selection. In these studies, alpha-investing has limited prediction accuracy and a large number of selected features. SAOLA sometimes offers outstanding efficiency in running time and prediction accuracy but possesses a large number of selected features. OSFS offers high prediction accuracy in many datasets, but its running time increases exponentially with an increasing number of features with low redundancy and high relevance. To address the limitations of the above-mentioned works, we propose an online learning algorithm named OSFASW, which samples streaming features in real-time by a self-adaption sliding-window and discards the irrelevant and redundant features by conditional independence. The OSFASW obtains an approximate Markov blanket with high prediction accuracy, meanwhile reducing the number of selected features. The efficiency of the proposed OSFASW algorithm was validated in a performance test on widely used datasets, e.g., NIPS2003 and causalityworkbench. Through the extensive experimental results, we demonstrate that OSFASW significantly improves the prediction accuracy and requires a smaller number of selected features than alpha - investing, OSFS, and SAOLA.']}
{'论文标题':['Hybrid collaborative recommendation of co-embedded item attributes and graph features'],'论文作者':['Dong, BB', ';\xa0', 'Zhu, Y', ';\xa0', 'Wu, XD'],'论文摘要':["In recent decades, personalized recommendation systems have attracted much attention from multiple disciplines for recommending interested products and services to users. Recommendation accentuates both the importance of feature learning tasks and the challenges posed by the sparsity of rating matrix. A common method for addressing the sparsity problem is to extend the feature space by the attributes of users and/or items. However, there are two main drawbacks in most existing recommendation methods. The first is the high computational cost of most existing recommendation models when using additional information from users and/or items to expand the feature space. The second problem is that it is difficult to obtain user additional information due to the high cost of acquiring tag knowledge and the increase in user privacy awareness. In this paper, we propose a novel and simple model to address the abovementioned issues, which employs a semi-autoencoder to co-embed the attributes and the graph features of the items for rating prediction (short for Item-Agrec). More specifially, a semi-autoencoder is introduced to learn the hidden nonlinear features of items for achieving a low computational cost, and thus the proposed Item-Agrec model can flexibly use side information from different sources. Meanwhile, in the case that it is not easy to obtain the user's additional information, we take the item's graph features and attributes into consideration for improving the accuracy of recommendation. Experiments on several real world datasets demonstrate the effectiveness of the proposed Item-Agrec compared with state-of-theart attribute-aware and content-aware methods.", '(c) 2021 Elsevier B.V. All rights reserved.']}
{'论文标题':['A Double-window-based Classification Algorithm for Concept Drifting Data Streams'],'论文作者':['Zhu Qun', ';\xa0', 'Zhang Yu-hong', ';\xa0', 'Li Pei-pei'],'论文摘要':['Tracking concept drifts in data streams has recently become a hot topic in data mining. Most of the existing work is built on a single-window-based mechanism to detect concept drifts. Due to the inherent limitation of the single-window-based mechanism, it is a challenge to handle different types of drifts. Motivated by this, a new classification algorithm based on a double-window mechanism for handling various concept drifting data streams (DWCDS) is proposed in this paper. In terms of an ensemble classifier in random decision trees, a double-window-based mechanism is presented to detect concept drifts periodically, and the model is updated dynamically to adapt to concept drifts. Extensive studies on both synthetic and real-word data demonstrate that DWCDS could quickly and efficiently detect concept drifts from streaming data, and the performance on the robustness to noise and the accuracy of classification is also improved significantly.', '数据流中概念漂移问题的研究已成为近年来流数据挖掘领域的研究热点之一.已有的研究工作多依据单窗口中错误率的变化来检测概念漂移,难以适应不同类型的漂移.为此,本文提出一种新的基于双层窗口机制的数据流分类算法 (Double-windows-based classification algorithm for concept drifting data streams,DWCDS),该算法采用随机决策树模型构建集成分类器,利用双层窗口机制周期性地检测滑动窗口中流数据分布的变化,并动态地更新模型以适应概念漂移.分析与实验结果表明:该算法可以快速有效地跟踪检测含噪数据流中的概念漂移,且抗噪性能与分类精度显著提高.']}
{'论文标题':['Multi-modal multi-view Bayesian semantic embedding for community question answering'],'论文作者':['Sang, L', ' (', 'Sang, Lei', ') ', ', ', ';\xa0', 'Xu, M', ' (', 'Xu, Min', ') ', ';\xa0', 'Qian, SS', ' (', 'Qian, ShengSheng', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':['Semantic embedding has demonstrated its value in latent representation learning of data, and can be effectively adopted for many applications. However, it is difficult to propose a joint learning framework for semantic embedding in Community Question Answer (CQA), because CQA data have multi-view and sparse properties. In this paper, we propose a generic Multi-modal Multi-view Semantic Embedding (MMSE) framework via a Bayesian model for question answering. Compared with existing semantic learning methods, the proposed model mainly has two advantages: (1) To deal with the multi-view property, we utilize the Gaussian topic model to learn semantic embedding from both local view and global view. (2) To deal with the sparse property of question answer pairs in CQA, social structure information is incorporated to enhance the quality of general text content semantic embedding from other answers by using the shared topic distribution to model the relationship between these two modalities (user relationship and text content). We evaluate our model for question answering and expert finding task, and the experimental results on two real-world datasets show the effectiveness of our MMSE model for semantic embedding learning. (C) 2018 Published by Elsevier B.V.']}
{'论文标题':['Synonymous Entity Recognition based on Feature Fusion'],'论文作者':['Cai, DS', ';\xa0', 'He, JJ', ';\xa0', 'Hu, XG'],'论文摘要':["It is necessary to unify data from different sources in Web information fusion as the same entity has different expressions in the real world. People usually use the information obtained from the search engine to make decisions in daily life. The idea of decision-making by means of search engines' information is used for synonymous entity recognition. In this paper, we propose a method of synonymous entity recognition based on feature fusion, which used multiplicative information fusion technique to fuse multi-results of synonymous entity recognition. In order to utilize the information obtained from the search engine, we combine entity name and other entity features as query words and design a new similarity function VarSim to measure entities' similarity. The F-score of algorithm on the basis of feature fusion on the two datasets are 13.42% and 1.81% higher than that of the recognition method on the basis of the entity name, which demonstrates the effectiveness of the synonymous entity recognition approach using feature fusion."]}
{'论文标题':['Mining Patterns in Big Data K-H Networks'],'论文作者':['Hamed, AA', ';\xa0', 'Wu, XD', ';\xa0', 'Fandy, T'],'论文摘要':['Can keyword-hashtag networks, derived from Big Data environments such as Twitter, yield clinicians a powerful tool to extrapolate patterns that may lead to development of new medical therapy and/or drugs? In our paper, we present a systematic network mining method to answer this question. We present HashnetMiner, a new pattern detection algorithm that operates on networks of medical concepts and hashtags. Concepts are selected from widely accessible databases (e.g., Medical Subject Heading [MeSH] descriptors, and Drugs.com), and hashtags are harvested from the associations with concepts that appear in tweets. The algorithm discerns promising discoveries that will be further explained in this paper. To the best of our knowledge, this is the first effort that uses Big Data networks mining to address such a question.']}
{'论文标题':['Exploratory Analysis of Protein Translation Regulatory Networks Using Hierarchical Random Graphs'],'论文作者':['Wu, DD', ';\xa0', 'Hu, XH', ';\xa0', 'He, TT'],'论文摘要':["Protein translation is a vital cellular process for any living organism. The availability of interaction databases provides an opportunity for researchers to exploit the immense amount of data in silico such as studying biological networks. There has been an extensive effort using computational methods in deciphering the transcriptional regulatory networks. However, research on translation regulatory networks has caught little attention in the bioinformatics and computational biology community. In this paper, we present an exploratory analysis of yeast protein translation regulatory networks using hierarchical random graphs. We derive a protein translation regulatory network from a protein-protein interaction dataset. Using a hierarchical random graph model, we show that the network exhibits well organized hierarchical structure. In addition, we apply this technique to predict missing links in the network. The results have potential implications for better understanding mechanisms of translational control from a system's perspective."]}
{'论文标题':['Keyphrase extraction from Chinese news web pages based on semantic relations'],'论文作者':['Xie, F', ';\xa0', 'Wu, X', ';\xa0', 'Wang, FY'],'论文摘要':['Keyphrases are very useful for saving time on browsing through the news web pages. A new keyphrase extraction method from Chinese news web pages based on semantic relations is presented in this paper. Semantic relations between phrases are analyzed, and a lexical chain is used to construct a semantic relation graph. Keyphrases are extracted and a semantic link graph is built on the lexical chains. News web pages with core hints are selected from www.163.com to test our method. The experimental results show that the proposed method substantially outperforms the method based on term frequency, especially when the number of keyphrases extracted is 3 - the precision is improved by 26.97 percent, and the recall is improved by 20.93 percent.']}
{'论文标题':['Max-Flow Rate Priority Algorithm for Evacuation Route Planning'],'论文作者':['Guo, D', ';\xa0', 'Gao, C', ';\xa0', 'Hu, XG'],'论文摘要':["Given a transportation network with multiple source vertexes, multiple destination vertexes, and a certain number of evacuees, it's significant to generate an optimal route schedule to complete evacuation in the shortest possible time. This paper proposes a heuristic approach to solve the evacuation route planning problem with capacity constrained transportation network. At first, we obtain multiple candidate routes. Each vertex in these candidate routes has available residual capacity on its predictive arrived time step. Then, a Max-Flow Rate Priority (MFRP) algorithm is designed to iteratively select the evacuation routes with maximum flow rate by time step marching. After that, MFRP updates the potential available capacity of the transportation network for the next iteration. MFRP improves the performance of the evacuation route planning as its high-velocity evacuee stream. The experimental results on different datasets indicate that the proposed approach can produce well-performed tactic for the evacuation route planning problem."]}
{'论文标题':['Knowledge reduction for decision tables with attribute value taxonomies'],'论文作者':['Ye, MQ', ';\xa0', 'Wu, XD', ';\xa0', 'Hu, DH'],'论文摘要':["Attribute reduction and attribute generalization are two basic methods for simple representations of knowledge. Attribute reduction can only reduce the number of attributes and is thus unsuitable for attributes with hierarchical domains. Attribute generalization can transform raw attribute domains into a coarser granularity by exploiting attribute value taxonomies (AVTs). As the control of how high an attribute should be generalized is typically quite subjective, it can easily result in over-generalization or under-generalization. This paper investigates knowledge reduction for decision tables with AVTs, which can objectively control the generalization process, and construct a reduced data set with fewer attributes and smaller attribute domains. Specifically, we make use of Shannon's conditional entropy for measuring classification capability for generalization and propose a novel concept for knowledge reduction, designated attribute-generalization reduct, which can objectively generalize attributes to maximize high levels while keep the same classification capability as the raw data. We analyze major relationships between attribute reduct and attribute-generalization reduct and prove that finding a minimal attribute-generalization reduct is an NP-hard problem and develop a heuristic algorithm for attribute-generalization reduction, namely, AGR-SCE. Empirical studies demonstrate that our algorithm accomplishes better classification performance and assists in computing smaller rule sets with better generalized knowledge compared with the attribute reduction method. (C) 2013 Elsevier B.V. All rights reserved."]}
{'论文标题':['Web News Extraction via Tag Path Feature Fusion Using DS Theory'],'论文作者':['Wu, GQ', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['Contents, layout styles, and parse structures of web news pages differ greatly from one page to another. In addition, the layout style and the parse structure of a web news page may change from time to time. For these reasons, how to design features with excellent extraction performances for massive and heterogeneous web news pages is a challenging issue. Our extensive case studies indicate that there is potential relevancy between web content layouts and their tag paths. Inspired by the observation, we design a series of tag path extraction features to extract web news. Because each feature has its own strength, we fuse all those features with the DS (Dempster-Shafer) evidence theory, and then design a content extraction method CEDS. Experimental results on both CleanEval datasets and web news pages selected randomly from well-known websites show that the F (1)-score with CEDS is 8.08% and 3.08% higher than existing popular content extraction methods CETR and CEPR-TPR respectively.']}
{'论文标题':['A Large Probabilistic Semantic Network Based Approach to Compute Term Similarity'],'论文作者':['Li, PP', ' (', 'Li, Peipei', ') ', ';\xa0', 'Wang, HX', ' (', 'Wang, Haixun', ') ', ';\xa0', 'Zhu, KQ', ' (', 'Zhu, Kenny Q.', ') ', ';\xa0', 'Wang, ZY', ' (', 'Wang, Zhongyuan', ') ', ', ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', '],'论文摘要':['Measuring semantic similarity between two terms is essential for a variety of text analytics and understanding applications. Currently, there are two main approaches for this task, namely the knowledge based and the corpus based approaches. However, existing approaches are more suitable for semantic similarity between words rather than the more general multi-word expressions (MWEs), and they do not scale very well. Contrary to these existing techniques, we propose an efficient and effective approach for semantic similarity using a large scale semantic network. This semantic network is automatically acquired from billions of web documents. It consists of millions of concepts, which explicitly model the context of semantic relationships. In this paper, we first show how to map two terms into the concept space, and compare their similarity there. Then, we introduce a clustering approach to orthogonalize the concept space in order to improve the accuracy of the similarity measure. Finally, we conduct extensive studies to demonstrate that our approach can accurately compute the semantic similarity between terms of MWEs and with ambiguity, and significantly outperforms 12 competing methods under Pearson Correlation Coefficient. Meanwhile, our approach is much more efficient than all competing algorithms, and can be used to compute semantic similarity in a large scale.']}
{'论文标题':['Online Feature Selection with Capricious Streaming Features: A General Framework'],'论文作者':['Wu, D', ' (', 'Wu, Di', ') ', ', ', ';\xa0', 'He, Y', ' (', 'He, Yi', ') ', ';\xa0', 'Luo, X', ' (', 'Luo, Xin', ') ', ', ', ';\xa0', 'Shang, MS', ' (', 'Shang, Mingsheng', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':['Online streaming feature selection has received extensive attention in the past few years. Existing approaches have a common assumption that the feature space of the fixed data instances increases dynamically without any missing entry. This assumption, however, does not always hold in many real-world applications. For example, in a credit evaluation system, we cannot collect the complete dynamic features for each person and/or enterprise. Motivated by this observation, this paper aims at conducting online feature selection from capricious streaming features, where features flow in one by one with some random missing entries while the number of data instances remains fixed. To do so, we propose a general framework named GF-CSF. The main idea of GF-CSF is to adopt latent factor analysis to preprocess capricious streaming features for completing their missing entries before conducting feature selection. Both theoretical and experimental analyses indicate that GF-CSF can efficiently improve any existing model of online streaming features selection to achieve online capricious streaming features selection.']}
{'论文标题':['Targeted aspects oriented topic modeling for short texts'],'论文作者':['He, J', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['Topic modeling has demonstrated its value in short text topic discovery. For this task, a common way adopted by many topic models is to perform a full analysis to find all the possible topics. However, these topic models overlook the importance of deeper topics, leading to confusing topics discovered. In practice, people always tend to find more focused topics on some special aspects (or events), rather than a set of coarse topics. Therefore, in this paper, we propose a novel method, Targeted Aspects Oriented Topic Modeling (TATM), to discover more focused topics on specific aspects in short texts. Specifically, each short text is assigned to only one targeted aspect derived from an enhanced Dirichlet Multinomial Mixture process (E-DMM). This process helps group similar words as many as possible, which achieves topic homogeneity. In addition, TATM discovers the topics for each targeted aspect from as many angles as possible by performing target-level modeling, which achieves topic completeness. Thus, TATM can make a balance between the two conflicting properties without employing any additional information or pre-trained knowledge. The extensive experiments conducted on five real-world datasets demonstrate that our proposed model can effectively discover more focused and complete topics, and it outperforms the state-of-the-art baselines.']}
{'论文标题':['Data Governance Technology'],'论文作者':['Wu Xin-Dong', ';\xa0', 'Dong Bing-Bing', ';\xa0', 'Du Xin-Zheng', ';\xa0', 'Yang Wei'],'论文摘要':['Along with the pervasiveness of information technology, the amount of data generated by human beings is growing at an exponential rate. Such massive data requires management with new methodologies. Data governance is the management of data for an organization (enterprise or government) as a strategic asset, from the collection of data to a set of management mechanisms for processing and applications, aiming to improve data quality, achieve a wide range of data sharing, and ultimately maximize the data value. Research and development on big data is nowadays popular in various domains, but big data governance is still in its infancy, and the decision-making of an organization cannot be separated from excellent data governance. This paper first introduces the concepts, developments, and necessity of data governance and big data governance, then analyzes existing data governance technologies-data specification, data cleaning, data exchange, and data integration, and also discusses the maturity measurement and framework design of data governance. Based on these introductions, analyses and reviews, the paper puts forward a \'AO governance" model for big data governance, which aims to facilitate HAO Intelligence with human intelligence (HI), artificial intelligence (AI), and organizational intelligence (OI), and then instantiates the \'AO governance" model with public security data governance as an example. Finally, the paper summarizes data governance with its challenges and opportunities.']}
{'论文标题':['A 2-Tier Clustering Algorithm with Map-Reduce'],'论文作者':['Jing Zhang', ';\xa0', 'Gongqing Wu', ';\xa0', 'Xindong Wu'],'论文摘要':['In the field of data mining, clustering is one of the important methods. K-Means is a typical distance-based clustering algorithm; 2-tier clustering should implement scalable clustering by means of dividing, sampling and knowledge integrating. Among those tools of distributed processing, Map-Reduce has been widely embraced by both academia and industry. Hadoop is an open-source parallel and distributed programming framework for the implementation of Map-Reduce computing model. With the analysis of the Map-Reduce paradigm of computing, we find that Hadoop parallel and distributed computing model is appropriate for the implementation of scalable clustering algorithm. This paper takes advantages of K-Means, 2-tier clustering mechanism and Map-Reduce computing model; proposes a new method for parallel and distributed clustering to explore distributed clustering problem based on Map-Reduce. The method aims to apply the clustering algorithm effectively to the distributed environment. The extensive studies demonstrate that the proposed algorithm is scalable, and the time performance is stable. Meanwhile, adding number of cluster nodes would improve the time performance of clustering.']}
{'论文标题':['A semi-random multiple decision-tree algorithm for mining data streams'],'论文作者':['Hu, XG', ';\xa0', 'Li, PP', ';\xa0', 'Wu, GQ'],'论文摘要':['Mining with streaming data is a hot topic in data mining. When performing classification on data streams, traditional classification algorithms based on decision trees, such as ID3 and C4.5, have a relatively poor efficiency in both time and space due to the characteristics of streaming data. There are some advantages in time and space when using random decision trees. An incremental algorithm for mining data streams, SRMTDS (Semi-Random Multiple decision Trees for Data Streams), based on random decision trees is proposed in this paper. SRMTDS uses the inequality of Hoeffding bounds to choose the minimum number of split-examples, a heuristic method to compute the information gain for obtaining the split thresholds of numerical attributes, and a Naive Bayes classifier to estimate the class labels of tree leaves. Our extensive experimental study shows that SRMTDS has an improved performance in time, space, accuracy and the anti-noise capability in comparison with VFDTc, a state-of-the-art decision-tree algorithm for classifying data streams.']}
{'论文标题':['Mining Recurring Concept Drifts with Limited Labeled Streaming Data'],'论文作者':['Li, PP', ';\xa0', 'Wu, XD', ';\xa0', 'Hu, XG'],'论文摘要':['Tracking recurring concept drifts in data streams is a significant and challenging issue for machine learning and data mining that frequently appears in real world stream classification problems. However, it has received little attention from the research community. Motivated by this, we propose a Semi-supervised classification algorithm for data streams with REcurring concept Drifts and Limited LAbeled data, called REDLLA, in which, a decision tree is adopted as the classification model. When growing a tree, a clustering algorithm based on k-Means is installed to produce concept clusters and label unlabeled data at leaves. In view of deviations between history concept clusters and new ones, potential concept drifts are distinguished and recurring concepts are maintained. Extensive studies on both synthetic and real-world data confirm the advantages of our REDLLA algorithm over two state-of-the-art online classification algorithms and several known online semi-supervised algorithms, even in the case with more than 90% unlabeled data.']}
{'论文标题':['A RANDOM DECISION TREE ENSEMBLE FOR MINING CONCEPT DRIFTS FROM NOISY DATA STREAMS'],'论文作者':['Li, PP', ';\xa0', 'Wu, XD', ';\xa0', 'Gao, YJ'],'论文摘要':['Detecting concept drifts and reducing the impact from the noise in real applications of data streams are challenging but valuable for inductive learning. It is especially a challenge in a light demand on the overheads of time and space. However, though a great number of inductive learning algorithms based on ensemble classification models have been proposed for handling concept drifting data streams, little attention has been focused on the detection of the diversity of concept drifts and the influence from noise in data streams simultaneously. Motivated by this, we present a new light-weighted inductive algorithm for concept drifting detection in virtue of an ensemble model of random decision trees (named CDRDT) to distinguish various types of concept drifts from noisy data streams in this article. We use variably small data chunks to generate random decision trees incrementally. Meanwhile, we introduce the inequality of Hoeffding bounds and the principle of statistical quality control to detect the different types of concept drifts and noise. Extensive studies on synthetic and real streaming data demonstrate that CDRDT could effectively and efficiently detect concept drifts from the noisy streaming data. Therefore, our algorithm provides a feasible reference framework of classification for concept drifting data streams with noise.']}
{'论文标题':['Strict pattern matching under non-overlapping condition'],'论文作者':['Wu, YX', ';\xa0', 'Shen, C', ';\xa0', 'Wu, XD'],'论文摘要':['Pattern matching (or string matching) is an essential task in computer science, especially in sequential pattern mining, since pattern matching methods can be used to calculate the support (or the number of occurrences) of a pattern and then to determine whether the pattern is frequent or not. A state-of-the-art sequential pattern mining with gap constraints (or flexible wildcards) uses the number of non-overlapping occurrences to denote the frequency of a pattern. Non-overlapping means that any two occurrences cannot use the same character of the sequence at the same position of the pattern. In this paper, we investigate strict pattern matching under the non-overlapping condition. We show that the problem is in P at first. Then we propose an algorithm, called NETLAP-Best, which uses Nettree structure. NETLAP-Best transforms the pattern matching problem into a Nettree and iterates to find the rightmost root-leaf path, to prune the useless nodes in the Nettree after removing the rightmost root-leaf path. We show that NETLAP-Best is a complete algorithm and analyse the time and space complexities of the algorithm. Extensive experimental results demonstrate the correctness and efficiency of NETLAP-Best.']}
{'论文标题':['TKDE guidelines for survey papers'],'论文作者':['Clifton, C', ';\xa0', 'Wu, XD', ';\xa0', 'Faloutsos, C'],'论文摘要':[]}
{'论文标题':['Rating Knowledge Sharing in Cross-Domain Collaborative Filtering'],'论文作者':['Li, B', ';\xa0', 'Zhu, XQ', ';\xa0', 'Zhang, CQ'],'论文摘要':['Cross-domain collaborative filtering (CF) aims to share common rating knowledge across multiple related CF domains to boost the CF performance. In this paper, we view CF domains as a 2-D site-time coordinate system, on which multiple related domains, such as similar recommender sites or successive time-slices, can share group-level rating patterns. We propose a unified framework for cross-domain CF over the site-time coordinate system by sharing group-level rating patterns and imposing user/item dependence across domains. A generative model, say ratings over site-time (ROST), which can generate and predict ratings for multiple related CF domains, is developed as the basic model for the framework. We further introduce cross-domain user/item dependence into ROST and extend it to two real-world cross-domain CF scenarios: 1) ROST (sites) for alleviating rating sparsity in the target domain, where multiple similar sites are viewed as related CF domains and some items in the target domain depend on their correspondences in the related ones; and 2) ROST (time) for modeling user-interest drift over time, where a series of time-slices are viewed as related CF domains and a user at current time-slice depends on herself in the previous time-slice. All these ROST models are instances of the proposed unified framework. The experimental results show that ROST (sites) can effectively alleviate the sparsity problem to improve rating prediction performance and ROST (time) can clearly track and visualize user-interest drift over time.']}
{'论文标题':['Co-occurrence pattern mining based on a biological approximation scoring matrix'],'论文作者':['Guo, D', ';\xa0', 'Yuan, EM', ';\xa0', 'Wu, XD'],'论文摘要':['Mining co-occurrence frequency patterns from multiple sequences is a hot topic in bioinformatics. Many seemingly disorganized constituents repetitively appear under different biological matrices, such as PAM250 and BLOSUM62, which are considered hidden frequent patterns (FPs). A hidden FP with both gap and flexible approximation operations (replacement, deletion or insertion) deepens the difficulty in discovering its true occurrences. To effectively discover co-occurrence FPs (Co-FPs) under these conditions, we design a mining algorithm (co-fp-miner) using the following steps: (1) a biological approximation scoring matrix is designed to discover various deformations of a single FP pattern; (2) a data-driven intersection tactic is used to generate candidate Co-FPs; (3) a deterministic Apriori-like rule is proposed to prune unnecessary Co-FPs; and (4) finally, we employ a backtracking matching scheme to validate true Co-FPs. The co-fp-miner algorithm is an unified framework for both exact and approximate mining on multiple sequences. Experiments on DNA and protein sequences demonstrate that co-fp-miner is more efficient on solutions, time and memory consumption than that of other peers.']}
{'论文标题':['Synonymous Entity Recognition based on Feature Fusion'],'论文作者':['Cai, DS', ' (', 'Cai, Desheng', ') ', ';\xa0', 'He, JJ', ' (', 'He, Jingjing', ') ', ';\xa0', 'Wu, GQ', ' (', 'Wu, Gongqing', ') ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') '],'论文摘要':["It is necessary to unify data from different sources in Web information fusion as the same entity has different expressions in the real world. People usually use the information obtained from the search engine to make decisions in daily life. The idea of decision-making by means of search engines' information is used for synonymous entity recognition. In this paper, we propose a method of synonymous entity recognition based on feature fusion, which used multiplicative information fusion technique to fuse multi-results of synonymous entity recognition. In order to utilize the information obtained from the search engine, we combine entity name and other entity features as query words and design a new similarity function VarSim to measure entities' similarity. The F-score of algorithm on the basis of feature fusion on the two datasets are 13.42% and 1.81% higher than that of the recognition method on the basis of the entity name, which demonstrates the effectiveness of the synonymous entity recognition approach using feature fusion."]}
{'论文标题':['Extremal Optimization-based Semi-Supervised Algorithm with Conflict Pairwise Constraints for Community Detection'],'论文作者':['Li, L', ';\xa0', 'Du, M', ';\xa0', 'Wu, GQ'],'论文摘要':['The research on community structure is a key to analyze the network functionality and topology, and thus it is significant to detect and analysis the community structure. During the abstract process from an actual system to a network, especially for a large-scale network, it is inevitable to have mistaken connections between nodes or have connection missing. In addition, in real applications, from time to time we can obtain prior information in the form of pairwise constraints between nodes besides topology information, although they may be inaccurate or conflicted. These noises in the network-related information will dramatically reduce the accuracy of community detection. Hence, in this paper, we introduce a dissimilarity index to determine the trustworthiness of pairwise constraints and settle the conflict of pairwise constraints. Then, focusing on the community detection with false connections or conflicted connections, we propose a pairwise constrained structure-enhanced extremal optimization-based semi-supervised algorithm (PCSEO-SS algorithm). Compared with existing semi-supervised community detection approaches, the experimental results executed on real networks and synthetic networks, show that PCSEO-SS can solve the problem of false connections or conflicted connections to some extent and detect the community structure more precisely.']}
{'论文标题':['A Bit-Parallel Algorithm for Sequential Pattern Matching with Wildcards'],'论文作者':['Guo, D', ';\xa0', 'Hong, XL', ';\xa0', 'Wu, XD'],'论文摘要':['Pattern matching with both gap constraints and the one-off condition is a challenging topic, especially in bioinformatics, information retrieval, and dictionary query. Among the algorithms to solve the problem, the most efficient one is SAIL, which is time consuming, especially when the pattern is long. In addition, existing algorithms based on bit-parallelism cannot handle a pattern that has only one pattern character between successive wildcards and the minimum local length constraints are zero. We propose an algorithm BPBM to handle online sequential pattern matching. In BPBM, an extended bit-parallelism operation is used to accelerate the matching process. An effective transition window mechanism with two nondeterministic finite state automatons (NFAs) is adopted to drop the useless scan window. It identifies gap constraints automatically and just scans once to export occurrences with exact match positions. Theoretical analysis and experimental results show that the BPBM algorithm is more competitive than other peers. It has an absolute advantage on search time complexity. It also has better stability that decreases operation costs with the increasing of the size of sequence alphabet or the length of the pattern. We also study off-line pattern matching. With twice pruning, left-most and right-most, we can increase the matching ratio about 2.08% on average.']}
{'论文标题':['Personalized News Filtering and Summarization on the Web'],'论文作者':['Wu, XD', ';\xa0', 'Xie, F', ';\xa0', 'Ding, W'],'论文摘要':["Information on the World Wide Web is congested with large amounts of news contents. Recommendation, filtering, and summarization of Web news have received much attention in Web intelligence, aiming to find interesting news and summarize concise content for users. In this paper, we present our research on developing the Personalized News Filtering and Summarization system (PNFS). An embedded learning component of PNFS induces a user interest model and recommends personalized news. A keyword knowledge base is maintained and provides a real-time update to reflect the general Web news topic information and the user's interest preferences. The non-news content irrelevant to the news Web page is filtered out. Keywords that capture the main topic of the news are extracted using lexical chains to represent semantic relations between words. An Example run of our PNFS system demonstrates the superiority of this Web intelligence system."]}
{'论文标题':['A matrix extension problem with entropy optimization'],'论文作者':['Fritzsche, B.', ';\xa0', 'Kirstein, B.'],'论文摘要':['An extension problem for partial Hermitian block matrices (some blocks specified, some free) is treated. The set of all solutions is represented as a matrix ball. The extension problem is associated with an entropy maximization procedure. The maximum of entropy is realized in the center of the permissible matrix ball. Some interesting applications are discussed.']}
{'论文标题':['Research on crowdsourcing truth inference method based on graph embedding'],'论文作者':['Liangzhu Zhou', ';\xa0', 'Xingrui Zhuo', ';\xa0', 'Xianyu Bao'],'论文摘要':['Crowdsourcing is a cheap and popular method to solve problems that are difficult for computers to handle. Due to the differences in ability among workers on crowdsourcing platforms, existing research use aggregation strategies to deal with the labels of different workers to improve the utility of crowdsourcing data. However, most of these studies are based on probabilistic graphical models, which have problems such as difficulty in setting initial parameters. This paper proposes a novel crowdsourcing method Truth Inference based on Graph Embedding (TIGE) for single-choice questions, the method draws on the idea of graph autoencoder, constructs feature vectors for each crowdsourcing task, embeds the relationship between crowdsourcing tasks and workers in graphs, then uses graph neural networks to convert crowdsourcing problems into graph node prediction problems. The feature vectors are continuously optimized in the convolutional layer to obtain the final result. Compared with the six state-of-the-art algorithms on real-world datasets, our method has significant advantages in accuracy and F1-score.']}
{'论文标题':['Dynamically Jointing character and word embedding for Chinese text Classification'],'论文作者':['Tang, XT', ' (', 'Tang, Xuetao', ') ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') ', ';\xa0', 'Li, PP', ' (', 'Li, Peipei', ') '],'论文摘要':['Chinese text classification is drawing attention in these few years. Different from English texts, there is no natural separator between Chinese words. With the development of deep learning, many character-level only models have been proposed for Chinese text classification to tackle this problem, which have achieved more success than word-level models. But the word information is also important for Chinese text representation, especially for short texts with less information. However, most of neural network models either just concatenate character-level representation and word-level representation, or use massive external knowledge to represent the whole text, which is complex and time-consuming. For better and easier representing the Chinese text without any external knowledge and using as much character and word information as possible, we propose a simple model jointed character and word embedding dynamically, called DJCW. Firstly, the character-level and word-level BiLSTM Model is introduced to extract features of texts with indefinite lengths. Secondly, the char and word are weightedly combined and the weights are changed dynamically. Finally, experiments conducted on five open-source text datasets show our model can handle the texts with different lengths and has achieved good stability results.']}
{'论文标题':['Study on Social Network Forensics'],'论文作者':['Wu Xin-Dong', ';\xa0', 'Li Ya-Dong', ';\xa0', 'Hu Dong-Hui'],'论文摘要':['Based on advances in computing technology and information technology, social networks have emerged as a new tool for people to exchange information and build interaction networks, and have become a key topic for social software studies in social computing. Social network forensics seeks to acquire, organize, analyze and visualize user information as direct, objective and fair evidence from a third-party perspective. Along with the rapid development of the Internet, social network forensics faces new challenges in dealing with user information being diverse, real-time and dynamic, huge in volume, and interactive, and also photo trustworthiness. It therefore has become a hot issue for opinion analysis, affective computing, content analysis in social networking relations, as well as individual, group and social behaviors in social networks and social computing. This paper designs a forensic model for social network forensics, and implements it on Sina microblogging. This model provides user information analysis, facial image recognition, and location presentation for trustworthiness analysis of digital evidence, and applies visualization to help reduce the difficulty of analysis and forensics on massive data from social networks.']}
{'论文标题':['Application of Fuzzy Dynamic Programming Model in the Coordination Analysis and Allocation of Regional Water Resources'],'论文作者':['Yu, XF', ';\xa0', 'Zhang, XW', ';\xa0', 'Yu, K'],'论文摘要':['In this paper, combining stage semi-structured decision-making fuzzy optimizing dynamic programming method and the coordination analysis of water resources system, the system coordination degree decision-making model of fuzzy optimizing dynamic programming in each stage is proposed. The model properties are given, and the algorithm is discussed with an example. Water resources of considering the system coordination degree and not considering the system coordination degree are separately carried out the optimal allocation, and the results have been compared and analyzed.']}
{'论文标题':['Semi-Supervised Representation Learning: Transfer Learning with Manifold Regularized Auto-encoders'],'论文作者':['Zhu, Y', ';\xa0', 'Hu, XG', ';\xa0', 'Li, PP'],'论文摘要':['The excellent performance of transfer learning has emerged in the past few years. How to find feature representations which minimizes the distance between source and target domain is the crucial problem in transfer learning. Recently, deep learning methods have been proposed to learn higher level and robust representation. However, in traditional methods, label information in source domain is not designed to optimize both feature representations and parameters of the learning model. Additionally, data redundance may incur performance degradation on transfer learning. To address these problems, we propose a novel semi-supervised representation learning framework for transfer learning. To obtain this framework, manifold regularization is integrated for the parameters optimization, and the label information is encoded using a softmax regression model in auto-encoders. Meanwhile, whitening layer is introduced to reduce data redundance before auto-encoders. Extensive experiments demonstrate the effectiveness of our proposed framework compared to other competing state-of-the-art baseline methods.']}
{'论文标题':['Scalable and Accurate Online Feature Selection for Big Data'],'论文作者':['Yu, K', ';\xa0', 'Wu, XD', ';\xa0', 'Pei, J'],'论文摘要':['Feature selection is important in many big data applications. Two critical challenges closely associate with big data. First, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Second, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a Scalable and Accurate OnLine Approach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintains a parsimonious model over time in an online manner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real datasets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on datasets of extremely high dimensionality and have superior performance over the state-of-the-art feature selection methods.']}
{'论文标题':['Plant Leaf Identification via a Growing Convolution Neural Network with Progressive Sample Learning'],'论文作者':['Zhao, ZQ', ';\xa0', 'Xie, BJ', ';\xa0', 'Wu, XD'],'论文摘要':['Plant identification is an important problem for ecologists, amateur botanists, educators, and so on. Leaf, which can be easily obtained, is usually one of the important factors of plants. In this paper, we propose a growing convolution neural network (GCNN) for plant leaf identification and report the promising results on the ImageCLEF2012 Plant Identification database. The GCNN owns a growing structure which starts training from a simple structure of a single convolution kernel and is gradually added new convolution neurons to. Simultaneously, the growing connection weights are modified until the squared-error achieves the desired result. Moreover, we propose a progressive learning method to determine the number of learning samples, which can further improve the recognition rate. Experiments and analyses show that our proposed GCNN outperforms other state-of-the-art algorithms such as the traditional CNN and the hand-crafted features with SVM classifiers.']}
{'论文标题':['Active Learning through Adaptive Heterogeneous Ensembling'],'论文作者':['Lu, ZY', ';\xa0', 'Wu, XD', ';\xa0', 'Bongard, JC'],'论文摘要':['An open question in ensemble-based active learning is how to choose one classifier type, or appropriate combinations of multiple classifier types, to construct ensembles for a given task. While existing approaches typically choose one classifier type, this paper presents a method that trains and adapts multiple instances of multiple classifier types toward an appropriate ensemble during active learning. The method is termed adaptive heterogeneous ensembles (henceforth referred to as AHE). Experimental evaluations show that AHE constructs heterogeneous ensembles that outperform homogeneous ensembles composed of any one of the classifier types, as well as bagging, boosting and the random subspace method with random sampling. We also show in this paper that the advantage of AHE over other methods is increased if (1) the overall size of the ensemble also adapts during learning; and (2) the target data set is composed of more than two class labels. Through analysis we show that the AHE outperforms other methods because it automatically discovers complementary classifiers: for each data instance in the data set, instances of the classifier type best suited for that data point vote together, while instances of the other, inappropriate classifier types disagree, thereby producing a correct overall majority vote.']}
{'论文标题':['Large-scale social network community detection method based-on high-order tensor decomposition comprises determining search characteristics of social network community information and utilizing certain program to perform corresponding data'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Competitive Entry of Information Goods Under Quality Uncertainty'],'论文作者':['Zhang, Z', ';\xa0', 'Nan, GF', ';\xa0', 'Tan, Y'],'论文摘要':["When confronted with a new product, consumers often find it difficult to predict how it will perform, and such uncertainty reduces consumers' willingness to adopt the product. In this paper, we consider a market whereby consumers decide when and which product to buy, given that they know the product quality of the incumbent but are uncertain about that of the entrant. We investigate how consumer uncertainty about product quality affects firms' behavior-based pricing and customer acquisition and retention dynamics. Using a two-period vertical model, we find that, under high-end encroachment, an increase in consumer uncertainty reduces the entrant's profit and hurts the incumbent's profit when the quality differential between the products is relatively small, whereas, under low-end encroachment, increasing uncertainty not only benefits the incumbent but also can favor the entrant. An important implication for entrants is that the marketing activities, which aim to reduce consumer uncertainty about product functionalities, may fail to improve profitability. We also find that the entrant lowers the price for uninformed customers and raises the price for repeat buyers under high-end encroachment but lowers the price for all customers under low-end encroachment. We further examine the subsidy strategy and show that, when the entrant's product has a significant quality advantage and consumer uncertainty is high but not very high, the optimal strategy for the entrant is to acquire all consumers who do not buy from the incumbent by providing subsidies and to drop the low-valuation customers by means of a high price after their uncertainty is resolved."]}
{'论文标题':['A Novel Variable Precision Reduction Approach to Comprehensive Knowledge Systems'],'论文作者':['Yang, C', ';\xa0', 'Liu, HB', ';\xa0', 'Wu, XD'],'论文摘要':['A comprehensive knowledge system reveals the intangible insights hidden in an information system by integrating information from multiple data sources in a synthetical manner. In this paper, we present a variable precision reduction theory, underpinned by two new concepts: 1) distribution tables and 2) genealogical binary trees. Sufficient and necessary conditions to extract comprehensive knowledge from a given information system are also presented and proven. A complete variable precision reduction algorithm is proposed, in which we introduce four important strategies, namely, distribution table abstracting, attribute rank dynamic updating, hierarchical binary classifying, and genealogical tree pruning. The completeness of our algorithm is proven theoretically and its superiority to existing methods for obtaining complete reducts is demonstrated experimentally. Finally, having obtaining the complete reduct set, we demonstrate how the relationships between the complete reduct set and the comprehensive knowledge system can be visualized in a double-layer lattice structure using Hasse diagrams.']}
{'论文标题':['Detecting and Assessing Anomalous Evolutionary Behaviors of Nodes in Evolving Social Networks'],'论文作者':['Wang, H', ';\xa0', 'Wu, J', ';\xa0', 'Wu, XD'],'论文摘要':["Based on the performance of entire social networks, anomaly analysis for evolving social networks generally ignores the otherness of the evolutionary behaviors of different nodes, such that it is difficult to precisely identify the anomalous evolutionary behaviors of nodes (AEBN). Assuming that a node's evolutionary behavior that generates and removes edges normally follows stable evolutionary mechanisms, this study focuses on detecting and assessing AEBN, whose evolutionary mechanisms deviate from their past mechanisms, and proposes a link prediction detection (LPD) method and a matrix perturbation assessment (MPA) method. LPD describes a node's evolutionary behavior by fitting its evolutionary mechanism, and designs indexes for edge generation and removal to evaluate the extent to which the evolutionary mechanism of a node's evolutionary behavior can be fitted by a link prediction algorithm. Furthermore, it detects AEBN by quantifying the differences among behavior vectors that characterize the node's evolutionary behaviors in different periods. In addition, MPA considers AEBN as a perturbation of the social network structure, and quantifies the effect of AEBN on the social network structure based on matrix perturbation analysis. Extensive experiments on eight disparate real-world networks demonstrate that analyzing AEBN from the perspective of evolutionary mechanisms is important and beneficial."]}
{'论文标题':['Discovering the k Representative Skyline Over a Sliding Window'],'论文作者':['Bai, M', ';\xa0', 'Xin, JC', ';\xa0', 'Wu, XD'],'论文摘要':['A representative skyline contains k skyline points that can represent its corresponding full skyline. The existing measuring criteria of k representative skylines are specifically designed for static data, and they cannot effectively handle streaming data. In this paper, we focus on the problem of calculating the k representative skyline over data streams. First, we propose a new criterion to choose k skyline points as the k representative skyline for data stream environments, termed the k largest dominance skyline (k-LDS), which is representative to the entire data set and is highly stable over the streaming data. Second, we propose an efficient exact algorithm, called Prefix-based Algorithm (PBA), to solve the k-LDS problem in a 2-dimensional space. The time complexity of PBA is only O((M - k) x k) where M is the size of the full skyline set. Third, the k-LDS problem for a d-dimensional (d >= 3) space turns out to be very complex. Therefore, a greedy algorithm is designed to answer k-LDS queries. To further accelerate the calculation, we propose a epsilon-greedy algorithm which can achieve an approximate factor of 1/(1+epsilon) (1 - 1/root e). Experimental results on both synthetic and real-world data show that our k-LDS significantly outperforms its competitors in data stream environments. Furthermore, we demonstrate that the proposed epsilon-greedy algorithm can solve epsilon-LDS efficiently and with a competitive accuracy.']}
{'论文标题':['Representation learning with collaborative autoencoder for personalized recommendation'],'论文作者':['Zhu, Y', ';\xa0', 'Wu, XD', ';\xa0', 'Li, Y'],'论文摘要':["In the past decades, recommendation systems have provided lots of valuable personalized suggestions for the users to address the problem of information over-loaded. Collaborative Filtering (CF) is one of the most commonly applied and successful recommendation approaches, which refers to using the preferences of groups with similar interests to recommend information to other users. Recently, in addition to the traditional matrix factorization techniques, deep learning methods have been proposed to learn more abstract and higher-level representations for recommendation. However, most previous deep recommendation methods learn the higher level feature representations of users and items through an identical model structure, which ignores the different characteristics of the user-based and item-based data. In addition, the rating matrix is usually sparse which may result in a significant degradation of recommendation performance. To address these problems, we propose a representation learning method with Collaborative Autoencoder for Personalized Recommendation (CAPR for short). In this method, user-based and item-based feature representations are learned by two different autoencoders for capturing different features of the data. Meanwhile, items' attributions are combined into the feature representations with semi-autoencoder for alleviating the sparsity problem. Extensive experimental results confirm the effectiveness of our proposed method compared to other state-of-the-art matrix factorization methods and deep recommendation methods."]}
{'论文标题':['Group Feature Selection with Streaming Features'],'论文作者':['Li, HG', ' (', 'Li, Haiguang', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ';\xa0', 'Li, Z', ' (', 'Li, Zhao', ') ', ';\xa0', 'Ding, W', ' (', 'Ding, Wei', ') '],'论文摘要':['Group feature selection makes use of structural information among features to discover a meaningful subset of features. Existing group feature selection algorithms only deal with pre-given candidate feature sets and they are incapable of handling streaming features. On the other hand, feature selection algorithms targeted for streaming features can only perform at the individual feature level without considering intrinsic group structures of the features. In this paper, we perform group feature selection with streaming features. We propose to perform feature selection at the group and individual feature levels simultaneously in a manner of a feature stream rather than a pre-given candidate feature set. In our approach, the group structures are fully utilized to reduce the cost of evaluating streaming features. We have extensively evaluated the proposed method. Experimental results have demonstrated that our proposed algorithms statistically outperform state-of-the-art methods of feature selection in terms of classification accuracy.']}
{'论文标题':['UE Computation Offloading Based on Task and Channel Prediction of Single User'],'论文作者':['Zhang, Z', ';\xa0', 'Cong, ZQ', ';\xa0', 'Tao, XF'],'论文摘要':['This paper established a partial computation of-floading paradigm to improve energy consumption and delay performance in single user scenario. This algorithm utilizes task and channel prediction to obtain future state to optimize objective function. A unique advantage of this algorithm is task and channel prediction utilizing markov property. The proposed algorithm utilizing task and channel prediction achieved more than 90% energy consumption reduction compared with no prediction. Finally, simulation achieved reduce in energy consumption are presented to corroborate the theoretical analysis as well as validate the effectiveness of the proposed algorithm. The algorithm proposed in this paper greatly improves the performance of energy-sensitive and delay-sensitive devices.']}
{'论文标题':['Online Learning in Variable Feature Spaces under Incomplete Supervision'],'论文作者':['He, Y', ';\xa0', 'Yuan, X', ';\xa0', 'Wu, XD'],'论文摘要':['This paper explores a new online learning problem where the input sequence lives in an over-time varying feature space and the ground-truth label of any input point is given only occasionally, making online learners less restrictive and more applicable. The crux in this setting lies in how to exploit the very limited labels to efficiently update the online learners. Plausible ideas such as propagating labels from labeled points to their neighbors through uncovering the point-wise geometric relations face two challenges: (1) distance measurement fails to work as different points may be described by disparate sets of features and (2) storing the geometric shape, which is formed by all arrived points, is unrealistic in an online setting. To address these challenges, we first construct a universal feature space that accumulates all observed features, making distance measurement feasible. Then, we use manifolds to represent the geometric shapes and approximate them in a sparse means, making manifolds computational and memory tractable in online learning. We frame these two building blocks into a regularized risk minimization algorithm. Theoretical analysis and empirical evidence substantiate the viability and effectiveness of our proposal.']}
{'论文标题':['An Empirical Study of the Noise Impact on Cost-Sensitive Learning'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD', ';\xa0', 'Shi, Y'],'论文摘要':['In this paper, we perform an empirical study of the impact of noise on cost-sensitive (CS) learning, through observations on how a CS learner reacts to the mislabeled training examples in terms of misclassification cost and classification accuracy. Our empirical results and theoretical analysis indicate that mislabeled training examples can raise serious concerns for cost-sensitive classification, especially when misclassifying some classes becomes extremely expensive. Compared to general inductive learning, the problem of noise handling and data cleansing is more crucial, and should be carefully investigated to ensure the success of CS learning.']}
{'论文标题':['Microblog oriented interest extraction with both content and network structure'],'论文作者':['Wang, H', ';\xa0', 'Huang, XL', ';\xa0', 'Li, L'],'论文摘要':["Microblog is an important social media platform, which is a microcosm of microblog users' real life, which it possible to obtain user's real intention and interests by identifying user interest from microblog. In the literature, most existing approaches for extracting user interests usually make use of only the information contained either in the textual posts, or in the social network structure of microblog. In this paper, we propose a systematic framework for interest extraction taking both the textual and social network information of microblog into account to get high quality tags. We first extract users' candidate interests based on the content of microblog, then propose a graph-based approach UNITE based on social network information for ranking user interest, finally introduce a more reasonable and objective metric for evaluation. Experimental results on Sina Weibo, one of the most popular microblog in China, demonstrate that our proposed approach makes dramatic improvements over state-of-the-art baselines."]}
{'论文标题':['AN APPROACH TO GENERATION OF SEMANTIC NETWORK FROM RELATIONAL DATABASE SCHEMA'],'论文作者':['WU, XD', ';\xa0', 'ZHANG, DC'],'论文摘要':[]}
{'论文标题':['Big data based user searching and matching method, involves calling multi-label classification algorithm model as base model, where algorithm model includes binary association model, classifier chain model and multi-label model'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Unsupervised Lifelong Learning with Curricula'],'论文作者':['He, Y', ';\xa0', 'Chen, S', ';\xa0', 'Wu, XD'],'论文摘要':['Lifelong machine learning (LML) has driven the development of extensive web applications, enabling the learning systems deployed on web servers to deal with a sequence of tasks in an incremental fashion. Such systems can retain knowledge from learned tasks in a knowledge base and seamlessly apply it to improve the future learning. Unfortunately, most existing LML methods require labels in every task, whereas providing persistent human labeling for all future tasks is costly, onerous, error-prone, and hence impractical. Motivated by this situation, we propose a new paradigm named unsupervised lifelong learning with curricula (ULLC), where only one task needs to be labeled for initialization and the system then performs lifelong learning for subsequent tasks in an unsupervised fashion. A main challenge of realizing this paradigm lies in the occurrence of negative knowledge transfer, where partial old knowledge becomes detrimental for learning a given task yet cannot be filtered out by the learner without the help of labels. To overcome this challenge, we draw insights from the learning behaviors of humans. Specifically, when faced with a difficult task that cannot be well tackled by our current knowledge, we usually postpone it and work on some easier tasks first, which allows us to grow our knowledge. Thereafter, once we go back to the postponed task, we are more likely to tackle it well as we are more knowledgeable now. The key idea of ULLC is similar - at any time, a pool of candidate tasks are organized in a curriculum by their distances to the knowledge base. The learner then starts from the closer tasks, accumulates knowledge from learning them, and moves to learn the faraway tasks with a gradually augmented knowledge base. The viability and effectiveness of our proposal are substantiated through extensive empirical studies on both synthetic and real datasets.']}
{'论文标题':['Multi-label relational classification via node and label correlation'],'论文作者':['Zhang, Z', ';\xa0', 'Wang, H', ';\xa0', 'Li, JY'],'论文摘要':['Multi-label classification on social network data deals with the problem of labeling nodes in the network (i. e. instances in the data set) with multiple classes. Existing connectivity-based approaches have been used in classification by exploiting the correlations between linked nodes. However, this popular strategy may not always perform well, as it ignores the neighborhood of nodes and the correlations between nodes and class labels. In this paper, we propose a novel multi-label relational classifier which exploits the correlations between nodes and class labels. We first identify similar nodes for each unlabeled node based on local network structure. Then we perform clustering on nodes with known labels. We introduce an aggregated class probability to capture the correlations between nodes and class labels based on the clustering results. Experiments with real-world datasets demonstrate that our proposed method improves classification performance comparing to the existing approaches. (c) 2018 Elsevier B.V. All rights reserved.']}
{'论文标题':['Research on rough set theory and applications in China'],'论文作者':['Gnoyin Wang', ';\xa0', 'Qinghua Zhang', ';\xa0', 'Houkuan Huang', ';\xa0', 'Dongyi Ye', ';\xa0', 'Qinghua Hu', ';\xa0', 'Xuegang Hu', ';\xa0', 'Zhongzhi Shi', ';\xa0', 'Yongli Li', ';\xa0', 'Lin Shang', ';\xa0', 'Liping An', ';\xa0', 'Ying Sai', ';\xa0', 'Shanben Chen', ';\xa0', 'Jiye Liang', ';\xa0', 'Keyun Qin', ';\xa0', 'Huanglin Zeng', ';\xa0', 'Kerning Xie', ';\xa0', 'Duoqian Miao', ';\xa0', 'Fan Min', ';\xa0', 'Zhaocong Wu', ';\xa0', 'Weizhi Wu', ';\xa0', 'Jianhua Dai'],'论文摘要':['This article gives a capsule view of research on rough set theory and applications ongoing at universities and laboratories in China. Included in this capsule view of rough set research is a brief description of the following things: Chinese research groups on rough set with their URLs for web pages, names of principal researchers (supervisors), numbers of graduate students, and topics being investigated. Statistical summaries showing the growth in the research on rough set theory and application in China are included. In addition, an introduction summarizing the research interests of Chinese researchers is included in this article. The contribution of this article is a complete overview of the principal research directions in rough set theory and its applications in China.']}
{'论文标题':['Feature selection with partition differentiation entropy for large-scale data sets'],'论文作者':['Li, FC', ';\xa0', 'Zhang, Z', ';\xa0', 'Jin, CX'],'论文摘要':['Feature selection, especially for large data sets, is a challenging problem in areas such as pattern recognition, machine learning and data mining. With the development of data collection and storage technologies, the data has become bigger than ever, thus making it difficult for learning from large data sets with traditional methods. In this paper, we introduce the partition differentiation entropy from the viewpoint of partition in rough sets to measure the significance and uncertainty of attributes, and present a feature selection method for large-scale data sets based on the information-theoretical measurement of attribute significance. Given a large-scale decision information system, the proposed method first divides it into small sub information systems according to the decision classes. Then by computing partition differentiation entropy in the sub-systems, the partition differentiation entropy of the attribute subset in the original decision information system is obtained. Accordingly, the important features are selected based on the value of partition differentiation entropy. The experimental results show that the idea of the proposed method is feasible and valid. (C) 2015 Elsevier Inc. All rights reserved.']}
{'论文标题':['Multi-bridge transfer learning'],'论文作者':['Hu, XG', ';\xa0', 'Pan, JH', ';\xa0', 'Zhang, YH'],'论文摘要':['Transfer learning, which aims to exploit the knowledge in the source domains to promote the learning tasks in the target domains, has attracted extensive research interests recently. The general idea of the previous approaches is to model the shared structure in one latent space as the bridge across domains by reducing the distribution divergences. However, there exist some latent factors in the other latent spaces, which can also be utilized to draw the corresponding distributions closer for establishing the bridges. In this paper, we propose a novel transfer learning method, referred to as Multi-Bridge Transfer Learning (MBTL), to learn the distributions in the different latent spaces together. Therefore, more latent factors shared can be utilized to transfer knowledge. Additionally, an iterative algorithm with convergence guarantee based on non-negative matrix tri-factorization techniques is proposed to solve the optimization problem. Comprehensive experiments demonstrate that MBTL can significantly outperform state-of-theart learning methods on the topic and sentiment classification tasks. (C) 2016 Elsevier B.V. All rights reserved.']}
{'论文标题':['Target object knowledge graph perfecting method, involves using combined knowledge graph for performing knowledge graph synthesis, and sending combined knowledge graph of target object to computing device'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Mixed reality lens based note maker application for taking down e.g. notes, has machine learning model for summarizing text, and mixed reality for merging real and virtual worlds to produce visualization environments'],'论文作者':[],'论文摘要':[]}
{'论文标题':['The research on application of Granular Transform in Classification based on Domain Knowledge'],'论文作者':['Zhang, J', ';\xa0', 'Hu, XG'],'论文摘要':["Granular transform of original data, can obtain the users' expected results in their interested level, and can decrease the size of data at the same time. So, it has become an important research issue in data mining area. In this paper, a model of granular transform is constructed based on domain knowledge. Then, it is integrated with classification rules learning. A generalization algorithm is proposed to learning classification rules, which is focusing on granular level based on domain knowledge. By defining generation operators of classification and using concept lattice, this method chooses the most compact generalization level and path to generalize the attributes controlled by misclassfication ratio based on certain mining tasks, and requests the users' interested knowledge from actual data in databases, in order to obtain the best classification rules. The experimentation shows the efficiency of this algorithm."]}
{'论文标题':['A semi-supervised bilingual lexicon induction method for distant language pairs based on bidirectional adversarial model'],'论文作者':['Wenwu Zhi', ';\xa0', 'Yuhong Zhang'],'论文摘要':["Bilingual lexicon induction (BLI) can transfer knowledgefrom well- to under- resourced language, and has been widelyapplied to various NLP tasks. Recent work on BLI is projection-based that learns a mapping to connect source and target embedding spaces, with the isomorphism assumption. Unfortunately, the isomorphism assumption doesn't hold gener-ally, especially in typologically distant language pairs. Moreover, without supervised signals guiding, the training will further com-plicates BLI, making the performance of unsupervised methods unsatisfactory. To broke the restrict of isomorphism, we propose a semi-supervised method for distant BLI tasks, named A Semi-supervised Bilingual Lexicon Induction method in Latent Space based on Bidirectional Adversarial Model. First, two latent spaces are learned by two autoencoders for source and target domain independently to weaken the constraint of isomorphism in the embedding spaces. Then we add a few pairs of dictionary to learn the initial mapping to connect the Latent Space. Last, based on initial mapping, Cycle-Consistency is combined with Distance constraint constraint to maintain the geometry structure of both embedding spaces stable in the learning of bi-direction mapping based on adversarial model. By conducting extensive experiments, our method gets state-of-the-art results on most language pairs, especially with significant improvements on distant language pairs."]}
{'论文标题':['Adaptive State Continuity-Based Sparse Inverse Covariance Clustering for Multivariate Time Series'],'论文作者':['Lei Li', ';\xa0', 'Wei Li', ';\xa0', 'Xuegang Hu'],'论文摘要':['Compared with univariate time series clustering, multivariate time series (MTS) clustering has become a challenging research topic on the data mining of time series. In this paper, a novel model-based approach Adaptive State Continuity-Based Sparse Inverse Covariance Clustering (ASCSICC) is proposed for MTS clustering. Specifically, the state continuity is introduced to make the traditional Gaussian mixture model (GMM) applicable to time series clustering. To prevent overfitting, the alternating direction method of multipliers (ADMM) is applied to optimize the parameter of GMM inverse covariance. In addition, the proposed approach simultaneously segments and clusters the time series. Technically, first, the adaptive state continuity is estimated based on the distance similarity of adjacent time series data. Then, a dynamic programming algorithm of cluster assignment by adaptive state continuity is taken as the E-step, and the ADMM for solving sparse inverse covariance is taken as the M-step. E-step and M-step are combined into an Expectation-Maximization (EM) algorithm to conduct the clustering process. Finally, we show the effectiveness of the proposed approach by comparing the ASC-SICC with several state-of-the-art approaches in experiments on two datasets from real applications.']}
{'论文标题':['Multi-fuzzy-objective graph pattern matching in big graph environments with reliability, trust and social relationship'],'论文作者':['Li, L', ';\xa0', 'Zhang, F', ';\xa0', 'Bu, CY'],'论文摘要':['With the advent of the era of big data, the scale of data has grown dramatically, and there is a close correlation between massive multi-source heterogeneous data, which can be visually depicted by a big graph. Big graph, especially from Web data, social networks, or biometric data, has attracted more and more attention from researchers, which usually contains complex relationships and multiple attributes. How to perform efficient query and matching on big graph data is the basic problem on analyzing big graph. Using multi-constrained graph pattern matching, we can design patterns that meet our specific requirements, and find matched subgraphs to locate the required patterns to accomplish specific tasks. So how to find matched subgraphs with good attributes in big graph becomes the key problem on big graph research. Considering the possibility that a node in a subgraph may fail due to reliability, in order to select more and better matched subgraphs, in this paper, we introduce fuzziness and reliability into multi-objective graph pattern matching, and then use a multi-objective genetic algorithm NSGA-II to find the subgraphs with higher reliability and better attributes including social trust and social relationship. Finally, a reliability-based multi-fuzzy-objective graph pattern matching method (named as RMFO-GPM) is proposed. The experimental results on real data sets show the effectiveness of the proposed RMFO-GPM method comparing with other state-of-art methods.']}
{'论文标题':['Three-Way Dicision Community Detection Algorithm Based on Local Group Information'],'论文作者':['Chen, J', ';\xa0', 'Fang, LD', ';\xa0', 'Hu, XG'],'论文摘要':["In network, nodes are joined together in tightly knit groups. Local group information is used to search the natural community. It can be crucial to help us to understand the functional properties of the networks and detect the true community structure. In this paper, we propose an algorithm called Three-way Decision Community Detection Algorithm based on Local Group Information(LGI-TWD) to detect community structure by using local group information. Firstly, we define sub-communities of each node v. Node v and v's neighbors which are reachable to each other construct one sub-communities of node v. Then, each sub-communities is regarded as a granular, and then hierarchical structure is constructed based on granulation coefficient. Finally, a further classification for boundary region's nodes can be done according to belonging degree. Compared with other community detection algorithms (N-TWD, CACDA, GN, NFA, LPA), the experimental results on six real world social networks show that LGI-TWD gets higher modularity value Q and more accurate communities."]}
{'论文标题':['Knowledge discovery in very large database'],'论文作者':['Nishio, S.'],'论文摘要':['Discusses regularity; IF-THEN rules; fact data; very large knowledge bases; machine learning; knowledge mining; scientific discovery; domain knowledge; positive and negative sets; over-generalization; nonmonotonic knowledge evolution; exception handling; defaults and overriding; abnormally predicates; probably approximately correct learning; tuple-oriented and attribute-oriented databases; parallel checks; functional dependency; concept hierarchy; characteristic rules and classification rules; learning from examples; generalization rules; dropping conditions; climbing the generalization tree; conceptual clustering; a knowledge discovery workbench; visualization; summarization; common-sense knowledge; decision trees; hypertext; the Management Information Base; parallel computing; the function-based induction algorithm; and knowledge discovery engineering.']}
{'论文标题':['Online Learning from Data Streams with Varying Feature Spaces'],'论文作者':['Beyazit, E', ';\xa0', 'Alagurajah, J', ';\xa0', 'Wu, XD'],'论文摘要':['We study the problem of online learning with varying feature spaces. The problem is challenging because, unlike traditional online learning problems, varying feature spaces can introduce new features or stop having some features without following a pattern. Other existing methods such as online streaming feature selection (Wu et al. 2013), online learning from trapezoidal data streams (Zhang et al. 2016), and learning with feature evolvable streams (Hou, Zhang, and Zhou 2017) are not capable to learn from arbitrarily varying feature spaces because they make assumptions about the feature space dynamics. In this paper, we propose a novel online learning algorithm OLVF to learn from data with arbitrarily varying feature spaces. The OLVF algorithm learns to classify the feature spaces and the instances from feature spaces simultaneously. To classify an instance, the algorithm dynamically projects the instance classifier and the training instance onto their shared feature subspace. The feature space classifier predicts the projection confidences for a given feature space. The instance classifier will be updated by following the empirical risk minimization principle and the strength of the constraints will be scaled by the projection confidences. Afterwards, a feature sparsity method is applied to reduce the model complexity. Experiments on 10 datasets with varying feature spaces have been conducted to demonstrate the performance of the proposed OLVF algorithm. Moreover, experiments with trapezoidal data streams on the same datasets have been conducted to show that OLVF performs better than the state-of-the-art learning algorithm (Zhang et al. 2016).']}
{'论文标题':['Junction Tree Factored Particle Inference Algorithm for Multi-Agent Dynamic Influence Diagrams'],'论文作者':['Yao, HL', ';\xa0', 'Chang, J', ';\xa0', 'Wang, H'],'论文摘要':['As MMDPs are difficult to represent structural relations among Agents and MAIDS can not model dynamic environment, we present Mufti-Agent dynamic influences (MADIDs). MADIDs have stronger knowledge representation ability and MADIDs may efficiently model dynamic environment and structural relations among Agents. Based on the hierarchical decomposition of MADIDs, a junction tree factored particle filter (JFP) algorithm is presented by combing the advantages of the junction trees and particle filter. JFP algorithm converts the distribution of MADIDs into the local factorial form, and the inference is performed by factor particle of propagation on the junction tree. Finally, and the results of algorithm comparison show that the error of JFP algorithm is obviously less than BK algorithm and PF algorithm without the loss of time performance.']}
{'论文标题':['Choice certainty in Discrete Choice Experiments: Will eye tracking provide useful measures?'],'论文作者':['Uggeldahl, K.', ';\xa0', 'Jacobsen, C.', ';\xa0', 'Lundhede, T.H.', ';\xa0', 'Olsen, S.B.'],'论文摘要':["In this study, we conduct a Discrete Choice Experiment (DCE) using eye tracking technology to investigate if eye movements during the completion of choice sets reveal information about respondents' choice certainty. We hypothesise that the number of times that respondents shift their visual attention between the alternatives in a choice set reflects their stated choice certainty. Based on one of the largest samples of eye tracking data in a DCE to date, we find evidence in favor of our hypothesis. We also link eye tracking observations to model-based choice certainty through parameterization of the scale function in a random parameters logit model. We find that choices characterized by more frequent gaze shifting do indeed exhibit a higher degree of error variance, however, this effects is insignificant once response time is controlled for. Overall, findings suggest that eye tracking can provide an observable and exogenous variable indicative of choice certainty, potentially improving the handling of respondent certainty and thus the performance of the choice models in DCEs. However, in our empirical case the benefits of using eye movement data as a proxy for choice certainty in the choice model are small at best, and our results suggest that response time provides a better proxy for stated choice certainty and provides larger improvements in model performance. [All rights reserved Elsevier]."]}
{'论文标题':['Imbalanced Networked Multi-Label Classification with Active Learning'],'论文作者':['Zhang, RL', ';\xa0', 'Li, L', ';\xa0', 'Bu, CY'],'论文摘要':['With the rapid development of social networks, the networked multi-label classification algorithms have gained wide attention. The existing networked multi-label classification algorithms mostly only consider the homogeneity or heterogeneity of the network without taking the imbalance of the network into account, and this is actually pretty common in real network environments, which deserves more attention. Moreover, the selection strategy of training set is very critical for multi-label classification algorithm, because it will directly affect both the parameter updating inside the classifier and the precision of the classifier. The application of active learning to the selection of training set can effectively improve the precision of the classifier. Similarly, the application of imbalanced data processing strategies to the selection of training sets also makes classifiers more suitable for imbalanced data networks. Thereout, we propose an algorithm BSHD (Block Sampling with selecting the Highest Degree nodes), which is an active learning based imbalanced networked multi-label classification algorithm. In this algorithm, we divide the network according to the edge density and utilize the oversampling and undersampling to dispose each block. Then we select the nodes with the highest degree from each block to form the training set. Experimental results show that our proposed BSHD outperforms other state-of-arts approaches.']}
{'论文标题':['Joint Feature Selection and Classification for Multilabel Learning'],'论文作者':['Huang, J', ';\xa0', 'Li, GR', ';\xa0', 'Wu, XD'],'论文摘要':['Multilabel learning deals with examples having multiple class labels simultaneously. It has been applied to a variety of applications, such as text categorization and image annotation. A large number of algorithms have been proposed for multilabel learning, most of which concentrate on multilabel classification problems and only a few of them are feature selection algorithms. Current multilabel classification models are mainly built on a single data representation composed of all the features which are shared by all the class labels. Since each class label might be decided by some specific features of its own, and the problems of classification and feature selection are often addressed independently, in this paper, we propose a novel method which can perform joint feature selection and classification for multilabel learning, named JFSC. Different from many existing methods, JFSC learns both shared features and label-specific features by considering pairwise label correlations, and builds the multilabel classifier on the learned low-dimensional data representations simultaneously. A comparative study with state-of-the-art approaches manifests a competitive performance of our proposed method both in classification and feature selection for multilabel learning.']}
{'论文标题':['Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation'],'论文作者':['Aliferis, CF', ' (', 'Aliferis, Constantin F.', ') ', ';\xa0', 'Statnikov, A', ' (', 'Statnikov, Alexander', ') ', ';\xa0', 'Tsamardinos, I', ' (', 'Tsamardinos, Ioannis', ') ', ';\xa0', 'Mani, S', ' (', 'Mani, Subramani', ') ', ';\xa0', 'Koutsoukos, XD', ' (', 'Koutsoukos, Xenofon D.', ') '],'论文摘要':['We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classification. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-defined sufficient conditions. In a first set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance.', 'Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distributions, types of classifiers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we find that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought.', 'In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show how local techniques can be used for scalable and accurate global causal graph learning.']}
{'论文标题':['Causal associative classification'],'论文作者':['Kui Yu', ';\xa0', 'Xindong Wu', ';\xa0', 'Hongliang Yao'],'论文摘要':['Associative classifiers have received considerable attention due to their easy to understand models and promising performance. However, with a high dimensional dataset, associative classifiers inevitably face two challenges: (1) how to extract a minimal set of strong predictive rules from an explosive number of generated association rules, and (2) how to deal with the highly sensitive choice of the minimal support threshold. In order to address these two challenges, we introduce causality into associative classification, and propose a new framework of causal associative classification. In this framework, we use causal Bayesian networks to bridge irrelevant and redundant features with irrelevant and redundant rules in associative classification. Without loss of prediction power, the feature space involved with the antecedent of a classification rule is reduced to the space of the direct causes, direct effects, and direct causes of the direct effects, a.k.a. the Markov blanket, of the consequent of the rule in causal Bayesian networks. The proposed framework is instantiated via baseline classifiers using emerging patterns. Experimental results show that our framework significantly reduces the model complexity while outperforming the other state-of-the-art algorithms.']}
{'论文标题':['A Model and an Algorithm to Mine Maximal Frequent Itemsets From Multidimensional Data Stream'],'论文作者':['Mao Guo-jun', ';\xa0', 'Sun Xiao-xi', ';\xa0', 'Zong Dong-jun'],'论文摘要':['In order to get valuable information, mining frequent itemsets from multidimensional data stream is needed. Through introduction of the concept of multidimensional item and multidimensional itemsets, the multidimensional data stream is expressed. A compact, compressed data structure MaxFP-Tree is designed to maintain multidimensional sets. Based on MaxFP-Tree, an incremental update algorithm to mine maximal frequent multidimensional itemsets is given. Experiment results show that the model and the algorithm of mining multidimensional data streams are efficient.', '为了挖掘到有价值的信息, 需要挖掘多维数据流上的频繁项目集, 因此引入多维项目和多维项目集的概念表示多维数据流上的项目.设计了一种紧凑、压缩的数据结构Max FP-Tree用于维护多维项目集, 并在 Max FP-Tree的基础上设计了挖掘多维数据流上最大频集的增量式更新算法.实验结果表明, 设计的挖掘多维数据流中最大频集的模型和算法是高效的.']}
{'论文标题':['A Hybrid Feature Selection Approach by Correlation-based Filters and SVM-RFE'],'论文作者':['Zhang, J', ';\xa0', 'Hu, XG', ';\xa0', 'Li, HZ'],'论文摘要':['Selecting a feature subset with strong discriminative power is a critical process for high dimensional data analysis, which has attracted much attention in many application domains, such as text categorization and genome projects. Since traditional feature selection methods provide limited contributions to classification, many researchers resort to hybrid or elaborate approaches to choose interesting features. In this paper, we propose a novel hybrid approach by correlation-based filters and Support Vector Machine-Recursive Feature Elimination (SVM-RFE) method for robust feature selection, which aims to yield robust results by aggregating multiple feature subsets (groups). Specifically, in the first stage, we incorporate correlation-based filters to identify Predominant Features and Complementary Features, and generate multiple groups for robustness; in the second stage, we aggregate multiple groups with SVM-RFE into a compact feature subset for high classification accuracy. Extensive experimental studies on both UCI data sets and microarray data sets have confirmed the effectiveness of our proposed approach.']}
{'论文标题':['Taking Advantage of Class-Specific Feature Selection'],'论文作者':['Pineda-Bautista, BB', ';\xa0', 'Carrasco-Ochoa, JA', ';\xa0', 'Martinez-Trinidad, JF'],'论文摘要':['In this work, a new method for class-specific feature selection, which selects a possible different feature subset for each class of a Supervised classification problem, is proposed. Since conventional classifiers do not allow using a different feature subset for each class, the use of a classifier ensemble and a new decision rule for classifying new instances are also proposed. Experimental results over different databases show that, using the proposed method, better accuracies than using traditional feature selection methods, are achieved.']}
{'论文标题':['Weighted Algorithm of Inductive Transfer Learning Based on Maximum Entropy Model'],'论文作者':['Yuhong Zhang', ';\xa0', 'Xuegang Hu', ';\xa0', 'Peipei Li'],'论文摘要':['Traditional machine learning and data mining algorithms mainly assume that the training and test data must be in the same feature space and follow the same distribution. However, in real applications, these two hypotheses are difficult to hold, traditional algorithms are hence no longer applicable. As a new framework of learning, transfer learning could solve this problem effectively. This paper focuses on one of important branches in this field, namely inductive transfer learning. Correspondingly, a weighted algorithm of inductive transfer learning, based on maximum entropy model, is proposed, called WTLME. It transfers the model parameters learned from the source domain to the target domain, and meanwhile adjusts the weights of instances in the target domain to obtain the model with high accuracy. Extensive studies demonstrate that our proposed algorithm of WTLME is more effective and efficient than traditional algorithms that require learning from scratch if the data distributions change. Moreover, WTLME is comparable to the previous transfer algorithm based on maximum entropy model.']}
{'论文标题':['Supervised Data Synthesizing and Evolving - A Framework for Real-World Traffic Crash Severity Classification'],'论文作者':['He, Y', ';\xa0', 'Wu, D', ';\xa0', 'Wu, XD'],'论文摘要':['Traffic crashes have threatened properties and lives for more than thirty years. Thanks to the recent proliferation of traffic data, the machine learning techniques have been broadly expected to make contributions in the traffic safety community due to their triumphs in many other domains. Among these contributions, the most cited method is to classify traffic crashes in different severities since they have significantly unequal occurrences and costs. However, considering the complexity of transportation system, the traffic data are usually highly imbalanced and lowly separable (HILS), so that few proposed works report satisfactory results. In this paper, we propose a novel framework to deal with the HILS traffic crash data. The framework comprises two parts. In part I, a novel Supervised Data Synthesizing and Evolving algorithm is proposed, which can properly represent the HILS data into a more balanced and separable form without altering the original data distribution. In part II, the details of a customized Multi-Layer Perceptron (MLP) are presented, serving the purpose of learning from the represented data with fast convergence and high accuracy. A real-world traffic crash dataset, as a benchmark, is employed to evaluate the classification performances of our framework and three state-of-the-art imbalanced learning algorithms. The experimental results validate that our framework significantly outperforms the other algorithms. Moreover, the impacts of various parameter settings are studied and discussed.']}
{'论文标题':['A survey on online feature selection with streaming features'],'论文作者':['Hu, XG', ';\xa0', 'Zhou, P', ';\xa0', 'Wu, XD'],'论文摘要':['In the era of big data, the dimensionality of data is increasing dramatically in many domains. To deal with high dimensionality, online feature selection becomes critical in big data mining. Recently, online selection of dynamic features has received much attention. In situations where features arrive sequentially over time, we need to perform online feature selection upon feature arrivals. Meanwhile, considering grouped features, it is necessary to deal with features arriving by groups. To handle these challenges, some state-of-the-art methods for online feature selection have been proposed. In this paper, we first give a brief review of traditional feature selection approaches. Then we discuss specific problems of online feature selection with feature streams in detail. A comprehensive review of existing online feature selection methods is presented by comparing with each other. Finally, we discuss several open issues in online feature selection.']}
{'论文标题':['An Effective Multilabel Classification Using Feature Selection'],'论文作者':['Sane, S.S.', ';\xa0', 'Chaudhari, P.', ';\xa0', 'Tidake, V.S.'],'论文摘要':['Recently, multilabel classification has received significant attention during the past years. A multilabel classification approach called coupled k-nearest neighbors algorithm for multilabel classification (called here as CK-STC) reported in the literature exploits coupled label similarities between the labels and provides improved performance [Liu and Cao in A Coupled k-Nearest Neighbor Algorithm for Multi-label Classification, pp. 176-187, 2015]. A multilabel feature selection is presented in Li et al. [Multi-label Feature Selection via Information Gain, pp. 346-355, 2014] and called as FSVIG here. FSVIG uses information gain that shows better performance when used with ML-NB, ML-kNN, and RandSvm when compared with existing multilabel feature selection algorithms.This paper investigates the performance of FSVIG when used with CK-STC and compares its performance with other multilabel feature selection algorithms available in MULAN using standard multilabel datasets. Experimental results show that FSVIG when used with CK-STC provides better performance in terms of average precision and one-error.']}
{'论文标题':['Iterative refinement of repeat sequence specification using constrained pattern matching'],'论文作者':['He, D', ';\xa0', 'Arslan, AN', ';\xa0', 'Wu, X'],'论文摘要':['Repeated sequences in genome are structures which indicate important biological functions such as protein binding. They are associated with various genetic diseases. We consider the problem of finding a specification for a "significant" repeating pattern in a given sequence. A significant pattern carries high amount of information, and it has many non-overlapping repeats. We propose for this problem, a method that takes as input an initial specification for a repeating pattern. A pattern is specified by a sequence of letters separated by varying length wildcards. The method presents to the user maximal occurrences for the current pattern specification in a way that no text symbol can be shared as a letter by two different pattern occurrences. This reduces the begin-end position-overlaps among different occurrences. The user modifies the specification manually to eliminate overlapping repeats. This process continues until a specification for a significant pattern is obtained.']}
{'论文标题':['Fuzzy interpretation of discretized intervals'],'论文作者':['Wu, XD'],'论文摘要':['When there are both numerical and nominal attributes in a database, existing data mining systems (such as rule induction and decision tree construction) discretize numerical domains into intervals and the discretized intervals are treated in a similar way to nominal values during induction. This correspondence describes a type of fuzzy intervals implemented in the HCV (Version 2.0) rule induction software (referred to as HCV hereafter) for the interpretation of rule induction results when rules with sharp intervals do not clearly apply to a test example at hand. A battery of experimental results with HCV show that these fuzzy intervals are useful.']}
{'论文标题':['Unsupervised Keyword Extraction Method based on Chinese Patent Clustering'],'论文作者':['Xie, YX', ';\xa0', 'Hu, XG', ';\xa0', 'Li, S'],'论文摘要':['Recently, patent data analysis has attracted a lot of attention, and patent keyword extraction is a hot problem. Most existing methods for patent keyword extraction are based on the frequency of words without semantic information. In this paper, we propose an Unsupervised Keyword Extraction Method (UKEM) based on Chinese patent clustering. More specifically, we use a Skip-gram model to train word embeddings based on a Chinese patent corpus. Then each patent is represented as a vector called patent vector. These patent vectors are clustered to obtain several cluster centroids. Next, the distance between each word vector in patent abstract and cluster centroid is computed to indicate the semantic importance of this word. The experimental results on several Chinese patent datasets show that the performance of our proposed method is better than several competitive methods.']}
{'论文标题':['Accelerating Learning Bayesian Network Structures by Reducing Redundant CI Tests'],'论文作者':['Wentao Hu', ';\xa0', 'Shuai Yang', ';\xa0', 'Kui Yu'],'论文摘要':['The type of constraint-based methods is one of the most important approaches to learn Bayesian network (BN) structures from observational data with conditional independence (CI) tests. In this paper, we find that existing constraint-based methods often perform many redundant CI tests, which significantly reduces the learning efficiency of those algorithms. To tackle this issue, we propose a novel framework to accelerate BN structure learning by reducing redundant CI tests without sacrificing accuracy. Specifically, we first design a CI test cache table to store CI tests. If a CI test has been computed before, the result of the CI test is obtained from the table instead of computing the CI test again. If not, the CI test is computed and stored in the table. Then based on the table, we propose two CI test cache table based PC (CTPC) learning frameworks for reducing redundant CI tests for BN structure learning. Finally, we instantiate the proposed frameworks with existing well-established local and global BN structure learning algorithms. Using twelve benchmark BNs, the extensive experiments have demonstrated that the proposed frameworks can significantly accelerate existing BN structure learning algorithms without sacrificing accuracy.']}
{'论文标题':['Stacked Convolutional Sparse Auto-Encoders for Representation Learning'],'论文作者':['Zhu, Y', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['Deep learning seeks to achieve excellent performance for representation learning in image datasets. However, supervised deep learning models such as convolutional neural networks require a large number of labeled image data, which is intractable in applications, while unsupervised deep learning models like stacked denoising auto-encoder cannot employ label information. Meanwhile, the redundancy of image data incurs performance degradation on representation learning for aforementioned models. To address these problems, we propose a semi-supervised deep learning framework called stacked convolutional sparse auto-encoder, which can learn robust and sparse representations from image data with fewer labeled data records. More specifically, the framework is constructed by stacking layers. In each layer, higher layer feature representations are generated by features of lower layers in a convolutional way with kernels learned by a sparse auto-encoder. Meanwhile, to solve the data redundance problem, the algorithm of Reconstruction Independent Component Analysis is designed to train on patches for sphering the input data. The label information is encoded using a Softmax Regression model for semi-supervised learning. With this framework, higher level representations are learned by layers mapping from image data. It can boost the performance of the base subsequent classifiers such as support vector machines. Extensive experiments demonstrate the superior classification performance of our framework compared to several state-of-the-art representation learning methods.']}
{'论文标题':['An intrusion detection model based on mining data streams'],'论文作者':['DongJun Zong', ';\xa0', 'GuoJun Mao', ';\xa0', 'XinDong Wu'],'论文摘要':['High speed, continuousness and infinity are the features in processing network data. With these characteristics, mining the data streams of network accesses is important and useful for discovering intrusion patterns. Based on data stream mining techniques, this paper proposes a new intrusion detection model that combines anomaly detection with misuse detection. Also, a new data structure named MaxFP-Tree and an efficient algorithm called ID-MaxFP are presented to provide the key solutions for finding maximal frequent itemsets from data streams. Experimental results show that these methods can achieve effective intrusion detection results and an efficient mining performance in time and space usages.']}
{'论文标题':['A Double-window-based Classification Algorithm For Concept Drifting Data Streams'],'论文作者':['Qun Zhu', ';\xa0', 'Xuegang Hu', ';\xa0', 'Xindong Wu'],'论文摘要':['Tracking concept drifts in data streams has recently become a hot topic in data mining. Most of the existing work is built on a single-window-based mechanism to detect concept drifts. Due to the inherent limitation of the single-window-based mechanism, it is a challenge to handle different types of drifts. Motivated by this, a new classification algorithm based on a double-window mechanism for handling various concept drifting data streams (named DWCDS) is proposed in this paper. In terms of an ensemble classifier in random decision trees, a double-window-based mechanism is presented to detect concept drifts periodically, and the model is updated dynamically to adapt to concept drifts. Extensive studies on both synthetic and real-word data demonstrate that DWCDS could quickly and efficiently detect concept drifts from streaming data, and the performance on the robustness to noise and the accuracy of classification is also improved significantly.']}
{'论文标题':['A feature selection approach suitable for data stream classification'],'论文作者':['Zhang Yu-hong', ';\xa0', 'Hu Xue-gang', ';\xa0', 'Yang Qiu-jie'],'论文摘要':['The problems of high-dimension, redundant features, and noise, which exist usually and simultaneously in the data stream, lead to long training time and low classification accuracy. An effective and real-time feature selection approach (feature selection-information value, FS-IV) was proposed for the data stream classification. In the FS-IV approach, a statistical index, the information value (IV), was introduced to measure the importance of features, and the feature was selected according to the threshold of IV value. Therefore, the FS-IV overcomes the problem of expensive cost of time and space and the problem of unobvious distinguishability in classical feature selection approaches in data stream. The experimental result shows that the FS-IV approach is little-cost and anti-noisy, and the FS-IV combined with the data stream classification approach can perform with notable shortened time while maintaining the accuracy.', '数据流环境下的高维、属性冗余、含噪音等问题是经常且可能同时存在的,在一定程度上影响了数据流的分类效果.为改善这一现状,提出一种快速、有效的数据流特征选择方法.引入统计指标IV(information value)值作为特征重要度的评价标准,在此基础上依据经验阈值来进行特征选择,从而解决了传统特征选择方法时空效率不高、区分度不明显、难以应用数据流的问题.实验结果表明:FS-IV具有较小的时间开销和较好的抗噪性能,该方法与已有的数据流分类模型相结合,在保证分类精度可比的情况下,能显著提高时空性能.']}
{'论文标题':['Ensemble Classification Method Based on Truth Discovery'],'论文作者':['Jin, YX', ';\xa0', 'Yang, Z', ';\xa0', 'Wu, GQ'],'论文摘要':['Classification is a hot topic in such fields as machine learning and data mining. The traditional approach of machine learning is to find a classifier closest to the real classification function, while ensemble classification is to integrate the results of base classifiers, then make an overall prediction. Compared to using a single classifier, ensemble classification can significantly improve the generalization of the learning system in most cases. However, the existing ensemble classification methods rarely consider the weight of the classifier, and there are few methods to consider updating the weights dynamically. In this paper, we are inspired by the idea of truth discovery and propose a new ensemble classification method based on the truth discovery (named ECTD). As far as we know, we are the first to apply the idea of truth discovery in the field of ensemble learning. Experimental results demonstrate that the proposed method performs well in ensemble classification.']}
{'论文标题':['Fast Orthogonal Nonnegative Matrix Tri-Factorization for Simultaneous Clustering'],'论文作者':['Li, Z', ' (', 'Li, Zhao', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ';\xa0', 'Lu, ZY', ' (', 'Lu, Zhenyu', ') '],'论文摘要':['Orthogonal Nonnegative Matrix Tri-Factorization (ONMTF), a dimension reduction method using three small matrices to approximate an input data matrix, clusters the rows and columns of an input data matrix simultaneously. However, ONMTF is computationally expensive due to an intensive computation of the Lagrangian multipliers for the orthogonal constraints. In this paper, we introduce Fast Orthogonal Nonnegative Matrix Tri-Factorization (FONT), which uses approximate constants instead of computing the Lagrangian multipliers. As a result, FONT reduces the computational complexity significantly. Experiments on document datasets show that FONT outperforms ONMTF in terms of clustering quality and running time. Moreover, FONT is further accelerated by incorporating Alternating Least Squares, and can be much faster than ONMTF.']}
{'论文标题':['Simplification and explication in Korean-Chinese translation: A newspaper editorial corpus-based study', '한중 번역의 단순화와 명시화 연구 ― 신문사설 코퍼스를 중심으로 ―'],'论文作者':['Kim,, Haerhim'],'论文摘要':['This paper tests two translation universal hypotheses, simplification and explication on the basis of a Korean-Chinese newspaper editorial translation corpus and a comparable native Chinese newspaper editorial corpus. The lexical density (the proportion of lexical words) and the lexical-function word ratio are calculated to verify simplification hypothesis.  Also, the function-total word ratio and the frequencies of conjunctions are examined to verify explication hypothesis. The analysis shows that the lexical density and the lexical-function word ratio in Korean-Chinese translation are significantly lower than in native Chinese.  A significantly greater ratio of function over total words and the tendency to use conjunctions more frequently are found in Korean-Chinese translation than in native Chinese. These findings provide evidence in favor of the simplification and explication hypotheses.']}
{'论文标题':['THE REFINEMENT OF PROBABILISTIC RULE SETS FOR CLASSIFICATION EXPERT-SYSTEMS - THE COMBINED OPTIMIZATION METHOD'],'论文作者':['MA, Y', ';\xa0', 'WILKINS, DC'],'论文摘要':["Expert shells that support classification problem solving usually allow the use of an uncertain reasoning method that assumes some degree of conditional independence between observations. As a consequence, probabilistic rules can behave in an undesirable manner and thereby decrease the expert system's performance with respect to classification accuracy. This paper describes one type of undesirable phenomenon, called sociopathicity, and describes results related to the induction and refinement of probabilistic rule sets with this property. A knowledge base is said to be sociopathic if additions to the knowledge base degrade problem solving performance independent of computational resources. The paper then presents an improved method of minimizing the error rate for sociopathic probabilistic rule sets. The refinement method, called Socio-Reducer2, is based on static and dynamic aspects of the probabilistic rules. Experimental results in a medical diagnosis domain show that it can reduce the diagnosis error rate by a reasonable margin."]}
{'论文标题':['Three-layer concept drifting detection in text data streams'],'论文作者':['Zhang, YH', ';\xa0', 'Chu, G', ';\xa0', 'Wu, XD'],'论文摘要':['Text data streams have widely appeared in real-world applications, in which, concept drifts owe a significant challenge for classification. Compared with relational data streams, concept drifts hidden in text streams usually reflect in the relationship between the feature vector and the instance labels. Meanwhile, existing concept drifting detection methods are mainly based on error rates of classification. When applying these methods in text streams, they perform poorly in the evaluations of false alarms and missing detections, etc. Motivated by this, we firstly give a systematic analysis of the concept drifts in text data streams. Then, we propose a three-layer concept drifting detection approach, where the three layers indicate the layer of label space, the layer of feature space and the layer of the mapping relationships between labels and features, respectively. In this approach, the latter two layers are based on the values of WoE (Weight of Evidence) and the IV (Information Value) index. Experimental results show that our approach can improve the performance of concept drifting detection and the accuracy of classification, especially when concept drifts in text data streams are frequent. (C) 2017 Elsevier B.V. All rights reserved.']}
{'论文标题':['Method for diagnosing fault based on self-adaptive manifold embedded dynamic distribution alignment, comprises collecting bearing vibration signal under different working condition and obtaining optimal solution of model coefficient vector'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Image Annotation By Multiple-Instance Learning With Discriminative Feature Mapping and Selection'],'论文作者':['Hong, RC', ' (', 'Hong, Richang', ') ', ';\xa0', 'Wang, M', ' (', 'Wang, Meng', ') ', ';\xa0', 'Gao, Y', ' (', 'Gao, Yue', ') ', ';\xa0', 'Tao, DC', ' (', 'Tao, Dacheng', ') ', ', ', ';\xa0', 'Li, XL', ' (', 'Li, Xuelong', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', '],'论文摘要':['Multiple-instance learning (MIL) has been widely investigated in image annotation for its capability of exploring region-level visual information of images. Recent studies show that, by performing feature mapping, MIL can be cast to a single-instance learning problem and, thus, can be solved by traditional supervised learning methods. However, the approaches for feature mapping usually overlook the discriminative ability and the noises of the generated features. In this paper, we propose an MIL method with discriminative feature mapping and feature selection, aiming at solving this problem. Our method is able to explore both the positive and negative concept correlations. It can also select the effective features from a large and diverse set of low-level features for each concept under MIL settings. Experimental results and comparison with other methods demonstrate the effectiveness of our approach.']}
{'论文标题':["Knowledge Graph for China's Genealogy"],'论文作者':['Wu, XD', ';\xa0', 'Jiang, TT', ';\xa0', 'Bu, CY'],'论文摘要':['Genealogical knowledge graphs depict the relationships of family networks and the development of family histories. They can help researchers to analyze and understand genealogical data, search for genealogical roots, and explore the origins of a family more easily. However, the multi-type, multi-source dynamic changes and specialized nature of genealogical data bring challenges to the development of contemporary knowledge graph models. Applying existing methods to genealogical data can result in problems of overlooking certain specialized vocabulary and dynamic properties such as personal experiences. In this paper, we propose a genealogical knowledge graph model GKGM that combines HAO intelligence (human intelligence + artificial intelligence + organizational intelligence) and ontology granularity division technology to address the above problems. Furthermore, a method of applying the model to construct genealogical knowledge graphs is demonstrated, and an experiment conducted on a real-world genealogical dataset verifies the feasibility and effectiveness of the model.']}
{'论文标题':['Stochastic Optimization for Market Return Prediction Using Financial Knowledge Graph'],'论文作者':['Fu, XY', ';\xa0', 'Ren, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['Interactive prediction of financial instrument returns is important. It is needed for asset managers to generate trading strategies as well as for stock exchange regulators to discover pricing anomalies. In this paper, we introduce an integrated stochastic optimization technique, namely genetic programming (GP) with generalized crowding (GC), GP+GC. GP+GC is as an integrated method for market return prediction, using a financial knowledge graph (KG). On the one hand, using time-series data for twenty-nine component stocks of the Dow Jones industrial average, we show that our stochastic optimization method can give strong prediction performance by providing a comparison of its return performances with two traditional benchmarks, namely a Buy & Hold strategy and the Moving Average Convergence Divergence (MACD) technical indicator. On the other hand, we use features extracted from a time-evolving knowledge graph constructed from fifty component stocks of the Shanghai Stock Exchange SSE50 index. These features are used by our GP+GC variant and then expression learnt by GP+GC are extracted into a KG. Overall, this work demonstrates how to integrate GP+GC with KGs in a powerful manner.']}
{'论文标题':['Cooperative Sparse Representation in Two Opposite Directions for Semi-Supervised Image Annotation'],'论文作者':['Zhao, ZQ', ';\xa0', 'Glotin, H', ';\xa0', 'Wu, XD'],'论文摘要':['Recent studies have shown that sparse representation (SR) can deal well with many computer vision problems, and its kernel version has powerful classification capability. In this paper, we address the application of a cooperative SR in semi-supervised image annotation which can increase the amount of labeled images for further use in training image classifiers. Given a set of labeled (training) images and a set of unlabeled (test) images, the usual SR method, which we call forward SR, is used to represent each unlabeled image with several labeled ones, and then to annotate the unlabeled image according to the annotations of these labeled ones. However, to the best of our knowledge, the SR method in an opposite direction, that we call backward SR to represent each labeled image with several unlabeled images and then to annotate any unlabeled image according to the annotations of the labeled images which the unlabeled image is selected by the backward SR to represent, has not been addressed so far. In this paper, we explore how much the backward SR can contribute to image annotation, and be complementary to the forward SR. The co-training, which has been proved to be a semi-supervised method improving each other only if two classifiers are relatively independent, is then adopted to testify this complementary nature between two SRs in opposite directions. Finally, the co-training of two SRs in kernel space builds a cooperative kernel sparse representation (Co-KSR) method for image annotation. Experimental results and analyses show that two KSRs in opposite directions are complementary, and Co-KSR improves considerably over either of them with an image annotation performance better than other state-of-the-art semi-supervised classifiers such as transductive support vector machine, local and global consistency, and Gaussian fields and harmonic functions. Comparative experiments with a nonsparse solution are also performed to show that the sparsity plays an important role in the cooperation of image representations in two opposite directions. This paper extends the application of SR in image annotation and retrieval.']}
{'论文标题':['Semantic Features Prediction for Pulmonary Nodule Diagnosis Based on Online Streaming Feature Selection'],'论文作者':['Yang, J', ';\xa0', 'Li, N', ';\xa0', 'Chen, Y'],'论文摘要':['Early diagnosis significantly improves the survival rate in lung carcinoma patients. This study attempts to construct a predictive network between the computational features and semantic features of pulmonary nodules using online feature selection and causal structure learning. In this paper, we exploit the causal discovery based on the streaming feature algorithm and causal discovery with symmetrical uncertainty based on the streaming feature algorithm. Different from the traditional learning methods that usually obtain all computational features in advance and then select the optimal subset of features from the computational features, the proposed approach integrates online streaming feature selection with causal structure learning. The critical challenges in this integration include: 1) the dynamic selection of computational features and 2) how to evaluate the feature subsets and implement causal structure learning. In addition, considering that building a causal structure network is a time-consuming process, we improve the process by using support vector machines based on the streaming feature algorithm. The experimental results show that our proposed algorithms improve on other traditional feature selection algorithms and ensemble learning algorithm without feature selection with regard to learning accuracy.']}
{'论文标题':['Lexical Simplification with Pretrained Encoders'],'论文作者':['Qiang, JP', ';\xa0', 'Li, Y', ';\xa0', 'Wu, XD'],'论文摘要':['Lexical simplification (LS) aims to replace complex words in a given sentence with their simpler alternatives of equivalent meaning. Recently unsupervised lexical simplification approaches only rely on the complex word itself regardless of the given sentence to generate candidate substitutions, which will inevitably produce a large number of spurious candidates. We present a simple LS approach that makes use of the Bidirectional Encoder Representations from Transformers (BERT) which can consider both the given sentence and the complex word during generating candidate substitutions for the complex word. Specifically, we mask the complex word of the original sentence for feeding into the BERT to predict the masked token. The predicted results will be used as candidate substitutions. Despite being entirely unsupervised, experimental results show that our approach obtains obvious improvement compared with these baselines leveraging linguistic databases and parallel corpus, outperforming the state-of-the-art by more than 12 Accuracy points on three well-known benchmarks.']}
{'论文标题':['Deep Matrix Factorization for Trust-Aware Recommendation in Social Networks'],'论文作者':['Wan, LT', ';\xa0', 'Xia, F', ';\xa0', 'Ma, JH'],'论文摘要':["Recent years have witnessed remarkable information overload in online social networks, and social network based approaches for recommender systems have been widely studied. The trust information in social networks among users is an important factor for improving recommendation performance. Many successful recommendation tasks are treated as the matrix factorization problems. However, the prediction performance of matrix factorization based methods largely depends on the matrixes initialization of users and items. To address this challenge, we develop a novel trust-aware approach based on deep learning to alleviate the initialization dependence. First, we propose two deep matrix factorization (DMF) techniques, i.e., linear DMF and non-linear DMF to extract features from the user-item rating matrix for improving the initialization accuracy. The trust relationship is integrated into the DMF model according to the preference similarity and the derivations of users on items. Second, we exploit deep marginalized Denoising Autoencoder (Deep-MDAE) to extract the latent representation in the hidden layer from the trust relationship matrix to approximate the user factor matrix factorized from the user-item rating matrix. The community regularization is integrated in the joint optimization function to take neighbours' effects into consideration. The results of DMF are applied to initialize the updating variables of Deep-MDAE in order to further improve the recommendation performance. Finally, we validate that the proposed approach outperforms state-of-the-art baselines for recommendation, especially for the cold-start users."]}
{'论文标题':['Approximate Repeating Pattern Mining with Gap Requirements'],'论文作者':['He, D', ' (', 'He, Dan', ') ', ';\xa0', 'Zhu, XQ', ' (', 'Zhu, Xingquan', ') ', ', ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', '],'论文摘要':['In this paper we define a new research problem for mining approximate repeating patterns (ARP) with gap constraints, where the appearance of a pattern is subject to an approximate matching, which is very common in biological sciences. To solve the problem, we propose an ArpGap (Approximate repeating pattern mining with Gap constraints) algorithm with three major components for approximate repeating pattern mining: (I) a data-driven pattern generation approach to avoid generating unnecessary patterns; (2) a back-tracking pattern search process to discover approximate occurrences of a pattern under gap constraints; and (3) an Apriori-like deterministic pruning approach to progressively prune patterns and cease the search process if necessary. Experimental results on synthetic and real-world protein sequences assert that ArpGap is efficient in terms of memory consumption and computational cost.']}
{'论文标题':['Visual data denoising with a unified Schatten-p norm and l(q) norm regularized principal component pursuit'],'论文作者':['Wang, J', ';\xa0', 'Wang, M', ';\xa0', 'Yan, SC'],'论文摘要':['To address the visual processing problem with corrupted data, in this paper, we propose a non-convex formulation to recover the authentic structure from the corrupted data. Typically, the specific structure is assumed to be low rank, which holds for a wide range of data, such as images and videos. Meanwhile, the corruption is assumed to be sparse. In the literature, such a problem is known as Robust Principal Component Analysis (RPCA), which usually recovers the low rank structure by approximating the rank function with a nuclear norm and penalizing the error by an l(1)-norm. Although RPCA is a convex formulation and can be solved effectively, the introduced norms are not tight approximations, which may cause the solution to deviate from the authentic one. Therefore, we consider here a non-convex relaxation, consisting of a Schatten-p norm and an l(q)-norm that promote low rank and sparsity respectively. We derive a proximal iteratively reweighted algorithm (PIRA) to solve the problem. Our algorithm is based on an alternating direction method of multipliers, where in each iteration we linearize the underlying objective function that allows us to have a closed form solution. We demonstrate that solutions produced by the linearized approximation always converge and have a tighter approximation than the convex counterpart. Experiments on benchmarks show encouraging results of our approach. (C) 2015 Elsevier Ltd. All rights reserved.']}
{'论文标题':['Attentive interaction-driven entity resolution over multi-source web information'],'论文作者':['He, Y', ';\xa0', 'Wu, GQ', ';\xa0', 'Hu, XG'],'论文摘要':['The task of multi-source web entity resolution (MSWER) aims to automatically discover entity references from multiple web sources that refer to the same real-world entity, which plays an important role in tasks such as question answering and recommendations. However, existing approaches typically suffer from three major limitations: (1) they usually treat the MSWER as an information retrieval task and focus on learning the similarity between entity references based on the associated features extracted from multiple sources; (2) they ignore the valuable implicit interactions between the associated features of different entities that cannot be directly captured based on the given data without any external knowledge; (3) they didn?t consider the redundant and noisy interactions between features. To overcome these limitations, this paper presents a novel attentive interaction-driven entity resolution model (AIDER). Our theme is to capture both the explicit and implicit interactions of features associated with entity references in the form of paths, and further develop an end-to-end entity resolution model for inferring the equivalent entity references. Accordingly, an external knowledge base is leveraged to construct paths for implicit interactions, and a well-designed attention mechanism is further employed to measure the importance of each path-based interaction, which focuses on useful interactions and neglects those redundant and noisy ones. Experimental results on three real-world datasets demonstrate that AIDER outperforms the state-of-the-art approaches. ? 2020 Elsevier B.V. All rights reserved.']}
{'论文标题':['A RANDOM DECISION TREE ENSEMBLE FOR MINING CONCEPT DRIFTS FROM NOISY DATA STREAMS'],'论文作者':['Li, PP', ' (', 'Li, Peipei', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') ', ';\xa0', 'Liang, QH', ' (', 'Liang, Qianhui', ') ', ';\xa0', 'Gao, YJ', ' (', 'Gao, Yunjun', ') '],'论文摘要':['Detecting concept drifts and reducing the impact from the noise in real applications of data streams are challenging but valuable for inductive learning. It is especially a challenge in a light demand on the overheads of time and space. However, though a great number of inductive learning algorithms based on ensemble classification models have been proposed for handling concept drifting data streams, little attention has been focused on the detection of the diversity of concept drifts and the influence from noise in data streams simultaneously. Motivated by this, we present a new light-weighted inductive algorithm for concept drifting detection in virtue of an ensemble model of random decision trees (named CDRDT) to distinguish various types of concept drifts from noisy data streams in this article. We use variably small data chunks to generate random decision trees incrementally. Meanwhile, we introduce the inequality of Hoeffding bounds and the principle of statistical quality control to detect the different types of concept drifts and noise. Extensive studies on synthetic and real streaming data demonstrate that CDRDT could effectively and efficiently detect concept drifts from the noisy streaming data. Therefore, our algorithm provides a feasible reference framework of classification for concept drifting data streams with noise.']}
{'论文标题':['A study of causal discovery with weak links and small samples'],'论文作者':['Dai, HH', ';\xa0', 'Korb, K', ';\xa0', 'Wu, XD'],'论文摘要':['Weak causal relationships and small sample size pose two significant difficulties to the automatic discovery of causal models from observational data. This paper examines the influence of weak causal links and varying sample sizes on the discovery of causal models. The experimental results illustrate the effect of larger sample sizes for discovering causal models reliably and the relevance of the strength of causal links and the complexity of the original causal model. We present, indicative evidence of thr superior robustness of MML (Minimum Message Length) methods to standard significance tests in the recovery of causal links. The comparative results show that the MML-CI (the MML Causal Inducer) causal discovery system finds better models than TETRAD II given small samples from linear causal models. The experimental results also reveal that MML-CI finds weak links with smaller sample sizes than can TETRAD II.']}
{'论文标题':['A Novel Android Malware Detection Method Based on Markov Blanket'],'论文作者':['Zhang, XT', ';\xa0', 'Hu, DH', ';\xa0', 'Yu, K'],'论文摘要':['Android platform is the most popular mobile operating system. With the increasing market share of Android smartphone, Android malware appears in many Android markets, and it has imposed a high risk on users. There have been some approaches focusing on permissions and API calls with machine-learning methods. However, most of the approaches do not consider the feature selection problem. In this paper, we propose a new method based on Markov blanket for Android malicious application detection. It have proven that in a faithful Bayesian network, the Markov blanket of a class attribute is a set of optimal features for feature selection for classification. We collect 5,553 malware and 3,449 benign applications to learn Markov blanket as our feature set, and use SVM to test our method. The experiment results show that our method can reduce the dimensions of feature set and keep a high accuracy with approximately 96% in Android malware detection.']}
{'论文标题':['Information entropy based remote sensing image feature discretization method, involves evaluating discretization result of image according to difference degree of indiscernibility to choose optimal discretization scheme'],'论文作者':[],'论文摘要':[]}
{'论文标题':['A Bayesian discretizer for real-valued attributes'],'论文作者':['Wu, XD'],'论文摘要':['Discretization of real-valued attributes into nominal intervals has been an important area for symbolic induction systems because many real world classification tasks involve both symbolic and numerical attributes. Among various supervised and unsupervised discretization methods, the information gain-based methods have been widely used and cited. This paper designs a new discretization method, called the Bayesian discretizer, and compares its performance with the information gain methods implemented in C4.5 and HCV (Version 2.0). Over the seven datasets tested, the Bayesian discretizer has the best results of four of them in terms of predictive accuracy.'],}
{'论文标题':['Mining sequential patterns with periodic wildcard gaps'],'论文作者':['Wu, YX', ';\xa0', 'Wang, LL', ';\xa0', 'Wu, XD'],'论文摘要':['Mining frequent patterns with periodic wildcard gaps is a critical data mining problem to deal with complex real-world problems. This problem can be described as follows: given a subject sequence, a pre-specified threshold, and a variable gap-length with wildcards between each two consecutive letters. The task is to gain all frequent patterns with periodic wildcard gaps. State-of-the-art mining algorithms which use matrices or other linear data structures to solve the problem not only consume a large amount of memory but also run slowly. In this study, we use an Incomplete Nettree structure (the last layer of a Nettree which is an extension of a tree) of a sub-pattern P to efficiently create Incomplete Nettrees of all its super-patterns with prefix pattern P and compute the numbers of their supports in a one-way scan. We propose two new algorithms, MAPB (Mining sequentiAl Pattern using incomplete Nettree with Breadth first search) and MAPD (Mining sequentiAl Pattern using incomplete Nettree with Depth first search), to solve the problem effectively with low memory requirements. Furthermore, we design a heuristic algorithm MAPBOK (MAPB for tOp-K) based on MAPB to deal with the Top-K frequent patterns for each length. Experimental results on real-world biological data demonstrate the superiority of the proposed algorithms in running time and space consumption and also show that the pattern matching approach can be employed to mine special frequent patterns effectively.']}
{'论文标题':['Frequent Pattern Mining based on Approximate Edit Distance Matrix'],'论文作者':['Guo, D', ';\xa0', 'Yuan, EM', ';\xa0', 'Hu, XG'],'论文摘要':["Frequent pattern mining has been a hot topic in many research domains, such as bioinformatics and text retrieval. Many seemingly disorganized constituents connected with different gap constraints often repetitively appear in these real-word sequences, which are considered as latent frequent patterns (FPs). A latent FP both with flexible gap and approximate constraints (replacement, deletion and insertion operations) deepen difficulty to discover its true occurrences. We design a Mining Approximate frequent PAttern (MAPA) algorithm to handle the problem: (1) We first extend an Approximate Edit Distance Matrix (A-EDM) by tackling replacement, insertion and deletion under the gap constraint to discover various deformations of patterns. (2) Then we employ an effectively back-tracking Approximate Pattern Matching (APM) scheme to obtain each candidate latent FP's support. (3) Finally, an Apriori-like deterministic pruning tactic is proposed to avoid generating unnecessary candidates for mining validation. The MAPA algorithm is a unified framework for both exact mining and approximate mining. Experiments on DNA and protein sequences demonstrate that MAPA is efficient on solutions, time and memory consumption than other peers."]}
{'论文标题':['An Effective Confidence-Based Early Classification of Time Series (vol 7, pg 96113, 2019)'],'论文作者':['Lv, JW', ' (', 'Lv, Junwei', ') ', ', ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') ', ', ', ', ', ';\xa0', 'Li, L', ' (', 'Li, Lei', ') ', ', ', ';\xa0', 'Li, PP', ' (', 'Li, Peipei', ') ', ', '],'论文摘要':['In the above article [1], reference [2] was missing.']}
{'论文标题':['Online Biterm Topic Model based short text stream classification using short text expansion and concept drifting detection'],'论文作者':['Hu, XG', ';\xa0', 'Wang, HY', ';\xa0', 'Li, PP'],'论文摘要':['Short text stream classification suffers from enormous challenges, due to the sparsity, high dimension and rapid variability of the short text stream. In this paper, we present a short text stream classification approach refined from online Biterm Topic Model (BTM) using short text expansion and concept drifting detection. Specifically, in our method, we firstly extend short text streams from an external resource to make up for the sparsity of data, and use online BTM to select representative topics instead of the word vector to represent the feature of short texts. Secondly, we propose a concept drift detection method based on the topic model to detect the hidden concept drifts in short text streams. Thirdly, we build an ensemble model using several data chunks and update with the newest data chunk and results of the concept drift detection. Finally, extensive experimental results demonstrate that compared to well-known baselines, our approach achieves a better performance in the classification and concept drifting detection. (C) 2018 Elsevier B.V. All rights reserved.']}
{'论文标题':['Efficient Missing Data Imputation for Supervised Learning'],'论文作者':['Shichao Zhang', ';\xa0', 'Xindong Wu', ';\xa0', 'Manlong Zhu'],'论文摘要':['In supervised learning, missing values usually appear in the training set. The missing values in a dataset may generate bias, affecting the quality of the supervised learning process or the performance of classification algorithms. These imply that a reliable method for dealing with missing values is necessary. In this paper, we analyze the difference between iterative imputation of missing values and single imputation in real-world applications. We propose an EM-style iterative imputation method, in which each missing attribute-value is iteratively filled using a predictor constructed from the known values and predicted values of the missing attribute-values from the previous iterations. Meanwhile, we demonstrate that it is reasonable to consider the imputation ordering for patching up multiple missing attribute values, and therefore introduce a method for imputation ordering. We experimentally show that our approach significantly outperforms some standard machine learning methods for handling missing values in classification tasks.']}
{'论文标题':['Classification with Streaming Features: An Emerging-Pattern Mining Approach'],'论文作者':['Yu, K', ';\xa0', 'Ding, W', ';\xa0', 'Wu, XD'],'论文摘要':['Many datasets from real-world applications have very high-dimensional or increasing feature space. It is a new research problem to learn and maintain a classifier to deal with very high dimensionality or streaming features. In this article, we adapt the well-known emerging-pattern-based classification models and propose a semi-streaming approach. For streaming features, it is computationally expensive or even prohibitive to mine long-emerging patterns, and it is nontrivial to integrate emerging-pattern mining with feature selection. We present an online feature selection step, which is capable of selecting and maintaining a pool of effective features from a feature stream. Then, in our offline step, separated from the online step, we periodically compute and update emerging patterns from the pool of selected features from the online step. We evaluate the effectiveness and efficiency of the proposed method using a series of benchmark datasets and a real-world case study on Mars crater detection. Our proposed method yields classification performance comparable to the state-of-art static classification methods. Most important, the proposed method is significantly faster and can efficiently handle datasets with streaming features.']}
{'论文标题':['Collaborative deep recommendation with global and local item correlations'],'论文作者':['Liu, HT', ';\xa0', 'Liu, HM', ';\xa0', 'Wu, XD'],'论文摘要':['Many recommendation methods have introduced item correlation information to alleviate the data sparsity and cold-start problems. However, existing approaches exploit either global or local item correlations, rarely consider both global and local item correlations, and thus they cannot provide advanced recommendation performance. Inspired by this, we propose a novel collaborative deep framework called GLICR to simultaneously incorporate the global and local item correlations into the model. More specifically, our proposed GLICR model tightly couples deep neural network with matrix factorization (MF), and jointly learns the deep feature representations of item content information in deep neural network and the rating matrix in MF. In addition, we introduce manifold regularization to learn the global and local item correlations directly from data. We conduct comprehensive experiments on real-world datasets at three different degrees of sparsity to confirm that our approach can effectively alleviate data sparsity problem and is superior to existing state-of-the-art recommendation techniques. This work is the first attempt that considers the global and local item correlations by manifold regularization in recommendation scenario. (C) 2019 Elsevier B.V. All rights reserved.']}
{'论文标题':['Prediction of Rice Brown Planthoppers based on System Dynamics'],'论文作者':['Wang, QR', ';\xa0', 'Xie, F', ';\xa0', 'Wu, XD'],'论文摘要':['Rice brown planthoppers are the main pest of Chinese rice production, and belong to the long distance migratory pest. The descriptive model of a rice brown planthopper has complex features, which are dynamic, continuous, nonlinear, multivariable, multi-feedback, and so on. By using the system dynamics method, we analyze the causal relationships between the various factors that influence the emergence and development of rice brown planthoppers from the whole-local perspective. Both the causal relationship diagram of the factors and the flowchart on the emergence and development of rice brown planthoppers are drawn. Combining with the discrete multivariate analysis method, we establish and implement a system dynamics simulation model on the emergence and development of rice brown planthoppers in Anhui. The experimental results show that the model has high accuracy.']}
{'论文标题':['Ontology-based feature weighting for biomedical literature classification'],'论文作者':['He, D', ';\xa0', 'Wu, XD'],'论文摘要':['Ontology-based methods have been applied to biomedical literature classification tasks recently. By mapping lexically different but semantically similar words into features in the domain ontology that underlies the words, we can achieve at least two benefits: the dimensionality of the feature space can be reduced effectively, and the semantic information that underlies the lexical words can be incorporated into the classification process, leading to better classification accuracies. In this paper, we propose an ontology-based feature weighting strategy for the biomedical literature classification problem. We assign weights to the features into which the lexical words are mapped, according to the structure of the domain ontology, and further optimize the weights using cross-validation. Our experiments on MEDLINE-indexed journal abstracts demonstrate that our method can achieve a significant improvement on the classification accuracies, especially when the classification task is hard.']}
{'论文标题':['Fusion knowledge map and deep learning based intelligent Chinese question answering system, has result generation module for searching recommended answer from knowledge reasoning module received by receiving module'],'论文作者':[],'论文摘要':[]}
{'论文标题':['A Chinese time ontology for the Semantic Web'],'论文作者':['Zhang, CX', ';\xa0', 'Cao, CG', ';\xa0', 'Wu, XD'],'论文摘要':['Representation of and reasoning with temporal knowledge are fundamental in information systems that involve changes and actions. To build such systems, a time ontology is demanded. The development of a time ontology is also an indispensable part of effort to realize the Semantic Web. Nevertheless, our practice shows that any practical time ontology is closely related with a specific calendar, culture or history. To this end, this paper presents a Chinese time ontology for knowledge systems and web services which involve temporal entities or temporal properties. First, we define a base time ontology. As a core component, it consists of a time system, a timing system, a Gregorian timing system, and a timing ontology. Upon this base ontology, other parts of the Chinese time ontology are finally constructed, including the traditional Chinese timing system, temporal representation in Chinese idiosyncratic ways, and transformation between temporal entities in the Gregorian timing system and temporal entities in the traditional Chinese timing system. We will argue that the base time ontology is not only a basic and integral part of the Chinese time ontology, but also a base for constructing other time ontologies. (C) 2011 Elsevier B.V. All rights reserved.']}
{'论文标题':['Extracting elite pairwise constraints for clustering'],'论文作者':['Jiang, H', ';\xa0', 'Ren, ZL', ';\xa0', 'Wu, XD'],'论文摘要':['Semi-supervised clustering under pairwise constraints (i.e. must-links and cannot-links) has been a hot topic in the data mining community in recent years. Since pairwise constraints provided by distinct domain experts may conflict with each other, a lot of research work has been conducted to evaluate the effects of noise imposing on semi-supervised clustering. In this paper, we introduce elite pairwise constraints, including elite must-link (EML) and elite cannot-link (ECL) constraints. In contrast to traditional constraints, both EML and ECL constraints are required to be satisfied in every optimal partition (i.e. a partition with the minimum criterion function). Therefore, no conflict will be caused by those new constraints. First, we prove that it is NP-hard to obtain EML or ECL constraints. Then, a heuristic method named Limit Crossing is proposed to achieve a fraction of those new constraints. In practice, this new method can always retrieve a lot of EML or ECL constraints. To evaluate the effectiveness of Limit Crossing, multi-partition based and distance based methods are also proposed in this paper to generate faux elite pairwise constraints. Extensive experiments have been conducted on both UCI and synthetic data sets using a semi-supervised clustering algorithm named COP-KMedoids. Experimental results demonstrate that COP-KMedoids under EML and ECL constraints generated by Limit Crossing can outperform those under either faux constraints or no constraints. (C) 2012 Elsevier B.V. All rights reserved.']}
{'论文标题':['Gaussian model-based fully convolutional networks for multivariate time series classification'],'论文作者':['Changyang Tai', ';\xa0', 'Ze Yang', ';\xa0', 'Xianyu Bao'],'论文摘要':['Multivariate time series (MTS) classification has been regarded as one of the most challenging problems in data mining due to the difficulty in modeling the correlation of variables and samples. In addition, high-dimensional MTS modeling has a large time and space consumption. This paper proposes a novel method, Gaussian Model-based Fully Convolutional Networks (GM-FCN), to improve the performance of high-dimensional MTS classification. Each original MTS is converted into multivariate Gaussian model parameters as the input of FCN. These parameters effectively capture the correlation be-tween MTS variables and significantly reduce the data scale by aligning an MTS size to its dimension. FCN is designed to learn more in-depth features of MTS based on these parameters for modeling the correlation between samples. Thus, GM-FCN can not only model the correlation between variables, but also the correlation between samples. We compare GM-FCN with nine state-of-the-art MTS classification methods, INN-ED, INN-DTW-i, INN-DTW-D, KLD-GMC, MLP, ResNet, Encoder, MCNN, and MCDCNN, on four high-dimensional public datasets, experimen-tal results show that the accuracy of G M - FCN is significantly superior to the others. Besides, the training time of GM-FCN is dozens of times faster than FCN using the original equal-length MTS data as input.']}
{'论文标题':['Mining Semantically Consistent Patterns for Cross-View Data'],'论文作者':['Zhang, L', ';\xa0', 'Zhao, Y', ';\xa0', 'Wu, XD'],'论文摘要':['In some real world applications, like information retrieval and data classification, we often are confronted with the situation that the same semantic concept can be expressed using different views with similar information. Thus, how to obtain a certain Semantically Consistent Patterns (SCP) for cross-view data, which embeds the complementary information from different views, is of great importance for those applications. However, the heterogeneity among cross-view representations brings a significant challenge on mining the SCP. In this paper, we propose a general framework to discover the SCP for cross-view data. Specifically, aiming at building a feature-isomorphic space among different views, a novel Isomorphic Relevant Redundant Transformation (IRRT) is first proposed. The IRRT linearly maps multiple heterogeneous low-level feature spaces to a high-dimensional redundant feature-isomorphic one, which we name as mid-level space. Thus, much more complementary information from different views can be captured. Furthermore, to mine the semantic consistency among the isomorphic representations in the mid-level space, we propose a new Correlation-based Joint Feature Learning (CJFL) model to extract a unique high-level semantic subspace shared across the feature-isomorphic data. Consequently, the SCP for cross-view data can be obtained. Comprehensive experiments on three data sets demonstrate the advantages of our framework in classification and retrieval.']}
{'论文标题':['Graph-Based Learning via Auto-Grouped Sparse Regularization and Kernelized Extension'],'论文作者':['Fang, YQ', ';\xa0', 'Wang, RL', ';\xa0', 'Wu, XD'],'论文摘要':["The key task in developing graph-based learning algorithms is constructing an informative graph to express the contextual information of a data manifold. Since traditional graph construction methods are sensitive to noise and less datum-adaptive to changes in density, a new method called '(1)-graph was proposed recently. A graph construction needs to have two important properties: sparsity and locality. The l(1)-graph has a strong sparsity property, but a weak locality property. Thus, we propose a new method of constructing an informative graph using auto-grouped sparse regularization based on the l(1)-graph, which is called as Group Sparse graph (GS-graph). We also show how to efficiently construct a GS-graph in reproducing kernel Hilbert space with the kernel trick. The new methods, the GS-graph and its kernelized version (KGS-graph), have the same noise-insensitive property as that of l(1)-graph and also can successively preserve the properties of sparsity and locality simultaneously. Furthermore, we integrate the proposed graph with several graph-based learning algorithms to demonstrate the effectiveness of our method. The empirical studies on benchmarks show that the proposed methods outperform the l(1)-graph and other traditional graph construction methods in various learning tasks."]}
{'论文标题':['Effective classification of noisy data streams with attribute-oriented dynamic classifier selection'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD', ';\xa0', 'Yang, Y'],'论文摘要':['Recently, mining from data streams has become an important and challenging task for many real-world applications such as credit card fraud protection and sensor networking. One popular solution is to separate stream data into chunks, learn a base classifier from each chunk, and then integrate all base classifiers for effective classification. In this paper, we propose a new dynamic classifier selection (DCS) mechanism to integrate base classifiers for effective mining from data streams. The proposed algorithm dynamically selects a single "best" classifier to classify each test instance at run time. Our scheme uses statistical information from attribute values, and uses each attribute to partition the evaluation set into disjoint subsets, followed by a procedure that evaluates the classification accuracy of each base classifier on these subsets. Given a test instance, its attribute values determine the subsets that the similar instances in the evaluation set have constructed, and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance. Experimental results and comparative studies demonstrate the efficiency and efficacy of our method. Such a DCS scheme appears to be promising in mining data streams with dramatic concept drifting or with a significant amount of noise, where the base classifiers are likely conflictive or have low confidence.']}
{'论文标题':['An Ensemble Method Based on Confidence Probability for Multi-domain Sentiment Classification'],'论文作者':['Quan Zhou', ';\xa0', 'Yuhong Zhang', ';\xa0', 'Xuegang Hu'],'论文摘要':['Multi-domain sentiment classification methods based on ensemble decision attracts more and more attention. These methods avoid collecting a large amount of new training data in target domain and expand aspect of deploying source domain systems. However, these methods face some important issues: the quantity of incorrect pre-labeled data remains high and the fixed weights limit accuracy of the ensemble classifier. Thus, we propose a novel method, named CEC, which integrates the ideas of self-training and co-training into multi-domain sentiment classification. Classification confidence is used to pre-label the data in the target domain. Meanwhile, CEC combines the base classifiers according to classification confidence probabilities when taking a vote for prediction. The experiments show the accuracy of the proposed algorithm has highly improved compared with the baseline algorithms.']}
{'论文标题':['Automatic generation of pedicle contours in 3D vertebral models'],'论文作者':['Huo, X', ';\xa0', 'Wang, H', ';\xa0', 'Cheng, L'],'论文摘要':['Objectives: Pedicle location and recognition play important roles in spinal morphology analysis and orthodontic screw implantation, which can help doctors avoid injuring the pedicle during screw implantation. However, because of the complex spatial structures of vertebrae and the close connection between the pedicle and other parts of the vertebrae, it is challenging to locate and recognize the pedicle of the vertebral arch in 2D or 3D vertebral images.', "Methods: In this paper, based on deep learning technology, we propose a method for automatically recognizing the vertebral pedicle in individual vertebral models and drawing pedicle contours. The goal is to provide references so doctors can simulate the pedicle screw implantation operation to prevent screw deviation and further enhance the automation of our team's scoliosis-correction assistive system. First, we preprocess the individual vertebral models to obtain their point clouds. Then, we use a modified PointNet model to segment the pedicle areas from the individual vertebral point clouds. We use the segmentation results to automatically fit the cross-sections of pedicles and finally generate the pedicle contours as surgical references.", 'Results: The experiments show that the method can generate contours quickly and accurately with a small amount of manual adjustment and can provide good references for simulating screw placement.', 'Conclusions: The efficiency of generating pedicle contours during the process of simulated screw placement is greatly improved, and the difficulty of using our simulation system has also been greatly reduced, both of which play essential roles in pedicle screw implantation and the formulation of surgical plans.']}
{'论文标题':['Efficient sequential pattern mining with wildcards for keyphrase extraction'],'论文作者':['Xie, F', ';\xa0', 'Wu, XD', ';\xa0', 'Zhu, XQ'],'论文摘要':['A keyphrase (a multi-word unit) in a document denotes one or multiple keywords capturing a main topic of the underlying document. Finding good keyphrases of a document can quickly summarize knowledge for efficient decision making and benefit domains involving intensive text information. To date, existing keyphrase extraction methods cannot be customized to each specific document, mainly because their patterns used to form paraphrases are too restrictive and may not capture flexible keyword relationships inside the text. In this paper, we propose a sequential pattern mining based document-specific keyphrase extraction method. Our key innovation is to use wildcards (or gap constraints) to help extract sequential patterns, so the flexible wildcard constraints within a pattern can capture semantic relationships between words, and the system will have full flexibility to discover different types of sequential patterns as candidates for keyphrase extraction. To achieve the goal, we regard each single document as a sequential dataset, and propose an efficient algorithm to mine sequential patterns with wildcard and one-off conditions that allows important keyphrases to be captured during the mining process. For each extracted keyphrase candidate, we use some statistical pattern features to characterize it, and further collect all keyphrases from the document to form a training set. A supervised learning classifier is trained to identify keyphrases from a test document. Because our pattern mining and pattern characterization processes are customized to each single document, keyphases extracted from our method are highly specific for each document. Experimental results demonstrate that the proposed sequential pattern mining method outperforms existing pattern mining methods in both runtime performance and completeness. Comparisons on keyphrase benchmark datasets also confirm that the proposed document-specific keyphrase extraction method is effective in improving the quality of extracted keyphrases. (C) 2016 Elsevier B.V. All rights reserved.']}
{'论文标题':['Causal relationship mining based on learning behavior in massive open online course data comprises constructing candidate independent variables affects quitting school behavior and eliminating error variables based on regression analysis'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Alliance Mechanism Based on Cloud Computing'],'论文作者':['Xu, J', ';\xa0', 'Yang, SQ', ';\xa0', 'Yu, K'],'论文摘要':['To introduce the concept and the characteristics of cloud cornputing at first, and through the expansion on the present situation and disadvantages of the cloud computing, it is necessary to propose the association mechanism of cloud computing in the future. Then to analyze the feasibility of CCA (Cloud Computing Alliance), which includes economical and technical feasibility; by literature and empirical researches at the same time, the paper described the present situation about development of CCA; and put forward several problems in the process of alliance establishment. In this paper the authors point out that the future development pattern of cloud computing will be established based on CCA.']}
{'论文标题':['Content-aware attributed entity embedding for synonymous named entity discovery'],'论文作者':['Cai, DS', ';\xa0', 'Wu, GQ'],'论文摘要':['Synonymous Named Entity Discovery (SNED) refers to the task of discovering named entities that refer to the same entity. Discovering synonymous named entity by manually designing features and similarity metrics is non-trivial and very difficult due to the diversity of the raw features (e.g. the associated attributes and text content). In this paper, we present Content-Aware Attributed Entity Embedding (CAAEE), an unsupervised SNED model to address this issue. By leveraging the associated attributes and text content information, our approach learns a projection which maps named entities to a low-dimensional feature space without any manually designed feature and supervised information. In the learned feature space, synonymous named entities are close to each other, which can reflect the similarity between named entities. We build two heterogeneous networks to jointly model named entities, their associated attributes and text content information. For each heterogeneous network, we design two objective function based on two probability distributions aimed at preserving the network structure. By jointly optimizing the objective functions, a low-dimensional representation is obtained for each named entity. The similarity between the learned low-dimensional representations is then used to discover synonymous named entities. In experiments, we compare our model with existing SNED models on two real-world named entity datasets. Experimental results show that CAAEE outperforms state-of-the-art methods with significant improvement. (C) 2018 Elsevier B.V. All rights reserved.']}
{'论文标题':['Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search'],'论文作者':['Gao, Y', ';\xa0', 'Wang, M', ';\xa0', 'Wu, XD'],'论文摘要':['Due to the popularity of social media websites, extensive research efforts have been dedicated to tag-based social image search. Both visual information and tags have been investigated in the research field. However, most existing methods use tags and visual characteristics either separately or sequentially in order to estimate the relevance of images. In this paper, we propose an approach that simultaneously utilizes both visual and textual information to estimate the relevance of user tagged images. The relevance estimation is determined with a hypergraph learning approach. In this method, a social image hypergraph is constructed, where vertices represent images and hyperedges represent visual or textual terms. Learning is achieved with use of a set of pseudo-positive images, where the weights of hyperedges are updated throughout the learning process. In this way, the impact of different tags and visual words can be automatically modulated. Comparative results of the experiments conducted on a dataset including 370+ images are presented, which demonstrate the effectiveness of the proposed approach.']}
{'论文标题':['A real-time transportation prediction system'],'论文作者':['Li, HG', ';\xa0', 'Li, Z', ';\xa0', 'Wu, XD'],'论文摘要':['In recent years, the use of advanced technologies such as wireless communication and sensors in intelligent transportation systems has made a significant increase in traffic data available. With this data, traffic prediction has the ability to improve traffic conditions and to reduce travel delays by facilitating better utilization of available capacity. This paper presents a real-time transportation prediction system named VTraffic for Vermont Agencies of Transportation by integrating traffic flow theory, advanced sensors, data gathering, data integration, data mining and visualization technologies to estimate and visualize the current and future traffic. In the VTraffic system, acoustic sensors were installed to monitor and to collect real-time data. Reliable predictions can be obtained from historical data and be verified and refined by the current and near future real-time data.']}
{'论文标题':['On big wisdom'],'论文作者':['Wu, MH', ';\xa0', 'Wu, XD'],'论文摘要':['This paper defines big wisdom with a HAO/BIBLE framework, which integrates human intelligence (HI), artificial intelligence (AI), and organizational/business intelligence (O/BL) with Bigdata analytics in Large Environments, for industrial intelligence in organizational activities. Big wisdom starts with Bigdata, discovers big knowledge, and facilitates human and machine synergism for complex problem solving. When the HAO/BIBLE framework is applied to a regular (non-Bigdata) environment, it becomes the well-known PEAS agent structure, and when the knowledge graph in HAO/BIBLE relies on domain expertise (rather than big knowledge), HAO/BIBLE serves as an expert system. This paper compares and contrasts Bigdata, big knowledge, and big wisdom and instantiates HAO/BIBLE with a case study for intelligent catering services to illustrate synergized HAO intelligence (HI+AI+OI) for big wisdom.']}
{'论文标题':['Unsupervised Graph Representation Learning With Variable Heat Kernel'],'论文作者':['Jing, YJ', ';\xa0', 'Wang, H', ';\xa0', 'Zhang, YY'],'论文摘要':['Graph representation learning aims to learn a low-dimension latent representation of nodes, and the learned representation is used for downstream graph analysis tasks. However, most of the existing graph embedding models focus on how to aggregate all the neighborhood node features to encode the semantic information into the representation and neglect the global structural features of the node such as community structure and centrality. In the paper, we propose a novel unsupervised graph representation learning method (VHKRep), where a variable heat kernel is designed to better capture implicit global features via heat diffusion with the different time scale and generate the robust node representation. We conduct extensive experiment on three real-world datasets for node classification and link prediction tasks. Compared with the state-of-the-art seven models, the experimental results demonstrate the effectiveness of our proposed method on both node classification and link prediction tasks.']}
{'论文标题':['Pattern Matching With Flexible Wildcards And Recurring Characters'],'论文作者':['Haiping Wang', ';\xa0', 'Fei Xie', ';\xa0', 'Xindong Wu'],'论文摘要':['Pattern matching is an important task, which is widely used in many fields, such as information retrieval and bioinformatics. Recently, a much more flexible pattern matching problem with wildcards has been proposed. Chen et al. introduced local constraints, global constraints and the one-off condition into the task of pattern matching, and the most representative algorithm SAIL was designed. However, the performance of SAIL is not analyzed well, which affects its application. Therefore, this paper analyzes the performance of SAIL in-depth, and discovers that the matching result is closely related to the features of patterns. Meanwhile, the completeness of SAIL in the pattern matching with no-recurring characters is proved, and an improved algorithm, named RSAIL, is proposed for pattern matching with recurring tail characters. Extensive experiments demonstrate that RSAIL improves the number of matches by 2.2% compared to SAIL.']}
{'论文标题':['Adversarial training with Wasserstein distance for learning cross-lingual word embeddings'],'论文作者':['Li, YL', ' (', 'Li, Yuling', ') ', ';\xa0', 'Zhang, YH', ' (', 'Zhang, Yuhong', ') ', ';\xa0', 'Yu, K', ' (', 'Yu, Kui', ') ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') '],'论文摘要':['Recent studies have managed to learn cross-lingual word embeddings in a completely unsupervised manner through generative adversarial networks (GANs). These GANs-based methods enable the alignment of two monolingual embedding spaces approximately, but the performance on the embeddings of low-frequency words (LFEs) is still unsatisfactory. The existing solution is to set up the low sampling rates for the embeddings of LFEs based on word-frequency information. However, such a solution has two shortcomings. First, this solution relies on the word-frequency information that is not always available in real scenarios. Second, the uneven sampling may cause the models to overlook the distribution information of LFEs, thereby negatively affecting their performance. In this study, we propose a novel unsupervised GANs-based method that effectively improves the quality of LFEs, circumventing the above two issues. Our method is based on the observation that LFEs tend to be densely clustered in the embedding space. In these dense embedding points, obtaining fine-grained alignment through adversarial training is difficult. We use this idea to introduce a noise function that can disperse the dense embedding points to a certain extent. In addition, we train a Wasserstein critic network to encourage the noise-adding embeddings and the original embeddings to have similar semantics. We test our approach on two common evaluation tasks, namely, bilingual lexicon induction and cross-lingual word similarity. Experimental results show that the proposed model has stronger or competitive performance compared with the supervised and unsupervised baselines.']}
{'论文标题':['MINING POSITIVE AND NEGATIVE ASSOCIATION RULES'],'论文作者':['Zhu, HL', ';\xa0', 'Xu, ZG'],'论文摘要':['Recently, mining negative association rules has received some attention and been proved to be useful in real world. This paper presents an efficient algorithm(PNAR) for mining both positive and negative association rules in databases. The algorithm extends traditional association rules to include negative association rules. When mining negative association rules, we adopt another minimum support threshold to mine frequent negative itemsets. With a correlation coefficient measure and pruning strategies,, the algorithm can find all valid association rules quickly and overcome some limitations of the previous mining methods. The experimental results demonstrate its effectiveness and efficiency.']}
{'论文标题':['A 2-Tier Clustering Algorithm with Map-Reduce'],'论文作者':['Jing Zhang', 'Gongqing Wu', 'Xindong Wu'],'论文摘要':['In the field of data mining, clustering is one of the important methods. K-Means is a typical distance-based clustering algorithm; 2-tier clustering should implement scalable clustering by means of dividing, sampling and knowledge integrating. Among those tools of distributed processing, Map-Reduce has been widely embraced by both academia and industry. Hadoop is an open-source parallel and distributed programming framework for the implementation of Map-Reduce computing model. With the analysis of the Map-Reduce paradigm of computing, we find that Hadoop parallel and distributed computing model is appropriate for the implementation of scalable clustering algorithm. This paper takes advantages of K-Means, 2-tier clustering mechanism and Map-Reduce computing model; proposes a new method for parallel and distributed clustering to explore distributed clustering problem based on Map-Reduce. The method aims to apply the clustering algorithm effectively to the distributed environment. The extensive studies demonstrate that the proposed algorithm is scalable, and the time performance is stable. Meanwhile, adding number of cluster nodes would improve the time performance of clustering.'],}
{'论文标题':['Anonymizing classification data using rough set theory'],'论文作者':['Ye, MQ', ';\xa0', 'Wu, XD', ';\xa0', 'Hu, DH'],'论文摘要':['Identity disclosure is one of the most serious privacy concerns in many data mining applications. A well-known privacy model for protecting identity disclosure is k-anonymity. The main goal of anonymizing classification data is to protect individual privacy while maintaining the utility of the data in building classification models. In this paper, we present an approach based on rough sets for measuring the data quality and guiding the process of anonymization operations. First, we make use of the attribute reduction theory of rough sets and introduce the conditional entropy to measure the classification data quality of anonymized datasets. Then, we extend conditional entropy under single-level granulation to hierarchical conditional entropy under multi-level granulation, and study its properties by dynamically coarsening and refining attribute values. Guided by these properties, we develop an efficient search metric and present a novel algorithm for achieving k-anonymity, Hierarchical Conditional Entropy-based Top-Down Refinement (HCE-TDR), which combines rough set theory and attribute value taxonomies. Theoretical analysis and experiments on real world datasets show that our algorithm is efficient and improves data utility. (C) 2013 Elsevier B.V. All rights reserved.']}
{'论文标题':['Probabilistic tensor analysis with Akaike and Bayesian information criteria'],'论文作者':['Tao, DC', ';\xa0', 'Sun, JM', ';\xa0', 'Faloutsos, C'],'论文摘要':['From data mining to computer vision, from visual surveillance to biometrics research, from biomedical imaging to bioinformatics, and from multimedia retrieval to information management, a large amount of data are naturally represented by multidimensional arrays, i.e., tensors. However, conventional probabilistic graphical models with probabilistic inference only model data in vector format, although they are very important in many statistical problems, e.g., model selection. Is it possible to construct multilinear probabilistic graphical models for tensor format data to conduct probabilistic inference, e.g., model selection? This paper provides a positive answer based on the proposed decoupled probabilistic model by developing the probabilistic tensor analysis (PTA), which selects suitable model for tensor format data modeling based on Akaike information criterion (AIC) and Bayesian information criterion (BIC). Empirical studies demonstrate that PTA associated with AIC and BIC selects correct number of models.']}
{'论文标题':['Accurate Markov Boundary Discovery for Causal Feature Selection'],'论文作者':['Wu, XY', ';\xa0', 'Jiang, BB', ';\xa0', 'Chen, HH'],'论文摘要':['Causal feature selection has achieved much attention in recent years, which discovers a Markov boundary (MB) of the class attribute. The MB of the class attribute implies local causal relations between the class attribute and the features, thus leading to more interpretable and robust prediction models than the features selected by the traditional feature selection algorithms. Many causal feature selection methods have been proposed, and almost all of them employ conditional independence (CI) tests to identify MBs. However, many datasets from real-world applications may suffer from incorrect CI tests due to noise or small-sized samples, resulting in lower MB discovery accuracy for these existing algorithms. To tackle this issue, in this article, we first introduce a new concept of PCMasking to explain a type of incorrect CI tests in the MB discovery, then propose a cross-check and complement MB discovery (CCMB) algorithm to repair this type of incorrect CI tests for accurate MB discovery. To improve the efficiency of CCMB, we further design a pipeline machine-based CCMB (PM-CCMB) algorithm. Using benchmark Bayesian network datasets, the experiments demonstrate that both CCMB and PM-CCMB achieve significant improvements on the MB discovery accuracy compared with the existing methods, and PM-CCMB further improves the computational efficiency. The empirical study in the real-world datasets validates the effectiveness of CCMB and PM-CCMB against the state-of-the-art causal and traditional feature selection algorithms.']}
{'论文标题':['Knowledge based tutoring system for learning Lithuanian language'],'论文作者':['Baniulis, K.', ';\xa0', 'Tamulynas, B.', ';\xa0', 'Normantiene, J.'],'论文摘要':['Summary form only given. The purpose of the work is to build a computer based Lithuanian language learning system which will satisfy the needs of various users (school children, students, etc.). The system is considered to be part of the general intelligent tutoring system (ITS), which includes the following: subject oriented tutoring modules, subject oriented database modules (vocabularies, tables, specific subject information), task and lesson building database modules, and knowledge based student modules. The ITS project includes: 1) building up the vocabulary; 2) filling in the vocabulary; 3) building lessons; 4) learning process control.']}
{'论文标题':['A Phrase-Based Method for Hierarchical Clustering of Web Snippets'],'论文作者':['Li, Z', ';\xa0', 'Wu, XD'],'论文摘要':["Document clustering has been applied in web information retrieval, which facilitates users' quick browsing by organizing retrieved results into different groups. Meanwhile, a tree-like hierarchical structure is well-suited for organizing the retrieved results in favor of web users. In this regard, we introduce a new method for hierarchical clustering of web snippets by exploiting a phrase-based document index. In our method, a hierarchy of web snippets is built based on phrases instead of all snippets, and the snippets are then assigned to the corresponding clusters consisting of phrases. We show that, as opposed to the traditional hierarchical clustering, our method not only presents meaningful cluster labels but also improves clustering performance."]}
{'论文标题':['Semi-Supervised Multi-Label Learning from Crowds via Deep Sequential Generative Model'],'论文作者':['Shi, WL', ';\xa0', 'Sheng, VS', ';\xa0', 'Gu, B'],'论文摘要':['Multi-label classification (MLC) is pervasive in real-world applications. Conventional MLC algorithms assume that enough ground truth labels are available for training a classifier. While in reality, obtaining ground truth labels is expensive and time-consuming. In the field of data mining, it is more efficient to use crowdsourcing for label collection. In this setting, an MLC algorithm needs to deal with the noisiness of the crowdsourced labels as well as the remaining massive unlabeled data. In this paper, we propose a deep generative model to describe the label generation process for this semi-supervised multi-label learning problem. Although deep generative models are widely used for MLC problems, no previous work could address the noisy crowdsourced multi-labels and unlabeled data simultaneously. To address this challenging problem, our novel generative model incorporates latent variables to describe the labeled/unlabeled data as well as the labeling process of crowdsourcing. We introduce an efficient sequential inference model to approximate the model posterior and infer the ground truth labels. Our experimental results on various scales of datasets demonstrate the effectiveness of our proposed model. It performs favorably against four state-of-the-art deep generative models.']}
{'论文标题':['Data Stream Ensemble Classification Algorithm Based on Tri-training'],'论文作者':['Hu Xuegang', ';\xa0', 'Ma Liwei', ';\xa0', 'Li Peipei'],'论文摘要':['Data stream classification is one of important research tasks in the field of data mining. Most existing data stream classification algorithms require the labeled data for training. However, there are few labeled data in data streams in real applications. To solve this problem, the labeled data can be obtained by manual labeling, but it is very expensive and time consuming. Considering the unlabeled data are huge and full of information, a data stream ensemble classification algorithm based on Tri-training for labeled and unlabeled data is proposed in this paper. The proposed algorithm divides data stream into chunks by sliding windows and trains base classifiers with Tri-training on the first coming k chunks with labeled and unlabeled data. Then the classifiers are iteratively updated by weighted voting until all unlabeled data are labeled. Meanwhile, the k + 1 data chunk is predicted by using the ensemble model of k Tri-training classifiers and the classifier with higher classification error is discarded, which reconstructs a new classifier on current data chunk to update the model. Experiments on 10 UCI data sets show that the proposed algorithm can significantly improve the classification accuracy of data stream even with 80% unlabeled data in comparison with traditional algorithms.', '数据流分类是数据挖掘领域的重要研究任务之一,已有的数据流分类算法大多是在有标记数据集上进行训练,而实际应用领域数据流中有标记的数据数量极少。为解决这一问题,可通过人工标注的方式获取标记数据,但人工标注昂贵且耗时。考虑到未标记数据的数量极大且隐含大量信息,因此在保证精度的前提下,为利用这些未标记数据的信息,本文提出了一种基于Tri-training的数据流集成分类算法。该算法采用滑动窗口机制将数据流分块,在前k块含有未标记数据和标记数据的数据集上使用Tri-training训练基分类器,通过迭代的加权投票方式不断更新分类器直到所有未标记数据都被打上标记,并利用k个Tri-training集成模型对第k+l块数据进行预测,丢弃分类错误率高的分类器并在当前数据块上重建新分类器从而更新当前模型。在10个UCI数据集上的实验结果表明:与经典算法相比,本文提出的算法在含80%未标记数据的数据流上的分类精度有显著提高。']}
{'论文标题':['A Comparative Study of Pattern Matching Algorithms on Sequences'],'论文作者':['Min, F', ';\xa0', 'Wu, XD'],'论文摘要':['In biological sequence pattern mining, pattern matching is a core component to count the matches of each candidate pattern. We consider patterns with wildcard gaps. A wildcard gap matches any subsequence with a length between predefined lower and upper bounds. Since the number of candidate patterns might be huge, the efficiency of pattern matching is critical. We study two existing pattern matching algorithms named Pattern mAtching with Independent wildcard Gaps (PAIG) and Gap Constraint Search (GCS). GCS was designed to deal with patterns with identical gaps, and we propose to revise it for the case of independent gaps. PAIG can deal with global length constraints while GCS cannot. Both algorithms have the same space complexity. In the worst case, the time complexity of GCS is lower. However, in the best case, PAM is more efficient. We discuss appropriate selection between PAIG and GCS through theoretical analysis and experimental results on a biological sequence.']}
{'论文标题':['A Matching Algorithm in PMWL based on CluTree'],'论文作者':['Liu, YL', ';\xa0', 'Wu, XD', ';\xa0', 'Gao, J'],'论文摘要':['Pattern matching with wildcards and length constraints (PMWL) is a complex problem which has important applications in bioinformatics, network security and information retrieval. Existing algorithms use the traditional left-most strategy when selecting among multiple candidate matching positions, which leads to incomplete final matching results. This paper presents a new data structure CluTree and a new matching algorithm RBCT*(1) based on CluTree. After establishing a cluster of trees with red and black nodes according to a pattern P and a text T, which is called CluTree, our RBCT algorithm uses the sharing degree, correlation degree and mixed information entropy of each node in the CluTree for path selection and dynamic pruning. Our RBCT algorithm traverses the CluTree and finds more occurrences compared to the existing algorithms under the one-off condition in a linear time cost. Theoretical analysis and experimental results show that the RBCT algorithm outperforms other peers in retrieval precision and matching efficiency.']}
{'论文标题':['A method of determining the fuzzy measure based on knowledge factors'],'论文作者':['Fa-Chao Li', ';\xa0', 'Zan Zhang'],'论文摘要':['This paper presents a method to determine the fuzzy measure based on knowledge factors, in order to solve the problem of interaction among indices in the comprehensive evaluation, and make full use of the knowledge existing in the information system. The method can be used as a strategy for measuring the significance of the attributes. Furthermore, the Choquet integral is an important aggregation operator used in various fields. In combination with the Choquet integral, the method is applied to the assessment of graduate scholarship. The result shows that the method offers good interpretation and operation.']}
{'论文标题':['System for mining source code programs to discover previously unknown patterns, has processor for performing closed and maximal frequent subtree data mining analysis on trees to produce idioms and utilizing idioms to facilitate development'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Local Graph Edge Partitioning'],'论文作者':['Ji, SW', ';\xa0', 'Bu, CY', ';\xa0', 'Wu, XD'],'论文摘要':['Graph edge partitioning, which is essential for the efficiency of distributed graph computation systems, divides a graph into several balanced partitions within a given size to minimize the number of vertices to be cut. Existing graph partitioning models can be classified into two categories: offline and streaming graph partitioning models. The former requires global graph information during the partitioning, which is expensive in terms of time and memory for large-scale graphs. The latter creates partitions based solely on the received graph information. However, the streaming model may result in a lower partitioning quality compared with the offline model. Therefore, this study introduces a Local Graph Edge Partitioning model, which considers only the local information (i.e., a portion of a graph instead of the entire graph) during the partitioning. Considering only the local graph information is meaningful because acquiring complete information for large-scale graphs is expensive. Based on the Local Graph Edge Partitioning model, two local graph edge partitioning algorithms Two-stage Local Partitioning and Adaptive Local Partitioning are given. Experimental results obtained on 14 real-world graphs demonstrate that the proposed algorithms outperform rival algorithms in most tested cases. Furthermore, the proposed algorithms are proven to significantly improve the efficiency of the real graph computation system GraphX.']}
{'论文标题':['Error awareness data mining'],'论文作者':['Zhu, XQ', ' (', 'Zhu, Xingquan', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':['Real-world data mining applications often deal with low-quality information sources where data collection inaccuracy, device limitations, data transmission and discretization errors, or man-made perturbations frequently result in imprecise or vague data. Two common practices are to adopt either data cleansing to enhance data consistency or simply take noisy data as quality sources and feed them into the data mining algorithms. Either way may substantially sacrifice the mining performances. In this paper, we consider an error awareness data mining framework, which takes advantage of statistical error information (such as noise level and noise distribution) to improve data mining results. We assume such noise knowledge is-available in advance, and propose a solution to incorporate it into the mining process. More specifically, we use noise knowledge to restore original data distributions, and then use the restored information to modify the model built from noise corrupted data. We present an Error Awareness Naive Bayes (EA_NB) classification algorithm, and provide extensive experimental comparisons to demonstrate the effectiveness of this effort.']}
{'论文标题':['Rule Synthesizing from Multiple Related Databases'],'论文作者':['He, D', ' (', 'He, Dan', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', ', ';\xa0', 'Zhu, XQ', ' (', 'Zhu, Xingquan', ') ', ', '],'论文摘要':['In this paper, we study the problem of rule synthesizing from multiple related databases where items representing the databases may be different, and the databases may not be relevant, or similar to each other. We argue that, for such multi-related databases, simple rule synthesizing without a detailed understanding of the databases is not able to reveal meaningful patterns inside the data collections. Consequently, we propose a. two-step clustering on the databases at both item and rule levels such that the databases in the final clusters contain both similar items and similar rules. A weighted rule synthesizing method is then applied on each such cluster to generate final rules. Experimental results demonstrate that the new rule synthesizing method is able to discover important rules which can not be synthesized by other methods.']}
{'论文标题':['Learning causal representations for robust domain adaptation [arXiv]'],'论文作者':['Shuai Yang', ';\xa0', 'Kui Yu', ';\xa0', 'Jiuyong Li'],'论文摘要':['Domain adaptation solves the learning problem in a target domain by leveraging the knowledge in a relevant source domain. While remarkable advances have been made, almost all existing domain adaptation methods heavily require large amounts of unlabeled target domain data for learning domain invariant representations to achieve good generalizability on the target domain. In fact, in many real-world applications, target domain data may not always be available. In this paper, we study the cases where at the training phase the target domain data is unavailable and only well-labeled source domain data is available, called robust domain adaptation. To tackle this problem, under the assumption that causal relationships between features and the class variable are robust across domains, we propose a novel Causal AutoEncoder (CAE), which integrates deep autoencoder and causal structure learning into a unified model to learn causal representations only using data from a single source domain. Specifically, a deep autoencoder model is adopted to learn low-dimensional representations, and a causal structure learning model is designed to separate the low-dimensional representations into two groups: causal representations and task-irrelevant representations. Using three real-world datasets the extensive experiments have validated the effectiveness of CAE compared to eleven state-of-the-art methods.']}
{'论文标题':['Multi-Objective Optimization-Based Networked Multi-Label Active Learning'],'论文作者':['Li, L', ' (', 'Li, Lei', ') ', ';\xa0', 'Chu, YQ', ' (', 'Chu, Yuqi', ') ', ';\xa0', 'Liu, GF', ' (', 'Liu, Guanfeng', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':['Along with the fast development of network applications, network research has attracted more and more attention, where one of the most important research directions is networked multi-label classification. Based on it, unknown labels of nodes can be inferred by known labels of nodes in the neighborhood. As both the scale and complexity of networks are increasing, the problems of previously neglected system overhead are turning more and more seriously. In this article, a novel multi-objective optimization-based networked multi-label seed node selection algorithm (named as MOSS) is proposed to improve both the prediction accuracy for unknown labels of nodes from labels of seed nodes during classification and the system overhead for mining the labels of seed nodes with third parties before classification. Compared with other algorithms on several real networked data sets, MOSS algorithm not only greatly reduces the system overhead before classification but also improves the prediction accuracy during classification.']}
{'论文标题':['L1 regularized ordering for learning Bayesian network classifiers'],'论文作者':['Ying Wang', ';\xa0', 'Hao Wang', ';\xa0', 'Hongliang Yao'],'论文摘要':['Learning a Bayesian network classifier from data is an active research topic in data mining. The key problem for constructing a Bayesian network classifier is to learn an accurate Bayesian network structure which is a difficult task. The K2 algorithm, as one of the most efficient Bayesian network learning methods can deal with this difficult task. However, K2 requires a variable ordering in advance. Existing methods for establishing this ordering neglect information of the variables selected. To address this problem, in this paper, we propose an L1 regularized Bayesian network classifier (L1-BNC). L1-BNC defines a variable ordering by the LARS (Least Angle Regression) method, and then with this ordering it uses K2 to construct a Bayesian network classifier. In comparison with seven Bayesian network classifiers, L1-BNC outperforms those classifiers on most datasets.']}
{'论文标题':['Numerical sensitive data recognition based on hybrid gene expression programming for active distribution networks'],'论文作者':['Deng, S', ';\xa0', 'Xie, XP', ';\xa0', 'Wu, XD'],'论文摘要':['Complex and flexible access mode, and frequent data interaction bring about large security risks to data transmission for active distribution networks. How to ensure data security is critical to the safe and stable operation of active distribution networks. Traditional methods, like access control, data encryption, and text filtering based on intelligent algorithms, are difficult to ensure the security of dynamically increased and high-dimensional numerical data transmission in active distribution networks. In this paper, we first propose a rough feature selection algorithm based on the average importance measurement (RFS-AIM) to simplify the complexity of data recognition. Then, we propose a sensitive data recognition function mining algorithm based on RFS-AIM and improved gene expression programming (SDR-IGEP) where population update operation is constructed by chromosome similarity based on the Jaccard coefficient. The operation avoids local convergence of the gene express programming by increasing individual diversity in the new population. Finally, we present a new incremental mining algorithm for a sensitive data recognition function based on global function fitting (ISDR-GFF) by using a grain granulation model for incremental datasets. The experimental results on IEEE benchmark datasets and real datasets show that the algorithms proposed in this paper outperform the state-of-the-art algorithms in terms of the average running time, precision, recall, F-1 index, accuracy, specificity and speedup on all experimental datasets. (C) 2020 Elsevier B.V. All rights reserved.']}
{'论文标题':['Mining fuzzy association rules: Interestingness measure and algorithm'],'论文作者':['Han, JC', ';\xa0', 'Beheshti, M'],'论文摘要':['Discovering association rules is an important task of data mining research. Mining traditional association rules is built upon transaction databases. This technique, however, has some limitations. Each transaction merely contains binary items with each item either present or absent in a transaction. Another limitation is that only positive association rules are discovered. Mining fuzzy association rules and discovering negative association rules have been developed to overcome these limitations, respectively. In this paper, we combine these two approaches to propose a novel approach for mining both positive and negative fuzzy association rules. The interestingness measure for both positive and negative fuzzy association rule is proposed, the algorithm for mining these rules is described.']}
{'论文标题':['Iterative Lasso based on feature selection for high dimensional data'],'论文作者':['Shi Wan-feng', ';\xa0', 'Hu Xue-gang', ';\xa0', 'Yu Kui'],'论文摘要':['With a high-dimensional and large dataset, like other feature selection methods, Lasso encounters the problems of large computation and overfitting. To address this issue, this paper proposed an improved Lasso method: iterative Lasso method. Iterative Lasso method first divided the feature set into K copies. Then it selected the features from the first feature subset, put the selected features into the second feature subset, and continued this iteration until up to the Kth feature subset. Experimental results show that the iterative Lasso method can effectively deal with the high-dimensional and large sample datasets.', 'Lasso方法与其他特征选择一样，对高维海量或高维小样本数据集的特征选择容易出现计算开销过大或过学习问题（过拟合）。为解决此问题，提出一种改进的Lasso方法：迭代式Lasso方法。迭代式Lasso方法首先将特征集分成K份，对第一份特征子集进行特征提取，将所得特征加入第二份，再对第二份特征进行特征提取：然后将所得特征加入第三份，依次迭代下去，直到第K份，得到最终特征子集。实验表明，迭代式Lasso方法能够很好地对高维海量或高维小样本数据集进行特征选择，是一种有效的特征选择方法。目前，此方法已经很好地应用在高维海量和高维小样本数据的分类或预测模型中。']}
{'论文标题':['Shared-view and specific-view information extraction for recommendation'],'论文作者':['Liu, HT', ';\xa0', 'Zhao, JD', ';\xa0', 'Wu, XD'],'论文摘要':['In various recommender systems, ratings and reviews are the main information to show user preferences. However, recommendation models that only use ratings, such as collaborative filtering, are vulnerable to data sparsity. And models only using review information will also suffer from the sparsity of reviews. On one hand, most ratings and reviews are interrelated and complementary, reviews may explain why a user gives a high or low rating to an item. On the other hand, ratings and reviews are numerical and textual information, respectively, and they reflect the preference of the user from a coarse-grained level and a fine-grained level A user may comment positively about some aspects of an item, even he gives a very low score to this item. There are specific information among each of them because of their heterogeneity. Therefore, it is possible to learn more accurate representation of users and items by effectively integrating ratings and text reviews from different views, that is, shared-view and specific-view. In this paper, we propose a Shared-view and Specific-view Information extraction model for Recommendation (SSIR), which integrates the information from reviews and interaction matrix to predict ratings Our model has two key components, including shared-view information extraction and specific-view exploitation. From the perspective of shared-view, SSIR jointly minimizes the loss of confusion adversarial and rating prediction loss to extract the shared information from reviews and user-item interaction matrix. For the specific-view part, SSIR applies orthogonal constraints on shared-view and specific-view modules to extract the discriminative features from reviews and interaction data. We fuse the features extracted from these two views to predict the final ratings. In addition, we use auxiliary reviews to deal with the sparsity problem of reviews. Experimental results on eight datasets show the effectiveness and robustness of our method, which could adapt to the recommendation scenarios with fewer reviews and ratings.']}
{'论文标题':['Clustering Web Services for Automatic Categorization'],'论文作者':['Liang, QH', ';\xa0', 'Li, PP', ';\xa0', 'Wu, XD'],'论文摘要':['Analyzing the functionality of Web services is the basis of using Web services effectively and efficiently. The first step in such an analysis of Web services is to categorize different services, which may be offered by different service providers, based on their functionalities. In this paper, we present a clustering-based approach to Web service categorization in order to form a hierarchy of service taxonomy. Our novel clustering scheme takes into consideration not only individual factors such as input or output of service operations, but also the latent inter-relationships among the individual factors. Given a set of services that may or may not have been categorized, we adopt individual methods to handle the issue and mark out their classification labels in terms of a common (given) taxonomy, such as UNSPSC. When a new service description is published, the unclassified service is compared with the classified ones and measures of the likelihood that the new service description is belonging to each cluster are calculated. Based on this calculation, the service will be assigned to a suitable category.']}
{'论文标题':['Support vector machines based on K-means clustering for real-time business intelligence systems'],'论文作者':['Jiaqi Wang', ';\xa0', 'Xindong Wu', ';\xa0', 'Chengqi Zhang'],'论文摘要':['Support vector machines (SVM) have been applied to build classifiers, which can help users make well-informed business decisions. Despite their high generalization accuracy, the response time of SVM classifiers is still a concern when applied into real-time business intelligence systems, such as stock market surveillance and network intrusion detection. This paper speeds up the response of SVM classifiers by reducing the number of support vectors. This is done by the K-means SVM (KMSVM) algorithm proposed in this paper. The KMSVM algorithm combines the K-means clustering technique with SVM and requires one more input parameter to be determined: the number of clusters. The criterion and strategy to determine the input parameters in the KMSVM algorithm are given in this paper. Experiments compare the KMSVM algorithm with SVM on real-world databases, and the results show that the KMSVM algorithm can speed up the response time of classifiers by both reducing support vectors and maintaining a similar testing accuracy to SVM.']}
{'论文标题':['Learning common and label-specific features for multi-Label classification with correlation information'],'论文作者':['Li, JL', ';\xa0', 'Li, PP', ';\xa0', 'Yu, K'],'论文摘要':['In multi-label classification, many existing works only pay attention to the label-specific features and label correlation while they ignore the common features and instance correlation, which are also essential for building a competitive classifier. Besides, existing works usually depend on the assumption that they tend to have the similar label-specific features if two labels are correlated. However, this assumption cannot always hold in some cases. Therefore, in this paper, we propose a new approach of learning common and label-specific features for multi-label classification using the correlation information from labels and instances. First, we introduce l(2,1)-norm and l(1)-norm regularizers to learn common and label-specific features simultaneously. Second, we use a regularizer to constrain label correlations on label outputs instead of coefficient matrix. Finally, instance correlations are also considered through the k-nearest neighbor mechanism. Comprehensive experiments manifest the superiority of our proposed approach against other well-established multi-label learning algorithms for label-specific features. (C) 2021 Elsevier Ltd. All rights reserved.']}
{'论文标题':['Noise handling with extension matrices'],'论文作者':['Wu, X.-D.', ';\xa0', 'Krisar, J.', ';\xa0', 'Mahlen, P.'],'论文摘要':["HCV is a heuristic attribute based induction algorithm based on the newly developed extension matrix approach. HCV divides the positive examples (PE) of a specific class in a given example set into intersecting groups and adopts a set of strategies to find a heuristic conjunctive formula in each group which covers all the group's positive examples and none of the negative examples (NE); it can therefore find a covering formula in the form of variable valued logic for PE against NE in low order polynomial time. The original algorithm performs quite well with those data sets where noise and continuous data are not of major concern. However, its performance decreases when the data sets are noisy and contain continuous attributes. The paper presents noise handling techniques developed and implemented in HCV (Version 2.0), a noise tolerant version of the HCV algorithm, and provides a performance comparison of HCV with other inductive algorithms C4.5 and NewID in noisy and continuous domains."]}
{'论文标题':['Price Competition in a Duopoly IaaS Cloud Market'],'论文作者':['Li, XW', ';\xa0', 'Gu, B', ';\xa0', 'Tanaka, Y'],'论文摘要':["Pricing cloud resources plays an important role in leading to the success of cloud computing. Cloud services are priced at different levels in infrastructure-as-a-service (IaaS) cloud market. For example, Amazon EC2 offers its cloud resources with three pricing schemes, the subscription model, pay-as-you-go model and spot pricing model. With more and more IaaS cloud service providers (CSPs) beginning to provide cloud services, they form a competitive market to compete for cloud users. Therefore, how to set optimal prices in order to maximize their revenue in a competitive IaaS cloud computing market while at the same time meeting the cloud users' demand satisfaction is a problem that CSPs should consider. Towards this end, in this paper, we study subscription pricing competition in a duopoly IaaS cloud computing market. First, we analyze whether or not the cloud users choose to use cloud service. Then, we present a game theoretic analysis of a cloud market with two CSPs competing non-cooperatively for cloud users."]}
{'论文标题':['Chinese idiom teaching frame, has bottom plate fixed with supporting rod, and movable plate provided with square opening corresponding to right end of idiom magnetic white board, where square opening is connected with shielding magnetic white board'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Representation learning via serial autoencoders for domain adaptation'],'论文作者':['Yang, S', ' (', 'Yang, Shuai', ') ', ';\xa0', 'Zhang, YH', ' (', 'Zhang, Yuhong', ') ', ';\xa0', 'Zhu, Y', ' (', 'Zhu, Yi', ') ', ';\xa0', 'Li, PP', ' (', 'Li, Peipei', ') ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') ', ', '],'论文摘要':['Domain adaption aims to promote the learning tasks in target domain by using the knowledge from source domain whose data distribution is different from target domain. The crucial problem in domain adaptation is learning more robust and higher-level feature representations to reduce the distribution divergences. Recently, deep learning methods based on autoencoders have been successfully applied in representation learning for domain adaptation. However, most existing methods of autoencoders rely on the single autoencoder model, which poses challenges for learning different characteristics of data. In this paper, we propose a new representation learning framework via serial autoencoders (SEAE), which extracts richer feature representations by serially connecting two different types of autoencoders. This framework consists of two encoding stages. In the first encoding stage, marginalized denoising autoencoder (MDAE) is applied to learn domain invariant features which occurred in both domains frequently. With the result of the first stage, stacked robust autoencoder via graph regularization (SRAEG) is used in the second encoding stage to improve the quality of feature representations. Additionally, SRAEG model can be computed in a closed form with less time cost. Experimental results demonstate the effectiveness of our proposed approach compared with several state-of-the-art baseline methods. (C) 2019 Elsevier B.V. All rights reserved.']}
{'论文标题':['Markov Boundary-Based Outlier Mining'],'论文作者':['Yu, K', ';\xa0', 'Chen, HH'],'论文摘要':['It is a grand challenge to identify the outliers existing in subspaces from a high-dimensional data set. A brute-force method is computationally prohibitive since it requires examining an exponential number of subspaces. Current state-of-the-art methods explore various heuristics to significantly prune subspaces, facing the tradeoff between the subspace completeness and search efficiency. In this brief, we discuss a principal type of subspace outliers whose behaviors are different from the others on individual attributes. We formulate such outliers by a novel notion of the Markov boundary-based (MBB) outliers. The central idea is that for each attribute T in a data set, we consider only the subspace representing the knowledge needed to predict the behavior on T, which is captured by the MB of T. Then, the outliers whose behavior is different from others on T can be detected in the subspace of the MB, and thus, our approach reduces the number of possible subspaces from exponential to linear with respect to dimensionality. Using both synthetic and real data sets, we validate the effectiveness and efficiency of our method.']}
{'论文标题':['Triangulation of Bayesian networks using an adaptive genetic algorithm'],'论文作者':['Wang, H', ';\xa0', 'Yu, K', ';\xa0', 'Yao, HL'],'论文摘要':['The search for an optimal node elimination sequence for the triangulation of Bayesian networks is an NP-hard problem. In this paper, a new method, called the TAGA algorithm, is proposed to search for the optimal node elimination sequence. TAGA adjusts the probabilities of crossover and mutation operators by itself, and provides an adaptive ranking-based selection operator that adjusts the pressure of selection according to the evolution of the population. Therefore the algorithm not only maintains the diversity of the population and avoids premature convergence, but also improves on-line and off-line performances. Experimental results show that the TAGA algorithm outperforms a simple genetic algorithm, an existing adaptive genetic algorithm, and simulated annealing on three Bayesian networks.']}
{'论文标题':['Enhanced Gabor Feature Based Classification Using a Regularized Locally Tensor Discriminant Model for Multiview Gait Recognition'],'论文作者':['Hu, HF', ' (', 'Hu, Haifeng', ') '],'论文摘要':['This paper presents a novel multiview gait recognition method that combines the enhanced Gabor (EG) representation of the gait energy image and the regularized local tensor discriminant analysis (RLTDA) method. EG first derives desirable gait features characterized by spatial frequency, spatial locality, and orientation selectivity to cope with the variations due to surface, shoe types, clothing, carrying conditions, and so on. Unlike traditional Gabor transformation, which does not consider the structural characteristics of the gait features, our representation method not only considers the statistical property of the input features but also adopts a nonlinear mapping to emphasize those important feature points. The dimensionality of the derivation of EG gait feature is further reduced by using RLTDA, which directly obtains a set of locally optimal tensor eigenvectors and can capture nonlinear manifolds of gait features that exhibit appearance changes due to variable viewing angles. An aggregation scheme is adopted to combine the complementary information from differently RLTDA recognizers at the matching score level. The proposed method achieves the best average Rank-1 recognition rates for multiview gait recognition based on image sequences from the USF HumanID gait challenge database and the CASIA gait database.']}
{'论文标题':['An improved CapsNet applied to recognition of 3D vertebral images'],'论文作者':['Wang, H', ';\xa0', 'Shao, K', ';\xa0', 'Huo, X'],'论文摘要':["Deep learning is currently widely applied in medical image processing and has achieved good results. However, recognizing vertebrae via image processing remains a challenging problem due to their complex spatial structures. CapsNet is a newly proposed network whose characteristics compensate for some shortcomings of traditional CNNs, and it has been shown to perform well on many tasks, including medical image recognition. In this paper, we applied a modified CapsNet to recognise 3D vertebral images by introducing an RNN module into CapsNet to further enhance its learning ability. This new network is called RNNinCaps, and it achieves the highest recognition performance on 3D vertebral images (the average accuracy of RNNinCaps exceeds the accuracy of the original CapsNet by 46.2% and that of a traditional CNN by 12.6%). RNNinCaps also performs better than several mainstream networks. RNNinCaps can promotes CapsNet's application in the field of 3D medical image recognition."]}
{'论文标题':['Multi-Instance Learning with Discriminative Bag Mapping'],'论文作者':['Wu, J', ';\xa0', 'Pan, SR', ';\xa0', 'Wu, XD'],'论文摘要':['Multi-instance learning (MIL) is a useful tool for tackling labeling ambiguity in learning because it allows a bag of instances to share one label. Bag mapping transforms a bag into a single instance in a new space via instance selection and has drawn significant attention recently. To date, most existing work is based on the original space, using all instances inside each bag for bag mapping, and the selected instances are not directly tied to an MIL objective. As a result, it is difficult to guarantee the distinguishing capacity of the selected instances in the new bag mapping space. In this paper, we propose a discriminative mapping approach for multi-instance learning (MILDM) that aims to identify the best instances to directly distinguish bags in the new mapping space. Accordingly, each instance bag can be mapped using the selected instances to a new feature space, and hence any generic learning algorithm, such as an instance-based learning algorithm, can be used to derive learning models for multi-instance classification. Experiments and comparisons on eight different types of real-world learning tasks (including 14 data sets) demonstrate that MILDM outperforms the state-of-the-art bag mapping multi-instance learning approaches. Results also confirm that MILDM achieves balanced performance between runtime efficiency and classification effectiveness.']}
{'论文标题':['Towards Scalable and Accurate Online Feature Selection for Big Data'],'论文作者':['Yu, K', ';\xa0', 'Wu, XD', ';\xa0', 'Pei, J'],'论文摘要':['Feature selection is important in many big data applications. There are at least two critical challenges. Firstly, in many applications, the dimensionality is extremely high, in millions, and keeps growing. Secondly, feature selection has to be highly scalable, preferably in an online manner such that each feature can be processed in a sequential scan. In this paper, we develop SAOLA, a Scalable and Accurate OnLine Approach for feature selection. With a theoretical analysis on a low bound on the pairwise correlations between features in the currently selected feature subset, SAOLA employs novel online pairwise comparison techniques to address the two challenges and maintain a parsimonious model over time in an online manner. An empirical study using a series of benchmark real data sets shows that SAOLA is scalable on data sets of extremely high dimensionality, and has superior performance over the state-ofthe-art feature selection methods.']}
{'论文标题':['Parallel algorithm for Bayesian networks parameter learning'],'论文作者':['Yu Kui', ';\xa0', 'Wang Hao', ';\xa0', 'Chen Dong-liang'],'论文摘要':['Because the EM algorithm requires significant computational resources for Bayesian networks parameter learning under large databases, the PL-EM algorithm is proposed to improve the learning speed. The PL-EM algorithm parallel computes the posteriori probabilities of hidden variables and expected sufficient statistics at E step;at M step, the algorithm makes use of conditional independence and the decomposability of the likelihood function to parallel compute each local likelihood function. Experimental results show the PL-EM algorithm is an effective method to solve Bayesian parameter learning for large datasets.', '针对大样本条件下EM算法学习贝叶斯网络参数的计算问题,提出一种并行EM算法(Parallel EM,PL-EM)提高大样本条件下复杂贝叶斯网络参数学习的速度.PL-EM算法在E步并行计算隐变量的后验概率和期望充分统计因子;在M步,利用贝叶斯网络的条件独立性和完整数据集下的似然函数可分解性,并行计算各个局部似然函数.实验结果表明PL-EM为解决大样本条件下贝叶斯网络参数学习提供了一种有效的方法.']}
{'论文标题':['Online Learning towards Big Data Analysis in Health Informatics'],'论文作者':['Wang, J', ';\xa0', 'Zhao, ZQ', ';\xa0', 'Gu, FQ'],'论文摘要':['The exponential increase of data in health informatics has brought a lot of challenges in terms of data transfer, storage, computation and analysis. One of the popular solutions to the above challenges is the cloud computing technology. However, the cloud computing technology requires high-performance computers and is only accessible with internet. In this paper, we introduce online learning and propose our method for data mining of big data in health informatics. In contrast to traditional data analysis scenario, online learning will preform the data analysis dynamically by the time the data are generated. The online learning method is efficient and especially adaptable to the online health care systems. We demonstrate the effectiveness of our online learning method on several real-world data sets.']}
{'论文标题':['Ensemble learning from multiple information sources via label propagation and consensus'],'论文作者':['Lin, YJ', ';\xa0', 'Hu, XG', ';\xa0', 'Wu, XD'],'论文摘要':['Many applications are facing the problem of learning from multiple information sources, where sources may be labeled or unlabeled, and information from multiple information sources may be beneficial but cannot be integrated into a single information source for learning. In this paper, we propose an ensemble learning method for different labeled and unlabeled sources. We first present two label propagation methods to infer the labels of training objects from unlabeled sources by making a full use of class label information from labeled sources and internal structure information from unlabeled sources, which are processes referred to as global consensus and local consensus, respectively. We then predict the labels of testing objects using the ensemble learning model of multiple information sources. Experimental results show that our method outperforms two baseline methods. Meanwhile, our method is more scalable for large information sources and is more robust for labeled sources with noisy data.']}
{'论文标题':['Fault characteristic parameter selection method based on fuzzy preference relations and adaptive hierarchical clustering, comprises calculating fuzzy preference relation and performing redundant feature removal'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Semi-supervised geometric mean of Kullback-Leibler divergences for subspace selection'],'论文作者':['Si-Bao Chen', ';\xa0', 'Hai-Xian Wang', ';\xa0', 'Xing-Yi Zhang', ';\xa0', 'Bin Luo'],'论文摘要':['Subspace selection is widely adopted in many areas of pattern recognition. A recent result, named maximizing the geometric mean of Kullback-Leibler (KL) divergences of class pairs (MGMD), is a successful method for subspace selection, which can significantly reduce the class separation problem. However, in many applications, labeled data are very limited while unlabeled data can be easily obtained. The estimation of divergences of class pairs is unstable using inadequate labeled data. To take advantage of unlabeled data for subspace selection, semi-supervised MGMD (SSMGMD) is proposed using graph Laplacian as normalization. Quasi-Newton method is adopted to solve the optimization problem. Experiments on synthetic data and real image data show the validity of SSMGMD.']}
{'论文标题':['Exploring video content structure for hierarchical summarization'],'论文作者':['Zhu, XQ', ' (', 'Zhu, XQ', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, XD', ') ', ';\xa0', 'Fan, JP', ' (', 'Fan, JP', ') ', ';\xa0', 'Elmagarmid, AK', ' (', 'Elmagarmid, AK', ') ', ';\xa0', 'Aref, WF', ' (', 'Aref, WF', ') '],'论文摘要':['In this paper, we propose a hierarchical video summarization strategy that explores video content structure to provide the users with a scalable, multilevel video summary. First, video-shot- segmentation and keyframe-extraction algorithms are applied to parse video sequences into physical shots and discrete keyframes. Next, an affinity (self-correlation) matrix is constructed to merge visually similar shots into clusters (supergroups). Since video shots with high similarities do not necessarily imply that they belong to the same story unit, temporal information is adopted by merging temporally adjacent shots (within a specified distance) from the supergroup into each video group. A video-scene-detection algorithm is thus proposed to merge temporally or spatially correlated video groups into scenario units. This is followed by a scene-clustering algorithm that eliminates visual redundancy among the units. A hierarchical video content structure with increasing granularity is constructed from the clustered scenes, video scenes, and video groups to keyframes. Finally, we introduce a hierarchical video summarization scheme by executing various approaches at different levels of the video content hierarchy to statically or dynamically construct the video summary. Extensive experiments based on real-world videos have been performed to validate the effectiveness of the proposed approach.']}
{'论文标题':['Pattern matching with wildcards based on key character location'],'论文作者':['Yingling Liu', ';\xa0', 'Xindong Wu', ';\xa0', 'Xiaoli Hong'],'论文摘要':['Pattern matching with wildcards is a complex problem and this problem has wide potential application in text search, biological sequences and information security etc. We propose a new algorithm called Quicksearch, for pattern matching with wildcards and length constraints based on key character location and subspace partition. This new algorithm increases by 40%-60% searching efficiency in comparison with SAIL when characters of pattern P in text T are unevenly distributed.']}
{'论文标题':['Subject Event Extraction from Chinese Court Verdict Case via Frame-filling'],'论文作者':['Wu, GQ', ' (', 'Wu, Gongqing', ') ', ', ', ';\xa0', 'Hu, SJ', ' (', 'Hu, Shengjie', ') ', ', ', ';\xa0', 'Wang, YH', ' (', 'Wang, Yinghuan', ') ', ', ', ';\xa0', 'Zhang, Z', ' (', 'Zhang, Zan', ') ', ', ', ';\xa0', 'Bao, XY', ' (', 'Bao, Xianyu', ') '],'论文摘要':['At present, the query and acquisition of the fragmented knowledge in Chinese court verdicts mainly adopt the class case retrieval method based on the search engine and the rough extraction method for a part of the data in court verdicts. These traditional methods cannot structurally extract fragmented knowledge in Chinese court verdicts and meet the needs of people for the follow-up analysis of court verdicts. Thus, in this paper, we present a structured subject event extraction method (SEE) for Chinese court verdict cases combining with techniques of event extraction (EE) and attribute-value pair extraction (AVPE). Specifically, we provide a subject event representation frame for organizing fragmented knowledge in Chinese court verdict cases. Then, we extract subject events from the unstructured cases based on the trained sequence labeling models and constructed heuristic rules, and fill them into the subject event representation frame in the form of attribute-value pairs (AVPs). The experimental results show that SEE can efficiently and automatically extract subject events from Chinese court verdict cases and visually display them via frame-filling, which promotes the efficiency of people in searching for legal materials and facilitates further research and analysis.']}
{'论文标题':['A Self-adaptive Sliding Window based Topic Model for Non-uniform Texts'],'论文作者':['He, J', ';\xa0', 'Li, L', ';\xa0', 'Wu, XD'],'论文摘要':['The contents generated from different data sources are usually non-uniform, such as long texts produced by news websites and short texts produced by social media. Uncovering topics over large-scale non-uniform texts becomes an important task for analyzing network data. However, the existing methods may fail to recognize the difference between long texts and short texts. To address this problem, we propose a novel topic modeling method for non-uniform text topic modeling referred to as self-adaptive sliding window based topic model (SSWTM). Specifically, in all kinds of texts, relevant words have a closer distance to each other than irrelevant words. Based on this assumption, SSWTM extracts relevant words by using a self-adaptive sliding window and models on the whole corpus. The self-adaptive sliding window can filter noisy information and change the size of a window according to different text contents. Experimental results on short texts from Twitter and long texts from Chinese news articles demonstrate that our method can discover more coherent topics for non-uniform texts compared with state-of-the-art methods.']}
{'论文标题':['How to Estimate the Regularization Parameter for Spectral Regression Discriminant Analysis and its Kernel Version?'],'论文作者':['Gui, J', ' (', 'Gui, Jie', ') ', ', ', ';\xa0', 'Sun, ZN', ' (', 'Sun, Zhenan', ') ', ';\xa0', 'Cheng, J', ' (', 'Cheng, Jun', ') ', ';\xa0', 'Ji, SW', ' (', 'Ji, Shuiwang', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':['Spectral regression discriminant analysis (SRDA) has recently been proposed as an efficient solution to large-scale subspace learning problems. There is a tunable regularization parameter in SRDA, which is critical to algorithm performance. However, how to automatically set this parameter has not been well solved until now. So this regularization parameter was only set to be a constant in SRDA, which is obviously suboptimal. This paper proposes to automatically estimate the optimal regularization parameter of SRDA based on the perturbation linear discriminant analysis (PLDA). In addition, two parameter estimation methods for the kernel version of SRDA are also developed. One is derived from the method of optimal regularization parameter estimation for SRDA. The other is to utilize the kernel version of PLDA. Experiments on a number of publicly available databases demonstrate the effectiveness of the proposed methods for face recognition, spoken letter recognition, handwritten digit recognition, and text categorization.']}
{'论文标题':['Multi-Label Truth Inference for Crowdsourcing Using Mixture Models'],'论文作者':['Zhang, J', ';\xa0', 'Wu, XD'],'论文摘要':['When acquiring labels from crowdsourcing platforms, a task may be designed to include multiple labels and the values of each label may belong to a set of various distinct options, which is the so-called multi-class multi-label annotation. To improve the quality of labels, requesters usually let one task be independently completed by a group of heterogeneous crowdsourced workers. Then, the true values of the multiple labels of each task are inferred from these repeated noisy labels. In this paper, we propose two novel probabilistic models MCMLI and MCMLD to address the multi-class multi-label inference problem in crowdsourcing. MCMLI assumes that the labels of each task are mutually independent and MCMLD utilizes a mixture of multiple independently multinoulli distributions to capture the correlation among the labels. Both models can jointly infer multiple true labels of each instance as well as estimate the reliability of crowdsourced workers modeled by a set of confusion matrices with an expectation-maximization algorithm. Experiments with three typical crowdsourcing scenarios and a real-world dataset show that our proposed models significantly outperform existing competitive alternatives. When the labels are strongly correlated, MCMLD substantially outperforms MCMLI. Furthermore, our models can be easily simplified to the one-coin models, which show more advantageous when errors are uniformly distributed, or labels are sparse.']}
{'论文标题':['Spam Filtering Method based on Learning from Small Samples'],'论文作者':['Pan Jie-zhu', ';\xa0', 'Zhou Xiao', ';\xa0', 'Hu Xue-gang'],'论文摘要':['It is difficult to collect sufficient labeled E-mails for training a client spam classifier. Aiming at the problem, this paper proposes a spam filtering method based on learning from small samples, which improves the filtering performance with unlabeled samples. An initial Naive Bayes(NB) classifier is trained with a dataset of labeled E-mails, and unlabeled E-mails are probabilistically labeled with it. A new classifier is trained with all E-mails, and iterates to convergence with EM algorithm. Experimental results prove that, given labeled small training samples with a size of 5 to 20, the performance of spam filtering can be effectively improved.', '针对客户端垃圾邮件过滤器难以获取足够训练样本的问题, 提出一种基于小样本学习的垃圾邮件过滤方法, 利用容易获取的未标记样本提高垃圾邮件过滤的性能. 该方法使用已标记的小样本邮件实例集训练一个初始Na?ve Bayes分类器, 以此标注未标记邮件, 再使用所有数据训练新的分类器, 利用EM算法进行迭代直至收敛. 实验结果证明, 当给定5个~20个已标记小样本训练邮件时, 该方法可有效提高垃圾邮件过滤性能']}
{'论文标题':['A Simple BERT-Based Approach for Lexical Simplification [arXiv]'],'论文作者':['Jipeng Qiang', ';\xa0', 'Yun Li', ';\xa0', 'Xindong Wu'],'论文摘要':['Lexical simplification (LS) aims to replace complex words in a given sentence with their simpler alternatives of equivalent meaning. Recently unsupervised lexical simplification approaches only rely on the complex word itself regardless of the given sentence to generate candidate substitutions, which will inevitably produce a large number of spurious candidates. We present a simple BERT-based LS approach that makes use of the pre-trained unsupervised deep bidirectional representations BERT. Despite being entirely unsupervised, experimental results show that our approach obtains obvious improvement than these baselines leveraging linguistic databases and parallel corpus, outperforming the state-of-the-art by more than 11 Accuracy points on three well-known benchmarks.']}
{'论文标题':['A practical algorithm for solving the sparseness problem of short text clustering'],'论文作者':['Qiang, JP', ';\xa0', 'Li, Y', ';\xa0', 'Wu, XD'],'论文摘要':['Dirichlet Multinomial Mixture (DMM) models have been successful in clustering short texts. However, the word co-occurrence information that can be captured by these models is limited to the short text corpus itself. If two words have strong relatedness but rarely co-occurring in short texts, these models can not fully capture the semantic relatedness between the two words. In this paper, we propose a novel model by incorporating word-word correlation into DMM, called WDMM. By constructing a sparse graph using word-word relationship, our model expands each short text using their neighboring words in each text that can help to solve the problem of sparseness in short texts. Therefore, the cluster label of each text is not only influenced by its words, but decided by their similar words in this corpus. Experimental results on real-world datasets demonstrated the substantial superiority of our WDMM model over the state-of-the-art methods.']}
{'论文标题':['Unsupervised Statistical Text Simplification'],'论文作者':['Qiang, JP', ';\xa0', 'Wu, XD'],'论文摘要':['Most recent approaches for Text Simplification (TS) have drawn on insights from machine translation to learn simplification rewrites from the monolingual parallel corpus of complex and simple sentences, yet their effectiveness strongly relies on large amounts of parallel sentences. However, there has been a serious problem haunting TS for decades, that is, the availability of parallel TS corpora is scarce or not fit for the learning task. In this paper, we will focus on one especially useful and challenging problem of unsupervised TS without a single parallel sentence. To the best of our knowledge, we present the first unsupervised text simplification system based on phrase-based machine translation system, which leverages a careful initialization of phrase tables and language models. On the widely used WikiLarge and WikiSmall benchmarks, our system respectively obtains 39.08 and 25.12 SARI points, even outperforms some supervised baselines.']}
{'论文标题':['Chinese Temporal Expression Recognition Combining Rules with a Statistical Model'],'论文作者':['Mengmeng Huang', ';\xa0', 'Jiazhu Xia', ';\xa0', 'Gongqing Wu'],'论文摘要':['Traditional rule-based methods for recognizing Chinese temporal expressions present a lower recall rate and they cannot recognize the event-type Chinese temporal expressions, thus, we propose a new Chinese temporal expression recognition method through combining rules with a statistical model. Firstly, we divide Chinese temporal expressions into seven categories and use basic time units as the smallest unit of recognition to simplify the complexity of rule-making. Then, we use regular rules to recognize Chinese temporal expressions and label the training data automatically. Meanwhile, we label the event-type temporal expressions that rule-based method cannot recognize. Lastly, we use the labeled training data to learn a Conditional Random Fields model for Chinese temporal expression recognition. Experimental results show that our proposed method significantly reduces the amount of annotation work and effectively improves the recognition performance. The F1 value reaches 88.73%, which is higher than the rule-based method by 6.13%.']}
{'论文标题':['Feature Interaction for Streaming Feature Selection'],'论文作者':['Zhou, P', ';\xa0', 'Li, PP', ';\xa0', 'Wu, XD'],'论文摘要':['Traditional feature selection methods assume that all data instances and features are known before learning. However, it is not the case in many real-world applications that we are more likely faced with data streams or feature streams or both. Feature streams are defined as features that flow in one by one over time, whereas the number of training examples remains fixed. Existing streaming feature selection methods focus on removing irrelevant and redundant features and selecting the most relevant features, but they ignore the interaction between features. A feature might have little correlation with the target concept by itself, but, when it is combined with some other features, they can be strongly correlated with the target concept. In other words, the interactive features contribute to the target concept as an integer greater than the sum of individuals. Nevertheless, most of the existing streaming feature selection methods treat features individually, but it is necessary to consider the interaction between features. In this article, we focus on the problem of feature interaction in feature streams and propose a new streaming feature selection method that can select features to interact with each other, named Streaming Feature Selection considering Feature Interaction (SFS-FI). With the formal definition of feature interaction, we design a new metric named interaction gain that can measure the interaction degree between the new arriving feature and the selected feature subset. Besides, we analyzed and demonstrated the relationship between feature relevance and feature interaction. Extensive experiments conducted on 14 real-world microarray data sets indicate the efficiency of our new method.']}
{'论文标题':['Cross-domain sentiment classification-feature divergence, polarity divergence or both?'],'论文作者':['Zhang, YH', ';\xa0', 'Hu, XG', ';\xa0', 'Wu, XD'],'论文摘要':["Sentiment classification, which aims to predict the polarity of users' viewpoint hidden in reviews, is a domain specific problem, it often fails to be tested in one domain as a classifier trained from another domain. It is hence significant and challenging for cross domain sentiment classification due to the following two cases: (1) the same feature is used to express different sentiments in different domains (we call it polarity divergence), and (2) different features are used to express similar sentiments in different domains (we call it feature divergence). Existing efforts focus on the latter and consider little on the former, in this paper, we consider both cases in cross-domain sentiment classification and propose a novel algorithm by transferring the polarity of features (TPF). Since the polarity of features is informative for sentiment classification, our algorithm transfers the polarity of features from the source domain to the target domain with the independent features as the bridge. It is worth to note that the polarities of independent features are reset when they are involved in the former case. In addition, the resetting of independent features' polarities in our algorithm can also be used as a preprocessing step in existing efforts. Empirical results show that our proposed method outperforms state-of-the-art methods in cross-domain sentiment classification. (C) 2015 Elsevier B.V. All rights reserved."]}
{'论文标题':['Robust ensemble learning for mining noisy data streams'],'论文作者':['Zhang, P', ';\xa0', 'Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we study the problem of learning from concept drifting data streams with noise, where samples in a data stream may be mislabeled or contain erroneous values. Our essential goal is to build a robust prediction model from noisy stream data to accurately predict future samples. For noisy data sources, most existing works rely on data preprocessing techniques to cleanse noisy samples before the training of decision models. In data stream environments, these data preprocessing techniques are, unfortunately, hard to apply, mainly because the concept drifting in a data stream may make it very difficult to differentiate noise from samples of changing concepts. Accordingly, we propose an aggregate ensemble (AE) learning framework. The aim of AE is to build a robust ensemble model that can tolerate data errors. Theoretical and empirical studies on both synthetic and real-world data streams demonstrate that the proposed AE learning framework is capable of building accurate classification models from noisy data streams. (C) 2010 Elsevier B.V. All rights reserved.']}
{'论文标题':['Generating Syntactic Tree Templates for Feature-Based Opinion Mining'],'论文作者':['Wu, L', ' (', 'Wu, Liang', ') ', ';\xa0', 'Zhou, YC', ' (', 'Zhou, Yuanchun', ') ', ';\xa0', 'Tan, F', ' (', 'Tan, Fei', ') ', ';\xa0', 'Yang, FL', ' (', 'Yang, Fenglei', ') ', ';\xa0', 'Li, JH', ' (', 'Li, Jianhui', ') '],'论文摘要':['Feature-based sentiment analysis aims to recognize appraisal expressions and identify the targets and the corresponding semantic polarity. State-of-the-art syntactic-based approaches mainly focused on designing effective features for machine learning algorithms and/or pre-define some rules to extract opinion words, target words and other opinion-related information. In this paper, we present a novel approach for identifying the relation between target words and opinion words. The proposed algorithm generates tree templates by mining syntactic structures of the annotated corpus. The proposed dependency tree templates cover not only the nodes directly linked with sentiment words and target words, but also subtrees of the nodes on syntactic path, which proved to be effective features for link relation extraction between opinions and targets. Experiment results show that the proposed approach achieves the best performance on the benchmark data set and can work well when syntactic tree templates are applied to different domains.']}
{'论文标题':['Discovering relational patterns across multiple databases'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':["Relational patterns across multiple databases can reveal special pattern relationships hidden inside data collections. Existing research in data mining has made significant efforts in discovering different types of patterns from single or multiple databases, but how to find patterns that have a higher support in database A than in database B with a given support threshold alpha is still an open problem. We propose in this paper DRAMA, a systematic framework for Discovering Relational patterns Across Multiple dAtabases. More specifically, given a series of data collections, we try to discover patterns from different databases with patterns' relationships satisfying the user specified constraints. Our method seeks to build a Hybrid Frequent Pattern tree (HFP-tree) from multiple databases, and mine patterns from the HFP-tree by integrating users' constraints into the pattern mining process."]}
{'论文标题':['Document-specific keyphrase candidate search and ranking'],'论文作者':['Wang, QR', ';\xa0', 'Sheng, VS', ';\xa0', 'Wu, XD'],'论文摘要':['This paper proposes an approach KeyRank to extract proper keyphrases from a document in English. It first searches all keyphrase candidates from the document, and then ranks them for selecting top-N ones as final keyphrases. Existing studies show that extracting a complete keyphrase candidate set that includes semantic relations in context, and evaluating the effectiveness of each candidate are crucial to extract high quality keyphrases from documents. Based on that words do not repeatedly appear in an effective keyphrase in English, a novel keyphrase candidate search algorithm using sequential pattern mining with gap constraints (called KCSP) is proposed to extract keyphrase candidates for KeyRank. And then an effectiveness evaluation measure pattern frequency with entropy (called PF-H) is proposed for KeyRank to rank these keyphrase candidates. Our experimental results show that KeyRank has better performance. Its first component KCSP is much more efficient than a closely related approach SPMW, and its second component PF-H is an effective evaluation mechanism for ranking keyphrase candidates.(1) (C) 2017 Elsevier Ltd. All rights reserved.']}
{'论文标题':['Varying density spatial clustering based on a hierarchical tree'],'论文作者':['Hu, XG', ';\xa0', 'Wang, DB', ';\xa0', 'Wu, XD'],'论文摘要':['The high efficiency and quality of clustering for dealing with high-dimensional data are strongly needed with the leap of data scale. Density-based clustering is an effective clustering approach, and its representative algorithm DBSCAN has advantages as clustering with arbitrary shapes and handling noise. However, it also has disadvantages in its high time expense, parameter tuning and inability to varying densities. In this paper, a new clustering algorithm called VDSCHT (Varying Density Spatial Clustering Based on a Hierarchical Tree) is presented that constructs a hierarchical tree to describe subcluster and tune local parameter dynamically. Density-based clustering is adopted to cluster by detecting adjacent spaces of the tree. Both theoretical analysis and experimental results indicate that VDSCHT riot only has the advantages of density-based clustering, but can also tune the local parameter dynamically to deal with varying densities. In addition, only one scan of database makes it suitable for mining large-scaled ones.']}
{'论文标题':['Relation discovery method based on knowledge map comprises constructing multiple relational graph models, constructing multiple relationship graph model, updating regular increment of graph model and updating graph model'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Extracting Web News Using Tag Path Patterns'],'论文作者':['Wu, GQ', ';\xa0', 'Wu, XD'],'论文摘要':['How to accurately extract the content of Web news is a popular and significant issue in Web Intelligence. Many Web news sites have similar structures and layout styles, and there are potential correlations between Web content layouts and tag path patterns. Compared with other extraction features, such as HTML tags, literal words and visual features, a tag path pattern not only addresses content segments well, but also has an advantage in the generalization. However, can we accurately extract Web news using only tag path patterns? Motivated by this problem, we propose a PPWIE extraction model. We design an extraction algorithm WEtr using self-defined tag path patterns, and then define a special tag path pattern called the distinguishing tag path pattern. In addition, to tackle the NPC-hard problem in path pattern mining, we propose a polynomial-time (ln|n|+1)-approximation algorithm MPM, in which n indicates the scale of positive samples. Our experiments show that our integration method WEtr+MPM in PPWIE can achieve better performance with more than 98% of precision, recall and the F-score on real world datasets.']}
{'论文标题':['Maximizing influence spread in modular social networks by optimal resource allocation'],'论文作者':['Cao, TY', ';\xa0', 'Wu, XD', ';\xa0', 'Hu, XH'],'论文摘要':['Influence maximization in a social network is to target a given number of nodes in the network such that the expected number of activated nodes from these nodes is maximized. A social network usually exhibits some degree of modularity. Previous research efforts that made use of this topological property are restricted to random networks with two communities. In this paper, we firstly transform the influence maximization problem in a modular social network to an optimal resource allocation problem. We assume that the communities of the social network are disconnected. We then propose a recursive relation for finding such an optimal allocation. We prove that finding an optimal allocation in a modular social network is NP-hard and propose a new dynamic programming algorithm to solve the problem. We name our new algorithm OASNET (Optimal Allocation in a Social NETwork). We compare OASNET with the high degree heuristics, the single degree discount heuristics, and the degree discount heuristics on three real world datasets. Experimental results show that OASNET outperforms comparison heuristics significantly on the independent cascade model when the diffusion probability is greater than a certain threshold. (C) 2011 Elsevier Ltd. All rights reserved.']}
{'论文标题':['Dynamic Bayesian networks inference algorithm based on adaptive particle filtering'],'论文作者':['Chen Dong-liang', ';\xa0', 'Wang Hao', ';\xa0', 'Yu Kui'],'论文摘要':["A dynamic Bayesian networks inference algorithm based on adaptive particle filtering was presented in this paper. The algorithm can change the number of particles over time with dynamic Bayesian networks' state uncertainty of dynamic Bayesian networks' evolvement. The number of particles is determined based on statistical bounds on the sample-based approximation quality. The algorithm chooses a small number of particles when the state uncertainty is low, and while it chooses a large number of particles when the state uncertainty is high. The experimental results show that the algorithm can make good choice between inference precision and inference time, and it can outperforms particle filtering. Compared with RBPF algorithm, the algorithm has some advantages on stability and applicability.", '提出一种基于自适应粒子滤波的动态贝叶斯网推理算法,该算法能随着动态贝叶斯网状态演化的不确定性动态改变抽样粒子数目,其根据是通过给定抽样误差界限来确定粒子数.当状态空间不确定性较低时,算法使用较少的粒子数;当状态空间不确定性很大时,将使用较多的粒子数.模拟实验表明该算法很好地兼顾了推理精度和推理时间,性能优于粒子滤波算法;与RBPF算法相比,该算法在稳定性和适用性方面也具有一定优势.']}
{'论文标题':['Core Network based Multi-Label Classification in Large-Scale Social Network Environments'],'论文作者':['Zhang, Z', ';\xa0', 'Wang, H', ';\xa0', 'Liu, GF'],'论文摘要':["Multi-label classification in social network environments is becoming a key area of data mining research in recent years. Given some nodes' labels (i.e., the sources), the task is to infer some other nodes' labels (i.e., the targets) in the same network. Relational classification methods, which leverage the correlation of labels between linked instances, have been shown to outperform traditional classifiers. However, typical relational classification methods make predictions about targets by executing collective inference over the full set of unlabeled nodes, and then to get the labels of targets. In large-scale social network environments, when we want to predict only a specific node's labels, collective inference procedure can seriously limit the efficiency of relational classifiers and make it inapplicable to large-scale social networks.", 'In this paper, we first propose a new concept Core Network which is composed of the shortest paths that link sources and targets. These paths have the most significant influence on classification. Then we propose a novel Heuristic Core Network discovery (i.e., HCN) algorithm to discover the core network. Finally, we propose two classification algorithms HCN-wvRN and HCN-SCRN. Both algorithms are capable of handling large-scale social networks in an efficient way. The difference between two algorithms is HCN-wvRN consumes much less time than existing methods, while HCN-SCRN can achieve higher classification accuracy than HCN-wvRN. We test on several real-world datasets, the experimental results demonstrate that our proposed methods make great improvements in algorithm efficiency while maintaining the classification accuracy.']}
{'论文标题':['Resampling-based noise correction for crowdsourcing'],'论文作者':['Xu, WQ', ';\xa0', 'Jiang, LX', ';\xa0', 'Li, CQ'],'论文摘要':['Crowdsourcing services provide an economic and efficient means of acquiring multiple noisy labels for each training instance in supervised learning. Ground truth inference methods, also known as consensus methods, are then used to obtain the integrated labels of training instances. Although consensus methods are effective, there still exists a level of noise in the set of integrated labels. Therefore, it is necessary to handle noise in the integrated labels to improve label and model quality. In this paper, we propose a resampling-based noise correction method (simply RNC). Different from previous label noise correction methods for crowdsourcing, RNC first employs a filter to obtain a clean set and a noisy set and then repeatedly resamples the clean and noisy sets several times according to a certain proportion. Finally, multiple classifiers built on the resampled data sets are used to re-label the training data. Experimental results based on 18 simulated data sets and five real-world data sets demonstrate that RNC rarely degrades the label and model quality compared to other three state-of-the-art noise correction methods and, in many cases, improves quality dramatically.']}
{'论文标题':['Convolutional neural network multi-task learning based human face attribute analysis method involves obtaining trained multi-task convolutional neural network model, and detecting face key point on input image'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Cost-guided class noise handling for effective cost-sensitive learning'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['Recent research in machine learning, data mining and related areas has produced a wide variety of algorithms for cost-sensitive (CS) classification, where instead of maximizing the classification accuracy, minimizing the misclassfication cost becomes the objective. However, these methods assume that training sets do not contain significant noise, which is rarely the case in real-world environments. In this paper, we systematically study the impacts of class noise on CS learning, and propose a cost-guided class noise handling algorithm to identify noise for effective CS learning. We call it Cost-guided Iterative Classification Filter (CICF), because it seamlessly integrates costs and an existing Classification Filter [1] for noise identification. Instead of putting equal weights to handle noise in all classes in existing efforts, CICF puts more emphasis on expensive classes, which makes it especially successful in dealing with datasets with a large cost-ratio. Experimental results and comparative studies from real-world datasets indicate that the existence of noise may seriously corrupt the performance of CS classifiers, and by adopting the proposed CICF algorithm, we can significantly reduce the misclassfication cost of a CS classifier in noisy environments.']}
{'论文标题':['Mining Concept-Drifting Data Streams with Multiple Semi-Random Decision Trees'],'论文作者':['Li, PP', ';\xa0', 'Hu, XG', ';\xa0', 'Wu, XD'],'论文摘要':['Classification with concept-drifting data streams has found wide applications. However, many classification algorithms on streaming data have been designed for fixed features of concept drift and cannot deal with the noise impact on concept drift detection. An incremental algorithm with Multiple Semi-Random decision Trees (MSRT) for concept-drifting data streams is presented in this paper, which takes two sliding windows for training and testing, uses the inequality of Hoeffding Bounds to determine the thresholds for distinguishing the true drift from noise, and chooses the classification function to estimate the error rate for periodic concept-drift detection. Our extensive empirical study shows that MSRT has an improved performance in time, accuracy and robustness in comparison with CVFDT, a state-of-the-art decision-tree algorithm for classifying concept-drifting data streams.']}
{'论文标题':['Mining Distribution Change in Stock Order Streams'],'论文作者':['Liu, XY', ';\xa0', 'Wu, XD', ';\xa0', 'Ramamohanarao, K'],'论文摘要':['Detecting changes in stock prices is a well known problem in finance with important implications for monitoring and business intelligence. Forewarning of changes in stock price, can be made by the early detection of changes in the distributions of stock order numbers. In this paper, we address the change detection problem for streams of stock order numbers and propose a novel incremental detection algorithm. Our algorithm gains high accuracy and low delay by employing a natural Poisson distribution assumption about the nature of stock order streams. We establish that our algorithm is highly scalable and has linear complexity. We also experimentally demonstrate its effectiveness for detecting change points, via experiments using both synthetic and real-world datasets.']}
{'论文标题':['Robust Application-Level QoS Management in Service-Oriented Systems'],'论文作者':['Liang, QH', ';\xa0', 'Lau, HC', ';\xa0', 'Wu, XD'],'论文摘要':['Quality of Service (QoS) in Web services has been an active research topic in service-oriented applications. This paper presents a framework for application-level QoS management in services-oriented systems using AI techniques. More precisely, we introduce a management layer, called the business end-user QoS management layer, to be in charge of QoS management at the application level of the applications based on composite Web services. This management layer relies on the measurements against various QoS dimensions of constituent Web services of such applications. A robust algorithm is applied in order to select the most suitable Web services that satisfy the user QoS requirements in a robust way. The effectiveness of the proposed algorithm for this layer of QoS management is demonstrated by experimental results.']}
{'论文标题':['Efficient pattern matching with periodical wildcards in uncertain sequences'],'论文作者':['Liu, HT', ';\xa0', 'Wang, LL', ';\xa0', 'Wu, XD'],'论文摘要':['Data uncertainty is inherent in many real-world applications such as sensor data monitoring and mobile tracking. Mining sequential patterns from uncertain/inaccurate data, such as sensor readings and GPS trajectories, is important to discover hidden knowledge in such applications. This paper addresses the problem of pattern matching with periodical wildcards for uncertain sequences. We present a dynamic programming approach, called CoDP, to compute the exact probability that a pattern q is a subsequence of an uncertain sequence s, and this approach can be further applied to substring matching for uncertain sequences. The efficiency and effectiveness of our algorithm have been verified through extensive experiments on both real and synthetic data.']}
{'论文标题':['Supervised tensor model learning and optimizing method, involves performing induce tensor decomposing process for obtaining modification data, and performing optimal projection tensor decomposing process for obtaining forecast tensor data'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Active Learning of Model Parameters for Influence Maximization'],'论文作者':['Cao, TY', ';\xa0', 'Wu, XD', ';\xa0', 'Wang, S'],'论文摘要':['Previous research efforts on the influence maximization problem assume that the network model parameters are known beforehand. However, this is rarely true in real world networks. This paper deals with the situation when the network information diffusion parameters are unknown. To this end, we firstly examine the parameter sensitivity of a popular diffusion model in influence maximization, i.e., the linear threshold model, to motivate the necessity of learning the unknown model parameters. Experiments show that the influence maximization problem is sensitive to the model parameters under the linear threshold model. In the sequel, we formally define the problem of finding the model parameters for influence maximization as an active learning problem under the linear threshold model. We then propose a weighted sampling algorithm to solve this active learning problem. Extensive experimental evaluations on five popular network datasets demonstrate that the proposed weighted sampling algorithm outperforms pure random sampling in terms of both model accuracy and the proposed objective function.']}
{'论文标题':['Modeling and Identification of Electrically Stimulated Muscles for Wrist Movement'],'论文作者':['Zhang, Z', ';\xa0', 'Liu, YH', ';\xa0', 'Chu, B'],'论文摘要':['The modeling of electrically stimulated muscles is of great importance for the tremor suppression via functional electrical stimulation (FES) approach. In this paper, with fully consideration of the characteristics of wrist muscles, a four input two output wrist muscle model with Hammerstein structure is proposed, by which the four-channel functional electrical stimulation signals can simultaneously stimulate the wrist muscles of flexor carpi radialis (FCR), extensor carpi radialis (ECR), flexor carpi ulnaris (FCU) and extensor carpi ulnaris (ECU) to realize 2 degrees of freedom (DOF) wrist movements. Then, we use the recursive least squares identification algorithm to identify the parameters of the pre-existing four input and one output system. Simulation results show that the least recursive squares identification algorithm of two-step method is advantageous in convergence and identification accuracy.']}
{'论文标题':['A Label Correlation Based Weighting Feature Selection Approach for Multi-label Data'],'论文作者':['Liu, L', ';\xa0', 'Zhang, J', ';\xa0', 'Hu, XG'],'论文摘要':['Exploiting label correlation is important for multi-label learning, where each instance is associated with a set of labels. However, most of existing multi-label feature selection methods ignore the label correlation. Therefore, we propose a Label Correlation Based Weighting Feature Selection Approach for Multi-Label Data, called MLLCWFS. It is a framework developed from traditional filtering feature selection methods for single-label data. To exploit the label correlation, we compute the importance of each label in mutual information, and adopt three weighting strategies to evaluate the correlation between features and labels. Extensive experiments conducted on four benchmark data sets using two base classifiers demonstrate that our approach is superior to the state-of-the-art feature selection algorithms for multi-label data.']}
{'论文标题':['Discovering markov blanket from multiple interventional datasets [arXiv]'],'论文作者':['Kui Yu', ';\xa0', 'Lin Liu', ';\xa0', 'Jiuyong Li'],'论文摘要':['In this paper, we study the problem of discovering the Markov blanket (MB) of a target variable from multiple interventional datasets. Datasets attained from interventional experiments contain richer causal information than passively observed data (observational data) for MB discovery. However, almost all existing MB discovery methods are designed for finding MBs from a single observational dataset. To identify MBs from multiple interventional datasets, we face two challenges: (1) unknown intervention variables; (2) nonidentical data distributions. To tackle the challenges, we theoretically analyze (a) under what conditions we can find the correct MB of a target variable, and (b) under what conditions we can identify the causes of the target variable via discovering its MB. Based on the theoretical analysis, we propose a new algorithm for discovering MBs from multiple interventional datasets, and present the conditions/assumptions which assure the correctness of the algorithm. To our knowledge, this work is the first to present the theoretical analyses about the conditions for MB discovery in multiple interventional datasets and the algorithm to find the MBs in relation to the conditions. Using benchmark Bayesian networks and real-world datasets, the experiments have validated the effectiveness and efficiency of the proposed algorithm in the paper.']}
{'论文标题':['A Parallel Clustering Algorithm with MPI - MKmeans'],'论文作者':['Zhang, J', ';\xa0', 'Wu, GQ', ';\xa0', 'Hao, SL'],'论文摘要':['Clustering is one of the most popular methods for exploratory data analysis, which is prevalent in many disciplines such as image segmentation, bioinformatics, pattern recognition and statistics etc. The most famous clustering algorithm is K-means because of its easy implementation, simplicity, efficiency and empirical success. However, the real-world applications produce huge volumes of data, thus, how to efficiently handle of these data in an important mining task has been a challenging and significant issue. In addition, MPI (Message Passing Interface) as a programming model of message passing presents high performances, scalability and portability. Motivated by this, a parallel K-means clustering algorithm with MPI, called MKmeans, is proposed in this paper. The algorithm enables applying the clustering algorithm effectively in the parallel environment. Experimental study demonstrates that MKmeans is relatively stable and portable, and it performs with low overhead of time on large volumes of data sets.']}
{'论文标题':['Generic object recognition with regional statistical models and layer joint boosting'],'论文作者':['Gao, J', ';\xa0', 'Xie, Z', ';\xa0', 'Wu, XD'],'论文摘要':['This paper presents novel regional statistical models for extracting object features, and an improved discriminative learning method, called as layer joint boosting, for generic multi-class object detection and categorization in cluttered scenes. Regional statistical properties on intensities are used to find sharing degrees among features in order to recognize generic objects efficiently. Based on boosting for multi-classification, the layer characteristic and two typical weights in sharing-code maps are taken into account to keep the maximum Hamming distance in categories, and heuristic search strategies are provided in the recognition process. Experimental results reveal that, compared with interest point detectors in representation and multi-boost in learning, joint layer boosting with statistical feature extraction can enhance the recognition rate consistently, with a similar detection rate. (c) 2007 Elsevier B.V. All rights reserved.']}
{'论文标题':['10 Challenging problems in data mining research'],'论文作者':['Yang, Q', 'Wu, XD'],'论文摘要':['In October 2005, we took an initiative to identify 10 challenging problems in data mining research, by consulting some of the most active researchers in data mining and machine learning for their opinions on what are considered important and worthy topics for future research in data mining. We hope their insights will inspire new research efforts, and give young researchers (including PhD students) a high-level guideline as to where the hot problems are located in data mining.'],}
{'论文标题':['Robust Face Recognition Model with Adaptive Correction Term via Generalized Alternating Direction Method of Multipliers'],'论文作者':['Wu, C', ';\xa0', 'Xiao, YH', ';\xa0', 'Liu, WJ'],'论文摘要':['During the past few years, face recognition technique has received significant attention in the fields of computer vision, neuroscience, psychology and others. The robust face recognition casts the problem as an l(1)-minimization problem to find a sparse representation of the test image in terms of the training set. The main purpose of this paper is to firstly construct an l(1)-l(1)-minimization model and secondly be solved via a generalized alternating direction method of multipliers. Most importantly, the model proposed therein contains an adaptive correction term to get sparse representation with higher accuracy. Extensive experiments on the simulated data verify that the proposed method is effective.']}
{'论文标题':['Visual Processing by a Unified Schatten-p Norm and lq Norm Regularized Principal Component Pursuit [arXiv]'],'论文作者':['Jing Wang', ';\xa0', 'Meng Wang', ';\xa0', 'Shuicheng Yan'],'论文摘要':['In this paper, we propose a non-convex formulation to recover the authentic structure from the corrupted real data. Typically, the specific structure is assumed to be low rank, which holds for a wide range of data, such as images and videos. Meanwhile, the corruption is assumed to be sparse. In the literature, such a problem is known as Robust Principal Component Analysis (RPCA), which usually recovers the low rank structure by approximating the rank function with a nuclear norm and penalizing the error by an l', '-norm. Although RPCA is a convex formulation and can be solved effectively, the introduced norms are not tight approximations, which may cause the solution to deviate from the authentic one. Therefore, we consider here a non-convex relaxation, consisting of a Schatten-p norm and an l', '-norm that promote low rank and sparsity respectively. We derive a proximal iteratively reweighted algorithm (PIRA) to solve the problem. Our algorithm is based on an alternating direction method of multipliers, where in each iteration we linearize the underlying objective function that allows us to have a closed form solution. We demonstrate that solutions produced by the linearized approximation always converge and have a tighter approximation than the convex counterpart. Experimental results on benchmarks show encouraging results of our approach. [doi:10.1016/j.patcog.2015.01.024].']}
{'论文标题':['Co-training based on Semi-supervised Ensemble Classification Approach for Multi-label Data Stream'],'论文作者':['Chu, Z', ';\xa0', 'Li, PP', ';\xa0', 'Hu, XG'],'论文摘要':['A large amount of data streams in the form of texts and images has been emerging in many real-world applications. These data streams often present the characteristics such as multi-labels, label missing and new class emerging, which makes the existing data stream classification algorithm face the challenges in precision space and time performance. This is because, on the one hand, it is known that data stream classification algorithms are mostly trained on all labeled single-class data, while there are a large amount of unlabeled data and few labeled data due to it is difficult to obtain labels in the real world. On the other hand, many of existing multi-label data stream classification algorithms mostly focused on the classification with all labeled data and without emerging new classes, and there are few semi-supervised methods. Therefore, this paper proposes a semi-supervised ensemble classification algorithm for multi-label data streams based on co-training. Firstly, the algorithm uses the sliding window mechanism to partition the data stream into data chunks. On the former w data chucks, the multi-label semi-supervised classification algorithm COINS based on co-training is used to training a base classifier on each chunk, and then an ensemble model with w COINS classifiers is generated ensemble model to adapt to the environment of data stream with a large number of unlabeled data. Meanwhile, a new class emerging detection mechanism is introduced, and the w+1 data chunk is predicted by the ensemble model to detect whether there is a new class emerging. When a new label is detected, the classifier is retrained on the current data chunk, and the ensemble model is updated. Finally, experimental results on five real data sets show that: as compared with the classical algorithms, the proposed approach can improve the classification accuracy of multi-label data streams with a large number of missing labels and new labels emerging.']}
{'论文标题':['Error detection and impact-sensitive instance ranking in noisy datasets'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD', ';\xa0', 'Yang, Y'],'论文摘要':['Given a noisy dataset, how to locate erroneous instances and attributes and rank suspicious instances based on their impacts on the system performance is an interesting and important research issue. We provide in this paper an Error Detection and Impact-sensitive instance Ranking (EDIR) mechanism to address this problem. Given a noisy dataset D, we first train a benchmark classifier T from D. The instances, that cannot be effectively classified by T are treated as suspicious and forwarded to a subset S. For each attribute A(i), we switch A(i) and the class label C to train a classifier AP(i) for A(i). Given an instance I-k in S, we use AP(i) and the benchmark classifier T to locate the erroneous value of each attribute A(i) To quantitatively rank instances in S, we define an impact measure based on the Information-gain Ratio (IR). We calculate IRi between attribute A(i) and C, and use IRi as the impact-sensitive weight of A(i). The sum of impact-sensitive weights from all located erroneous attributes of I-k indicates its total impact value. The experimental results demonstrate the effectiveness of our strategies.']}
{'论文标题':['Sequential association mining for video summarization'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we propose an association-based video summarization scheme that mines sequential associations from video data for summary creation. Given detected shots of video V, we first cluster them into visually distinct groups, and then construct a sequential sequence by integrating the temporal order and cluster type of each shot. An association mining scheme is designed to mine sequentially associated clusters from the sequence, and these clusters are selected as summary candidates. With a user specified summary length, our system generates the corresponding summary by selecting representative frames from candidate clusters and assembling them by their original temporal order. The experimental evaluation demonstrates the effectiveness of our summarization method.']}
{'论文标题':['Domain knowledge based Bayesian networks structure learning algorithm'],'论文作者':['Mo Fu-qiang', ';\xa0', 'Wang Hao', ';\xa0', 'Yu Kui'],'论文摘要':['To overcome shortcomings present in the SEM in the presence of missing values data sets such as lower precision and slow convergent speed, KB-SEM is proposed by introducing domain knowledge. A tabu list derived from collecting domain expert knowledge synthesized by using D-S evidence theory is embedded in the SEM to constrain and guide the searching path, and lo decrease the searching space. Experimental result shows that KB-SEM can improve the precision and the speed effectively, and to some extent avoid subjective bias and disturbance of noise in the data sets.', '针对SEM算法在缺省数据学习中存在精度偏低和收敛速度缓慢的问题,通过将领域知识引入到SEM算法中,提出了KB-SEM算法,该算法首先用D-S证据理论综合领域知识,然后将采集的知识以禁忌表的方式嵌入SEM中来限制和引导算法的搜索路径,缩小算法的搜索空间.实验表明,KB-SEM算法能有效地提高算法的学习精度和时间性能,且能在一定程度上避免主观偏见和数据噪音的干扰.']}
{'论文标题':['A Multi-Grouped LS-SVM Method for Short-term Urban Traffic Flow Prediction'],'论文作者':['Liu, F', ';\xa0', 'Wei, ZC', ';\xa0', 'Shi, L'],'论文摘要':['Predicting short-term urban traffic flow is a non-trivial task, for an intelligent transportation system could greatly facilitate urban transportation infrastructure construction and enhances the efficiency of traffic control. Unfortunately, urban traffic flow is influenced by numerous factors, which increases the complexity of prediction. In this paper, Multi-Grouped Least Squares Support Vector Machine (MLS-SVM) is proposed for short-term urban traffic flow prediction. In MLS-SVM, spatiotemporal factors (e.g., time, geography, and environment) are divided into different groups. Correlations between each grouped factor are then recognized. Finally, the predicted effect is optimized by combining sub-models for each group. Real-world datasets are used in the experiments of traffic flow prediction. Comparing with the rival methods (i.e., LS-SVM, Wavelet Neural Network, Multi-Factor Pattern Recognition), the simulation results demonstrated the validity and stability of MLS-SVM.']}
{'论文标题':['Imbalanced Multiple Noisy Labeling'],'论文作者':['Zhang, J', ';\xa0', 'Wu, XD', ';\xa0', 'Sheng, VS'],'论文摘要':['It can be easy to collect multiple noisy labels for the same object via Internet-based crowdsourcing systems. Labelers may have bias when labeling, due to lacking expertise, dedication, and personal preference. These cause Imbalanced Multiple Noisy Labeling. In most cases, we have no information about the labeling qualities of labelers and the underlying class distributions. It is important to design agnostic solutions to utilize these noisy labels for supervised learning. We first investigate how imbalanced multiple noisy labeling affects the class distributions of training sets and the performance of classification. Then, an agnostic algorithm Positive LAbel frequency Threshold (PLAT) is proposed to deal with the imbalanced labeling issue. Simulations on eight UCI data sets with different underlying class distributions show that PLAT not only effectively deals with the imbalanced multiple noisy labeling problems that off-the-shelf agnostic methods cannot cope with, but also performs nearly the same as majority voting under the circumstances without imbalance. We also apply PLAT to eight real-world data sets with imbalanced labels collected from Amazon Mechanical Turk, and the experimental results show that PLAT is efficient and better than other ground truth inference algorithms.']}
{'论文标题':['Mining Markov Blankets Without Causal Sufficiency'],'论文作者':['Yu, K', ' (', 'Yu, Kui', ') ', ';\xa0', 'Liu, L', ' (', 'Liu, Lin', ') ', ';\xa0', 'Li, JY', ' (', 'Li, Jiuyong', ') ', ';\xa0', 'Chen, HH', ' (', 'Chen, Huanhuan', ') '],'论文摘要':['Markov blankets (MBs) in Bayesian networks (BNs) play an important role in both local causal discovery and largescale BN structure learning. Almost all existing MB discovery algorithms are designed under the assumption of causal sufficiency, which states that there are no latent common causes for two or more of the observed variables in data. However, latent common causes are ubiquitous in many applications, and hence, this assumption is often violated in practice. Thus, developing algorithms for discovering MBs without assuming causal sufficiency is of practical significance, and it is crucial for causal structure learning in real-world data. In this paper, we focus on addressing this problem. Specifically, we adopt a maximal ancestral graph (MAG) model to represent latent common causes and the concept of MBs without assuming causal sufficiency. Then, we propose an effective and efficient algorithm to discover the MB of a target variable in an MAG. Using benchmark and real-world data sets, the experiments validate the algorithm proposed in this paper.']}
{'论文标题':['Short-term Load Forecasting by Using Improved GEP and Abnormal Load Recognition'],'论文作者':['Deng, S', ';\xa0', 'Chen, FL', ';\xa0', 'Wu, XD'],'论文摘要':['Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-term load forecasting algorithms in terms of the convergence speed, MAE. MAPE, RSME, and R-2.']}
{'论文标题':['Top 10 algorithms in data mining'],'论文作者':['Wu, XD', ';\xa0', 'Kumar, V', ';\xa0', 'Steinberg, D'],'论文摘要':['This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification, clustering, statistical learning, association analysis, and link mining, which are all among the most important topics in data mining research and development.']}
{'论文标题':['Mediapedia: Mining Web Knowledge to Construct Multimedia Encyclopedia'],'论文作者':['Hong, RC', ';\xa0', 'Tang, JH', ';\xa0', 'Chua, TS'],'论文摘要':['In recent years, we have witnessed the blooming of Web 2.0 content such as Wikipedia, Flickr and YouTube, etc. How might we benefit from such rich media resources available on the internet? This paper presents a novel concept called Mediapedia, a dynamic multimedia encyclopedia that takes advantage of, and in fact is built from the text and image resources on the Web. The Mediapedia distinguishes itself from the traditional encyclopedia in four main ways. (1) It tries to present users with multimedia contents (e.g., text, image, video) which we believed are more intuitive and informative to users. (2) It is fully automated because it downloads the media contents as well as the corresponding textual descriptions from the Web and assembles them for presentation. (3) It is dynamic as it will use the latest multimedia content to compose the answer. This is not true for the traditional encyclopedia. (4) The design of Mediapedia is flexible and extensible such that we can easily incorporate new kinds of mediums such as video and languages into the framework. The effectiveness of Mediapedia is demonstrated and two potential applications are described in this paper.']}
{'论文标题':['A Framework for Subgraph Detection in Interdependent Networks via Graph Block-Structured Optimization'],'论文作者':['Jie, F', ';\xa0', 'Wang, CP', ';\xa0', 'Wu, XD'],'论文摘要':["As the backbone of many real-world complex systems, networks interact with others in nontrivial ways from time to time. It is a challenging problem to detect subgraphs that have dependencies on each other across multiple networks. Instead of devising a method for a specific scenario, we propose a generic framework to discover subgraphs in multiple interdependent networks, which generalizes the classical subgraph detection problem in a single network and can be applied to more practical applications. Specifically, we propose the Graph Block-structured Gradient Hard Thresholding Pursuit (GB-GHTP) framework to optimize interdependent networks with block-structured constraints, which enjoys 1) a theoretical guarantee and 2) a nearly linear time complexity on the network size. It is demonstrated how our framework can be applied to three practical applications: 1) evolving anomalous subgraph detection in dynamic networks, 2) anomalous subgraph detection in networks of networks, and 3) connected dense subgraph detection in dual networks. We evaluate our framework on large-scale datasets with comprehensive experiments, which validate our framework's effectiveness and efficiency."]}
{'论文标题':['Multi-Label Learning with Missing Features'],'论文作者':['Li, JL', ';\xa0', 'Li, PP', ';\xa0', 'Hu, XG'],'论文摘要':['Multi-label learning deals with the problem that each example is associated with multiple class labels simultaneously. Existing multi-label learning approaches all assume the feature space is completed and construct classification models using examples with sufficient feature information. However, in real-world applications, it is difficult to get a fully completed feature matrix, that is, only partial feature information of each example can be obtained. In this paper, we formalize this problem as multi-label learning with missing features. To tackle this problem, we learn a feature correlation matrix and apply it to obtain a new supplementary feature matrix, which has richer feature information than the original missing feature matrix. After that, to improve the performance of multi-label classification, we constrain feature correlation on coefficient matrix by assuming that if two features are strongly correlated, the similarity between their corresponding parameter vector will be large. Besides, we also constrain label correlation on output of labels to capture more sufficient relationships between different labels. Extensive experiments show a competitive performance of our method against other state-of the-art multi-label learning approaches.']}
{'论文标题':['Multi-label text classification via joint learning from label embedding and label correlation'],'论文作者':['Liu, HT', ';\xa0', 'Chen, G', ';\xa0', 'Wu, XD'],'论文摘要':['For the multi-label text classification problems with many classes, many existing multi-label classification algorithms become infeasible or suffer an unaffordable cost. Some researches hence perform the Label Space Dimension Reduction(LSDR) to solve this problem, but a number of methods ignore the sequence information of texts and the label correlation in the original label space, and treat the label as a meaningless multi-hot vector. In this paper, we put forward a multi-label text classification algorithm LELC(joint learning from Label Embedding and Label Correlation) based on the multi-layer attention and label correlation to solve the issue of multi-label text classification with a large number of class labels. Specifically, we firstly extract features through Bidirectional Gated Recurrent Unit Network(BiGRU), multi-layer attention and linear layers. Bi-GRU will capture the content information and sequence information of the text at the same time, and the attention mechanism can help us select the valid features related to labels. Then, we use matrix factorization to perform LSDR, and consider label correlation of the original label space in this process, which allows us to implicitly encode the latent space and simplify the model learning. Finally, Deep Canonical Correlation Analysis(CCA) technology is exploited to couple features and the latent space in an end-to-end pattern, so that these two can influence each other to learn the mapping of feature space to latent space. Experiments on 11 real-world datasets show the comparability between our proposed model and the state-of-the-art methods. (c) 2021 Elsevier B.V. All rights reserved.']}
{'论文标题':['The Apriori property of sequence pattern mining with wildcard gaps'],'论文作者':['Min, Fan', ';\xa0', 'Wu, Youxi', ';\xa0', 'Wu, Xindong'],'论文摘要':['In biological sequence analysis, long and frequently occurring patterns tend to be interesting. Data miners try to obtain frequent patterns with periodical wildcard gaps. However, with the existing definition set, the Apriori property does not hold; consequently, state-of-the-art algorithms are rather complex. This paper proposes an alternative definition of the number of offset sequences by adding a number of dummy characters. With the new definition, the Apriori property holds, hence our Apriori algorithm can mine all frequent patterns with minimal endeavour. This study also serves as the foundation of further research works on sequence pattern mining.']}
{'论文标题':['A Bit-Parallel Algorithm for Sequential Pattern Matching with Wildcards'],'论文作者':['Guo, D', 'Hong, XL', 'Wu, XD'],'论文摘要':['Pattern matching with both gap constraints and the one-off condition is a challenging topic, especially in bioinformatics, information retrieval, and dictionary query. Among the algorithms to solve the problem, the most efficient one is SAIL, which is time consuming, especially when the pattern is long. In addition, existing algorithms based on bit-parallelism cannot handle a pattern that has only one pattern character between successive wildcards and the minimum local length constraints are zero. We propose an algorithm BPBM to handle online sequential pattern matching. In BPBM, an extended bit-parallelism operation is used to accelerate the matching process. An effective transition window mechanism with two nondeterministic finite state automatons (NFAs) is adopted to drop the useless scan window. It identifies gap constraints automatically and just scans once to export occurrences with exact match positions. Theoretical analysis and experimental results show that the BPBM algorithm is more competitive than other peers. It has an absolute advantage on search time complexity. It also has better stability that decreases operation costs with the increasing of the size of sequence alphabet or the length of the pattern. We also study off-line pattern matching. With twice pruning, left-most and right-most, we can increase the matching ratio about 2.08% on average.'],}
{'论文标题':['Adversarial training with Wasserstein distance for learning cross-lingual word embeddings'],'论文作者':['Li, YL', ';\xa0', 'Zhang, YH', ';\xa0', 'Hu, XG'],'论文摘要':['Recent studies have managed to learn cross-lingual word embeddings in a completely unsupervised manner through generative adversarial networks (GANs). These GANs-based methods enable the alignment of two monolingual embedding spaces approximately, but the performance on the embeddings of low-frequency words (LFEs) is still unsatisfactory. The existing solution is to set up the low sampling rates for the embeddings of LFEs based on word-frequency information. However, such a solution has two shortcomings. First, this solution relies on the word-frequency information that is not always available in real scenarios. Second, the uneven sampling may cause the models to overlook the distribution information of LFEs, thereby negatively affecting their performance. In this study, we propose a novel unsupervised GANs-based method that effectively improves the quality of LFEs, circumventing the above two issues. Our method is based on the observation that LFEs tend to be densely clustered in the embedding space. In these dense embedding points, obtaining fine-grained alignment through adversarial training is difficult. We use this idea to introduce a noise function that can disperse the dense embedding points to a certain extent. In addition, we train a Wasserstein critic network to encourage the noise-adding embeddings and the original embeddings to have similar semantics. We test our approach on two common evaluation tasks, namely, bilingual lexicon induction and cross-lingual word similarity. Experimental results show that the proposed model has stronger or competitive performance compared with the supervised and unsupervised baselines.']}
{'论文标题':['Clustering web documents using hierarchical representation with multi-granularity'],'论文作者':['Huang, FL', ';\xa0', 'Zhang, SC', ';\xa0', 'Wu, XD'],'论文摘要':['Web document cluster analysis plays an important role in information retrieval by organizing large amounts of documents into a small number of meaningful clusters. Traditional web document clustering is based on the Vector Space Model (VSM), which takes into account only two-level (document and term) knowledge granularity but ignores the bridging paragraph granularity. However, this two-level granularity may lead to unsatisfactory clustering results with "false correlation". In order to deal with the problem, a Hierarchical Representation Model with Multi-granularity (HRMM), which consists of five-layer representation of data and a two-phase clustering process is proposed based on granular computing and article structure theory. To deal with the zero-valued similarity problem resulted from the sparse term-paragraph matrix, an ontology based strategy and a tolerance-rough-set based strategy are introduced into HRMM. By using granular computing, structural knowledge hidden in documents can be more efficiently and effectively captured in HRMM and thus web document clusters with higher quality can be generated. Extensive experiments show that HRMM, HRMM with tolerance-rough-set strategy, and HRMM with ontology all outperform VSM and a representative non VSM-based algorithm, WFP, significantly in terms of the F-Score.']}
{'论文标题':['Mining High-voting Sequence Patterns with Privacy-preserving in Multi-databases'],'论文作者':['Zhang Ying', ';\xa0', 'Zhong Cheng'],'论文摘要':['In the multi-database environment, a multi-database sequential pattern classification algorithm with multiple databases and classes is designed by computing the similarity of item set in databases and according to a special classification principle; and an efficient mining high-voting sequential patterns method is given, according to the requirement of privacy preserving, a hiding sensitive patterns high-voting sequential patterns mining algorithm is presented by applying "classifying-cleaning-composing-mining" method. Compared with the multi-database sequential pattern mining algorithm with no-hiding sensitive patterns, the experimental results show that the proposed algorithm can not only mine the required global high-voting patterns but also hide the sensitive patterns and protect the privacy information in original multiple databases by spending extra very little time to process sensitive patterns matching.', '在多数据源环境中,通过计算数据库项集相似度,按照一定分类原则,设计了数据库数目和类个数可变的多数据源序列模式分类算法;在给出一种有效的多数据源高投票率序列模式挖掘方法的基础上,结合隐私保护要求,采取"分类清洗合成挖掘"方法,提出一种隐藏敏感模式的多源高投票率序列模式挖掘算法。与不隐藏敏感模式的多源高投票率序列模式挖掘算法进行实验测试对比,结果表明:本文提出的算法只需花费额外少量的敏感模式匹配处理时间,可确保算法能够在挖掘得到全局高投票率序列模式的同时,隐藏敏感模式、保护多源数据中的隐私信息.']}
{'论文标题':['Regional category parsing in undirected graphical models'],'论文作者':['Xie, Z', ';\xa0', 'Gao, J', ';\xa0', 'Wu, XD'],'论文摘要':['This article presents an approach for regional categorization in complex natural scenes with undirected graphs. A novel MRF-like model is proposed with spatial constraints in the feature space based on existing directed graphs, and an approximation of pseudo-likelihood is introduced for probability inference and parameter estimation. With this approximation, we can deal with the intractability of potential functions and get spatial relations between patches of different classes for more information in their co-occurrence matrix. The Receiver-Operating-Characteristic curves in our experiments demonstrate a better performance from our proposed method in comparison with directed probabilistic models such as LDA and constellation. (C) 2009 Elsevier B.V. All rights reserved.']}
{'论文标题':['Towards Efficient Local Causal Structure Learning [arXiv]'],'论文作者':['Shuai Yang', ';\xa0', 'Hao Wang', ';\xa0', 'Xindong Wu'],'论文摘要':['Local causal structure learning aims to discover and distinguish direct causes (parents) and direct effects (children) of a variable of interest from data. While emerging successes have been made, existing methods need to search a large space to distinguish direct causes from direct effects of a target variable T. To tackle this issue, we propose a novel Efficient Local Causal Structure learning algorithm, named ELCS. Specifically, we first propose the concept of N-structures, then design an efficient Markov Blanket (MB) discovery subroutine to integrate MB learning with N-structures to learn the MB of T and simultaneously distinguish direct causes from direct effects of T. With the proposed MB subroutine, ELCS starts from the target variable, sequentially finds MBs of variables connected to the target variable and simultaneously constructs local causal structures over MBs until the direct causes and direct effects of the target variable have been distinguished. Using eight Bayesian networks the extensive experiments have validated that ELCS achieves better accuracy and efficiency than the state-of-the-art algorithms. [doi:10.1109/TBDATA.2021.3062937].']}
{'论文标题':['Using knowledge anchors to reduce cognitive overhead'],'论文作者':['Ransom, S', ';\xa0', 'Wu, XD'],'论文摘要':[]}
{'论文标题':['A Local Similarity-Preserving Framework for Nonlinear Dimensionality Reduction with Neural Networks'],'论文作者':['Xiang Wang', ';\xa0', 'Xiaoyong Li', ';\xa0', 'Kui Yu'],'论文摘要':['Real-world data usually have high dimensionality and it is important to mitigate the curse of dimensionality. High-dimensional data are usually in a coherent structure and make the data in relatively small true degrees of freedom. There are global and local dimensionality reduction methods to alleviate the problem. Most of existing methods for local dimensionality reduction obtain an embedding with the eigenvalue or singular value decomposition, where the computational complexities are very high for a large amount of data. Here we propose a novel local nonlinear approach named ', ' for general purpose dimensionality reduction, which generalizes recent advancements in embedding representation learning of words to dimensionality reduction of matrices. It obtains the nonlinear embedding using a neural network with only one hidden layer to reduce the computational complexity. To train the neural network, we build the neighborhood similarity graph of a matrix and define the context of data points by exploiting the random walk properties. Experiments demonstrate that ', ' is more efficient than several state-of-the-art local dimensionality reduction methods in a large number of high-dimensional data. Extensive experiments of data classification and clustering on eight real datasets show that ', ' is better than several classical dimensionality reduction methods in the statistical hypothesis test, and it is competitive with recently developed state-of-the-art UMAP.']}
{'论文标题':['Detecting Differences Between Contrast Groups'],'论文作者':['Zhang, SC'],'论文摘要':['In medical research, doctors must evaluate the effectiveness of a new medicine B against a specified disease. This evaluation is often carried out by comparing B with an old medicine A, which has been used to treat the disease for many years. This comparison should include two important statistical summaries: mean and distribution function differences between A and B. The datasets; of applied/tested A and B are referred to contrast groups, and the mean and distribution differences are referred to group differences. Because the datasets to be contrasted are only two samples obtained by limited applications or tests on A and B, the differences derived from the datasets are inevitably uncertain. This generates a need of measuring the uncertainty of differences. In this paper.. an efficient strategy is designed for identifying confidence intervals for measuring the uncertainty of the differences between two contrast groups. This approach is suitable for most of those applications for which we have no prior knowledge about the underlying distribution of the data. We experimentally evaluate the proposed approach using the UCI, datasets against the bootstrap resampling method and the traditional method, and demonstrate that our method is efficient in measuring the structural differences between contrast groups.']}
{'论文标题':['An Agent-Based Hybrid System for Microarray Data Analysis'],'论文作者':['Zhang, ZL', ';\xa0', 'Yang, PY', ';\xa0', 'Zhang, CQ'],'论文摘要':[]}
{'论文标题':["Editor's Note"],'论文作者':['Golder, B'],'论文摘要':[]}
{'论文标题':['Disorientation and cognitive overhead in hypertext systems'],'论文作者':['Ransom, S.', ';\xa0', 'Xingdong Wu', ';\xa0', 'Schmidt, H.'],'论文摘要':['Disorientation and cognitive overhead are two interrelated problems that may in fact ultimately limit the usefulness of hypertext. Cognitive overhead upon the author arises from a viewpoint that sees the author as the person solely responsible for inserting all links within her work. This includes all links between the anchors in her own work and those in external frames, as well as cross links between the anchors within her own work. The need for an author to be aware, for each anchor within her work, of all possible other anchors makes the insertion of such links an O(N', ") problem where N is the number of possible anchors in the hypertext universe. To reduce this to an O(N) problem, the paper proposes the use of bi directional intelligent anchors. These are anchors that have both a knowledge of the concepts that a potential reader would be seeking were she to arrive at the anchor, together with a knowledge of the concepts that a reader would be seeking were she to jump from that anchor to another within the hypertext. Disorientation or the lost in hyperspace effect on the reader can be addressed in many cases by a fundamental re definition of the relationship between hypertext authors and their readers. To address this issue, this paper proposes a unit of authorship based more on the treatise than on the traditional fragmented model of hypertext that we call a scenic route and which is based loosely on A. Bush's (1945) Trails and R.H. Trigg's (1988; 1991) Guided Tours."]}
{'论文标题':['Linear time friend recommendation method, involves using bottom-up modularity value-added community discovery method to mine community set of target user in weighted heterogeneous network, and calculating membership value of target user attached to different communities'],'论文作者':[],'论文摘要':[]}
{'论文标题':['A Study on Big Knowledge and Its Engineering Issues'],'论文作者':['Lu, RQ', ' (', 'Lu, Ruqian', ') ', ', ', ';\xa0', 'Jin, XL', ' (', 'Jin, Xiaolong', ') ', ', ', ';\xa0', 'Zhang, SM', ' (', 'Zhang, Songmao', ') ', ', ', ';\xa0', 'Qiu, MK', ' (', 'Qiu, Meikang', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', '],'论文摘要':["After entering the big data era, a new term of 'big knowledge' has been coined to deal with challenges in mining a mass of knowledge from big data. While researchers used to explore the basic characteristics of big data, we have not seen any studies on the general and essential properties of big knowledge. To fill this gap, this paper studies the concepts of big knowledge, big-knowledge system, and big-knowledge engineering. Ten massiveness characteristics for big knowledge and big-knowledge systems, including massive concepts, connectedness, clean data resources, cases, confidence, capabilities, cumulativeness, concerns, consistency, and completeness, are defined and explored. Based on these characteristics, a comprehensive investigation is conducted on some large-scale knowledge engineering projects, including the Fifth Comprehensive Traffic Survey in Shanghai, the China's Xia-Shang-Zhou Chronology Project, the Troy and Trojan War Project, and the International Human Genome Project, as well as the online free encyclopedia Wikipedia. We also investigate the recent research efforts on knowledge graphs, where they are analyzed to determine which ones can be considered as big knowledge and big-knowledge systems. Further, a definition of big-knowledge engineering and its life cycle paradigm is presented. All of these projects are accordingly checked to determine whether they belong to big-knowledge engineering projects. Finally, the perspectives of big knowledge research are discussed."]}
{'论文标题':['Adaptive sequential pattern mining method with weak wildcards under one-off condition, involves adding patterns with occurrences greater than or equal to minimum support threshold to frequent pattern with pattern length of specified value'],'论文作者':[],'论文摘要':[]}
{'论文标题':['A Modified Semi-Supervised Learning Algorithm on Laplacian Eigenmaps'],'论文作者':['Zhao, ZQ', ';\xa0', 'Li, JZ', ';\xa0', 'Wu, XD'],'论文摘要':['In this letter, a modified algorithm is proposed to extend 2-class semi-supervised learning on Laplacian eigenmaps to multi-class learning problems. The modified algorithm significantly increases its learning speed, and at the same time attains a satisfactory classification performance that is not lower than the original algorithm.']}
{'论文标题':['Computing the minimum-support for mining frequent patterns'],'论文作者':['Zhang, SC', ' (', 'Zhang, Shichao', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ';\xa0', 'Zhang, CQ', ' (', 'Zhang, Chengqi', ') ', ';\xa0', 'Lu, JL', ' (', 'Lu, Jingli', ') '],'论文摘要':["Frequent pattern mining is based on the assumption that users can specify the minimum-support for mining their databases. It has been recognized that setting the minimum-support is a difficult task to users. This can hinder the widespread applications of these algorithms. In this paper we propose a computational strategy for identifying frequent itemsets, consisting of polynomial approximation and fuzzy estimation. More specifically, our algorithms (polynomial approximation and fuzzy estimation) automatically generate actual minimum-supports (appropriate to a database to be mined) according to users' mining requirements. We experimentally examine the algorithms using different datasets, and demonstrate that our fuzzy estimation algorithm fittingly approximates actual minimum-supports from the commonly-used requirements."]}
{'论文标题':['Method for machine-based video classifying to identify misleading videos, involves classifying subject video using model and based on first, second and third semantic similarities, and outputting classification of subject video to user'],'论文作者':[],'论文摘要':[]}
{'论文标题':['A no self-edge stochastic block model and a heuristic algorithm for balanced anti-community detection in networks'],'论文作者':['Zhu, JJ', ';\xa0', 'Liu, YG', ';\xa0', 'Wu, XD'],'论文摘要':['Many real-world networks own the characteristic of anti-community structure, i.e. disassortative structure, where nodes share no or few connections inside their groups but most of their connections outside. Detecting anti-community structure can explore negative relations among objects. However, the structures output by the existing algorithms are unbalanced, leading to no or few negative relations to be explored in some groups. Stochastic block models are promising methods for exploring disassortative structures in networks, but their results are highly dependent on the observed structure of a network. In this paper, we first improve the classic stochastic block model and propose a No sElf-edge Stochastic blOck Model (NESOM) for anti-community structure. NESOM considers the edges inside and among groups, respectively, and evolves a new objective function for evaluating anti-community structure. And then, a new heuristic algorithm NESOM-AC is proposed for balanced anti-community detection, which consists of three stages: creation of initial structure, decomposition of redundant group, and adjustment of group membership. Inspired by NESOM, we finally develop a new synthetic benchmark NESOM-Net for performance comparison. Experimental results on NESOM-Net with up to 100,000 nodes and 16 real-world networks demonstrate the effectiveness of NESOM-AC in anti-community detection when compared with five state-of-the-art algorithms. (C) 2020 Elsevier Inc. All rights reserved.']}
{'论文标题':['A Nettree for pattern matching with flexible wildcard constraints'],'论文作者':['Youxi Wu', ';\xa0', 'Xindong Wu', ';\xa0', 'Yan Li'],'论文摘要':['In this paper, a new nonlinear structure called Nettree is proposed. A Nettree is different from a tree in that a node may have more than one parent. An algorithm, named Nettree for pattern Matching with flexible wildcard Constraints (NAMEIC), based on Nettree is designed to solve pattern matching with flexible wildcard constraints. The problem is exponential with regard to the pattern length m. We prove the correctness of the algorithm, and illustrate how it works through an example. NAMEIC is W*m times faster than an existing approach because the result can be given after creating the Nettree in one pass, where W is the maximal gap flexibility. Experiments validate the correctness and efficiency of NAMEIC.']}
{'论文标题':['Automatic document classification system for information retrieval and natural language processing system, has genetic learning classifier which determines field category for term cluster, using genetic algorithm'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Context-Dependent Propagating-Based Video Recommendation in Multimodal Heterogeneous Information Networks'],'论文作者':['Sang, L', ';\xa0', 'Xu, M', ';\xa0', 'Wu, XD'],'论文摘要':["With the emergence of online social networks (OSNs), video recommendation has come to play a crucial role in mitigating the semantic gap between users and videos. Conventional approaches to video recommendation primarily focus on exploiting content features or simple user-video interactions to model the users' preferences. Although these methods have achieved promising results, they fail to model the complex video context interdependency, which is obscure/hidden in heterogeneous auxiliary data from OSNs. In this paper, we study the problem of video recommendation in Heterogeneous Information Networks (HINs) due to its excellence in characterizing heterogeneous and complex context information. We propose a Context-Dependent Propagating Recommendation network (CDPRec) to obtain accurate video embedding and capture global context cues among videos in HINs. The CDPRec can iteratively propagate the contexts of a video along links in a graph-structured HIN and explore multiple types of dependencies among the surrounding video nodes. Then, each video is represented as the composition of the multimodal content feature and global dependency structure information using an attention network. The learned video embedding with sequential based recommendation are jointly optimized for the final rating prediction. Experimental results on real-world YouTube video recommendation scenarios demonstrate the effectiveness of the proposed methods compared with strong baselines."]}
{'论文标题':['DATA AND FUZZY PARTITION-BASED IMPORTANCE MEASURE AND ITS APPLICATION IN COMPREHENSIVE EVALUATION'],'论文作者':['Tong, ZC', ';\xa0', 'Zhang, Z'],'论文摘要':['Comprehensive evaluation is an important part of modern decision making for practical problems such as resource management and complex system optimization. However, as the evaluation indices are always interactive and correlated, it is very difficult to determine the weight of index system. To solve this problem, the paper first proposes the interval neighborhood based on decision information system with numerical attributes. Then, fuzzy partition coverings in terms of the neighborhood are generated. By taking the interval neighborhood as the similarity description strategy, the positive region in the covering rough sets as knowledge carrier of the information system. It shows a metric to measure the hidden importance of the attributes. The evaluation model is constructed by using Choquet integral as an aggregation operator. The illustrated example validates the effectiveness of the method.']}
{'论文标题':['A classification approach for learning concept-drift in noisy data stream'],'论文作者':['Zhang Yuhong', ';\xa0', 'Hu Xuegang', ';\xa0', 'Li Peipei'],'论文摘要':['Classification of data streams with concept drift has become one of hot research spots. However, noise in real data directly affects the result of detection of concept drift and the quality of classification. Therefore, an anti-noise approach is of important value for research and application. Based on the ensemble random decision tree, an effective classification model for stream classification, an incremental approach ICDC was proposed by introducing the Hoeffding Bounds inequality to distinguish concept drift and noise in classification, which adjusts the period of detection and window size for training data in accordance with the detection results. Extensive studies on synthetic and real streaming databases demonstrate that ICDC performs quite effectively compared with several known single or ensemble online algorithms.', '隐含概念漂移的数据流分类问题是数据挖掘领域研究的热点之一,而实际数据中的噪音会直接影响概念漂移检测及分类质量,因此具有良好抗噪性能的数据流分类方法具有重要的研究和应用价值.随机决策树的集成模型是一种有效的数据流分类模型,为此本文基于随机决策树,引入Hoeffding Bounds不等式来检测和区分概念漂移和噪音,根据检测结果动态调整滑动窗口的大小和漂移检测周期,并提出一种增量式的集成分类方法ICDC,实验结果表明,本文算法在含噪音数据流上处理概念漂移是有效的.']}
{'论文标题':['Integrating induction and deduction for noisy data mining'],'论文作者':['Zhang, Y', ';\xa0', 'Wu, XD'],'论文摘要':['Data mining research has been drawing a lot of interest and attention from various fields since late 1980s. The rapid progress has been achieved from three aspects: the prosperity of data mining conferences, the significant number of data mining algorithms, and widely applied areas of data mining techniques. With the continuing growth of the data volumes in many domains, the need of employing data mining techniques provides not only new opportunities but also immense challenges. In this article, we present our study on a challenging topic - integrating induction and deduction for noisy data mining. In particular, we assume the mechanism that corrupts the input data is a set of structured knowledge in the form of Associative Corruption (AC) rules. We apply deductive reasoning to generate the noise corruption rules: make error corrections on the input data with the help of these rules; and perform inductive learning from the corrected input data. Our experimental results show that the proposed integration framework is effective. (C) 2009 Published by Elsevier Inc.']}
{'论文标题':['Positive and Unlabeled Multi-Graph Learning'],'论文作者':['Wu, J', ';\xa0', 'Pan, SR', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we advance graph classification to handle multi-graph learning for complicated objects, where each object is represented as a bag of graphs and the label is only available to each bag but not individual graphs. In addition, when training classifiers, users are only given a handful of positive bags and many unlabeled bags, and the learning objective is to train models to classify previously unseen graph bags with maximum accuracy. To achieve the goal, we propose a positive and unlabeled multi-graph learning (puMGL) framework to first select informative subgraphs to convert graphs into a feature space. To utilize unlabeled bags for learning, puMGL assigns a confidence weight to each bag and dynamically adjusts its weight value to select "reliable negative bags."A number of representative graphs, selected from positive bags and identified reliable negative graph bags, form a "margin graph pool"which serves as the base for deriving subgraph patterns, training graph classifiers, and further updating the bag weight values. A closed-loop iterative process helps discover optimal subgraphs from positive and unlabeled graph bags for learning. Experimental comparisons demonstrate the performance of puMGL for classifying real-world complicated objects.']}
{'论文标题':['Separation and recovery Markov boundary discovery and its application in EEG-based emotion recognition'],'论文作者':['Wu, XY', ';\xa0', 'Jiang, BB', ';\xa0', 'Chen, HH'],'论文摘要':['In a Bayesian network (BN), the Markov boundary (MB) presents the local causal structure around a target. Due to the interpretability and robustness, it has been widely applied to feature selection and BN structure learning. However, existing MB discovery algorithms might fail to identify some true positives, leading to poor performance in real-world applications. To tackle this issue, we introduce a two-phase-discovery strategy to search more true positives. Based on this strategy, we propose a more accurate and data-efficient algorithm, separation and recovery MB discovery algorithm (SRMB). SRMB first discovers an incomplete parent-child set and spouse set via an MB separation process, and then retrieves the ignored true positives via an MB recovery process, which further exploits a symmetry test to improve accuracy in unfaithful cases. Experiments on standard BN and real-world data sets demonstrate the effectiveness and superiority of SRMB in terms of MB discovery, BN structure learning, and feature selection. To demonstrate the superiority of SRMB in data with distribution shift, we further apply SRMB to EEG-based emotion recognition tasks, where distribution shift exists in multiple unstable sessions. We prove that the most predictive features are from Gamma/Beta frequency bands and are distributed at the lateral temporal area. (c) 2021 Elsevier Inc. All rights reserved.']}
{'论文标题':['Combining embedding-based and symbol-based methods for entity alignment'],'论文作者':['Jiang, TT', ';\xa0', 'Bu, CY', ';\xa0', 'Wu, XD'],'论文摘要':['The objective of entity alignment is to judge whether entities refer to the same object in the real world. Methods for entity alignment can be grossly divided into two groups: conventional symbol-based entity alignment methods and embedding-based entity alignment methods. Both groups of methods have advantages and disadvantages (which are detailed in Section 1). Therefore, combining the advantages of both methods might be a promising strategy. However, to the best of our knowledge, only the RTEA algorithm that was proposed in our previous conference paper (Proceeding of Pacific Rim International Conference on Artificial Intelligence, pp. 162-175, 2019) utilizes this strategy for entity alignment. This manuscript is an extended version of that conference paper, in which an improved algorithm, namely, ESEA (combining embedding-based and symbol-based methods for entity alignment), is proposed based on the following steps. First, a novel method for combining embedding models with symbol-based models is proposed. Entities with high vector similarities are obtained through a hybrid embedding model, and the final aligned entity pairs are calculated via symbol-based methods. Second, a series of symbol based methods, instead of only the edit distance method in the original version, are combined with embedding-based methods for relation alignment. Third, we combine symbol-based and embedding based methods in a more complicated framework with the objective of better exploiting the advantages of both methods. The experimental results on real-world datasets demonstrate that the proposed method outperformed several state-of-the-art embedding-based entity alignment approaches and outperformed our previous RTEA method.(c) 2021 Elsevier Ltd. All rights reserved.']}
{'论文标题':['A Heuristic Algorithm for MPMGOOC'],'论文作者':['Wu You-Xiu', ';\xa0', 'Wu Xin-Dong', ';\xa0', 'Min Fan'],'论文摘要':['Maximum Pattern Matching with Gaps and the One-Off Condition (MPMGOOC) is an interesting and challenging pattern matching problem, which seeks to find the maximal number of occurrences of a pattern in a sequence. In this paper, a heuristic algorithm based on a new nonlinear data structure, Nettree, is proposed for this problem. A Nettree is different from a regular tree in that a node may have more than one parent. The algorithm is named Selecting Better Occurrence (SBO). SBO uses some special concepts and properties of the Nettree to solve the task. In the loop of finding an occurrence, SBO uses two strategies, Strategy of Greedy-Search Parent (SGSP) and Strategy of RightMost Parent (SRMP) to find two occurrences with the same leaf, and then selects a better occurrence from the results of SGSP and SRMP. The main ideas of SGSP and SRMP are to find an Approximately Optimal Parent (AOP) and the rightmost parent of the current node at each step in the process of searching for an occurrence, respectively. Extensive experimental results on real-world biological data demonstrate that SBO achieves the best performance among all competitive algorithms in terms of solution quality. This paper not only provides a heuristic solution for the MPMGOOC problem, but also shows that the Nettree can be used to solve other complex problems.', '具有间隙约束和一次性条件的最大模式匹配(Maximum Pattern Matching with Gaps and One-Off Condition, MPMGOOC)是一种具有通配符长度约束的模式匹配问题,其任务是寻找彼此互不相关的最多出现．文中基于一种新的非线性数据结构网树,提出了一种解决MPMGOOC问题的启发式算法．与树结构不同之处在于,除根结点外,网树中任何结点可以多于1个双亲结点．文中给出了网树的定义及其相关的概念和性质．基于这些概念和性质,提出了一种选择较优出现(Selecting Better Occurrence,SBO)的启发式算法．该算法在搜索一个出现的循环中,采用了贪婪搜索双亲策略(Strategy of GreedySearch Parent,SGSP)和最右双亲策略(Strategy of RightMost Parent,SRMP)寻找相同叶子的两个出现并选择其中较好的出现作为SBO算法的结果．SGSP策略的核心思想是每一步都寻找当前结点的一个近似最优双亲(Approximately Optimimal Parent,AOP);SRMP策略的核心思想是每一步都寻找当前结点的最右双亲结点．实验结果表明,在多数情况下SBO算法可以获得更好的解且解的质量较其它算法有显著的提高．文中不但提供了一个解决MPMGOOC问题的启发式算法,更重要的是对于求解其它复杂问题具有一定的参考价值．']}
{'论文标题':['An Efficient Ensemble Method for Classifying Skewed Data Streams'],'论文作者':['Zhang, J', ';\xa0', 'Hu, XG', ';\xa0', 'Li, PP'],'论文摘要':['Class distributions of data streams in real application are usually unbalanced, they are hence called Skewed Data Streams (abbreviated as SDS). However, in the classification of SDS, it is a challenge for traditional methods because of the difficulty in the recognition of minority classes. Therefore, many approaches have been proposed to improve the recognition rate of minority classes, while they are time-consuming. Motivated by this, we propose an efficient Ensemble method for Classifying SDS called ECSDS. Our algorithm creates multiple classifiers based on C4.5, and adopts the threshold of F1-value to limit the updating frequency of classifiers. Meanwhile, it adds misclassified positive instances into the training data to guarantee the effectiveness of classifiers when updating. Experimental studies demonstrate that our proposed method enables reducing the time overhead and maintains a good performance on the classification accuracy.']}
{'论文标题':['Streaming Feature Selection for Multilabel Learning Based on Fuzzy Mutual Information'],'论文作者':['Lin, YJ', ' (', 'Lin, Yaojin', ') ', ', ', ';\xa0', 'Hu, QH', ' (', 'Hu, Qinghua', ') ', ';\xa0', 'Liu, JH', ' (', 'Liu, Jinghua', ') ', ';\xa0', 'Li, JJ', ' (', 'Li, Jinjin', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':['Due to complex semantics, a sample may be associated with multiple labels in various classification and recognition tasks. Multilabel learning generates trainingmodels tomap feature vectors to multiple labels. There are several significant challenges in multilabel learning. Samples in multilabel learning are usually described with high-dimensional features and some features may be sequentially extracted. Thus, we do not know the full feature set at the beginning of learning, referred to as streaming features. In this paper, we introduce fuzzy mutual information to evaluate the quality of features in multilabel learning, and design efficient algorithms to conduct multilabel feature selection when the feature space is completely known or partially known in advance. These algorithms are called multilabel feature selection with label correlation (MUCO) and multilabel streaming feature selection (MSFS), respectively. MSFS consists of two key steps: online relevance analysis and online redundancy analysis. In addition, we design a metric to measure the correlation between the label sets, and both MUCO and MSFS take label correlation to consideration. The proposed algorithms are not only able to select features from streaming features, but also able to select features for ordinal multilabel learning. However streaming feature selection is more efficient. The proposed algorithms are tested with a collection of multilabel learning tasks. The experimental results illustrate the effectiveness of the proposed algorithms.']}
{'论文标题':['Mining stable patterns in multiple correlated databases'],'论文作者':['Lin, YJ', ';\xa0', 'Hu, XG', ';\xa0', 'Wu, XD'],'论文摘要':['Many kinds of patterns (e.g., association rules, negative association rules, sequential patterns, and temporal patterns) have been studied for various applications, but very little work has been reported on multiple correlated databases that are all relevant. This paper proposes an efficient method for mining stable patterns from multiple correlated databases. First, we define the notion of stable items according to two constraint conditions, minsupp and van value. We then measure the similarity between stable items based on gray relational analysis, and present a hierarchical gray clustering method for mining stable patterns consisting of stable items. Finally, experiments are conducted on four datasets, and the results of the experiments show that our method is useful and efficient. (C) 2013 Elsevier B.V. All rights reserved.']}
{'论文标题':['Rule induction with extension matrices'],'论文作者':['Wu, XD'],'论文摘要':["This article presents a heuristic, attribute-based, noise-tolerant data mining program, HCV (Version 2.0), based on the newly-developed extension matrix approach. By dividing the positive examples (PE) of a specific class in a given example set into intersecting groups and adopting a set of strategies to find a heuristic conjunctive formula in each group which covers all the group's positive examples and none of the negative examples (NE), the HCV induction algorithm adopted in the HCV (Version 2.0) software finds a description formula in the form of variable-valued logic for PE against NE in low-order polynomial time at induction time. In addition to the HCV induction algorithm, this article also outlines some of the techniques for noise handling and discretization of numerical domains developed and implemented in the HCV (Version 2.0) software, and provides a performance comparison of HCV (Version 2.0) with other data mining algorithms ID3, C4.5, C4.5rules, and NewID in noisy and continuous domains. The empirical comparison shows that the rules generated by HCV (Version 2.0) are more compact than the decision trees or rules produced by ID3-like algorithms, and HCV's predicative accuracy is competitive with ID3-like algorithms."]}
{'论文标题':['Scalable and High Performing Learning and Mining in Large-Scale Networked Environments: A State-of-the-art Survey'],'论文作者':['Trandafili, E', ';\xa0', 'Biba, M'],'论文摘要':['Scalability is a major issue in the application of machine learning and data mining to large-scale networked environments. While there has been important progress in the learnability of models for medium-sized datasets, there is still much challenge in facing large-scale systems. In particular, with the evolution of distributed and networked environments, the complexity of the learning and mining process has now grown due to the possibility to integrating more data in the learning process. This paper provides a survey on the state-of-the-art on the methods and algorithms to enhance scalability of machine learning and data mining for large-scale networked systems.']}
{'论文标题':['A Serial Sample Selection Framework for Active Learning'],'论文作者':['Li, CC', ';\xa0', 'Zhao, PP', ';\xa0', 'Cui, ZM'],'论文摘要':['Active Learning is a machine learning and data mining technique that selects the most informative samples for labeling and uses them as training data. It aims to obtain a high performance classifier by labeling as little data as possible from large amount of unlabeled samples, which means sampling strategy is the core issue. Existing approaches either tend to ignore information in unlabeled data and are prone to querying outliers or noise samples, or calculate large amounts of non-informative samples leading to significant computation cost. In order to solve above problems, this paper proposed a serial active learning framework. It first measures uncertainty of unlabeled samples and selects the most uncertain sample set. From which, it further generates the most representative sample set based on the mutual information criterion. Finally, the framework selects the most informative sample from the most representative sample set based on expected error reduction strategy. Experimental results on multiple datasets show that our approach outperforms Random Sampling and the state of the art adaptive active learning method.']}
{'论文标题':['HSNP-Miner: High Utility Self-Adaptive Nonoverlapping Pattern Mining'],'论文作者':['Hossain, M.M.', ';\xa0', 'Youxi Wu', ';\xa0', 'Yan Li'],'论文摘要':['Sequential pattern mining (SPM) under the nonoverlapping condition (or nonoverlapping SPM) is a type of data mining used to extract frequent gapped subsequences (known as patterns) from sequences, which is more valuable and versatile than other related methods. In nonoverlapping SPM, two occurrences cannot reuse the same sequence letter in the exact location as the occurrences. This method evaluates the frequency of the patterns in the sequence, and ignores the impact of external utility (item price or profit). Therefore, some low-frequency and essential patterns are overlooked. To address this issue, this paper introduces High Utility Self-adaptive Nonoverlapping Pattern (HSNP) mining and proposes HSNP-Miner, which includes two steps: support calculation and candidate pattern generation. To calculate the support, we propose the NoSup algorithm, which can effectively calculate support while avoiding the creation of redundant nodes. An advanced upper bound method is employed to generate the candidate patterns more efficiently. Compared to other competitive methods, the experimental results demonstrate the efficiency of the proposed algorithm and the uniqueness of nonoverlapping sequence pat-tarns.']}
{'论文标题':['Multi-Label Streaming Feature Selection via Class-Imbalance Aware Rough Set'],'论文作者':['Zou, YZ', ';\xa0', 'Hu, XG', ';\xa0', 'Li, JL'],'论文摘要':['Multi-label feature selection aims to select discriminative attributes in multi-label scenario, but most of existing multi-label feature selection methods fail to consider streaming features, i.e. features gradually flow one by one, which is more common in real-world applications. In addition, though there are already some representative works on multi-label streaming feature selection, they fail to tackle the class-imbalance problem, which exists widely in multi-label learning. In fact, classimbalance will lead to the performance degradation of multi-label learning models. Thus considering class-imbalance problem in multi-label scenario is beneficial to multi-label feature selection because more precise feature evaluation is achieved. Motivated by this, we propose a new rough set named as class-imbalance aware rough set model which can fit class-imbalance problem well. To address streaming features, we construct a novel streaming feature selection framework called SFSCI(Streaming Feature Selection via Class-Imbalance aware rough set), which contains online irrelevancy discarding and online redundancy reduction. Finally, an empirical study on a series of benchmark data sets demonstrates that the proposed method is superior to other state-of-the-art multi-label feature selection methods, including several multi-label streaming feature selection methods.']}
{'论文标题':['Collaborative Knowledge-Enhanced Recommendation with Self-Supervisions'],'论文作者':['Pan, ZQ', ';\xa0', 'Chen, HH'],'论文摘要':['Knowledge-enhanced recommendation (KER) aims to integrate the knowledge graph (KG) into collaborative filtering (CF) for alleviating the sparsity and cold start problems. The state-of-the-art graph neural network (GNN)-based methods mainly focus on exploiting the connectivity between entities in the knowledge graph, while neglecting the interaction relation between items reflected in the user-item interactions. Moreover, the widely adopted BPR loss for model optimization fails to provide sufficient supervisions for learning discriminative representation of users and items. To address these issues, we propose the collaborative knowledge-enhanced recommendation (CKER) method. Specifically, CKER proposes a collaborative graph convolution network (CGCN) to learn the user and item representations from the connection between items in the constructed interaction graph and the connectivity between entities in the knowledge graph. Moreover, we introduce the self-supervised learning to maximize the mutual information between the interaction- and knowledge-aware user preferences by deriving additional supervision signals. We conduct comprehensive experiments on two benchmark datasets, namely Amazon-Book and Last-FM, and the experimental results show that CKER can outperform the state-of-the-art baselines in terms of recall and NDCG on knowledge-enhanced recommendation.']}
{'论文标题':['Class Noise Elimination Approach for Large Datasets Based on a Combination of Classifiers'],'论文作者':['Zerhari, B'],'论文摘要':['Noise points, or class noise, detection and elimination became increasingly important to handle large datasets. In fact, eliminating noise in this environment helps reduce computing costs, especially when using clustering algorithms. Nowadays, large varieties of clustering algorithms exist and produce good results. However, they often assume that the input data are free or have very low level of noise, which is rarely the case in real Big Data context. In this paper, we present a noise detection and elimination approach for large datasets. This approach relies on four important steps: divide data into subsets, extract the best rules, apply different classifiers to the subsets, and finally combine the classifiers results.']}
{'论文标题':['Spatiochromatic Context Modeling for Color Saliency Analysis'],'论文作者':['Zhang, J', ' (', 'Zhang, Jun', ') ', ';\xa0', 'Wang, M', ' (', 'Wang, Meng', ') ', ';\xa0', 'Zhang, SP', ' (', 'Zhang, Shengping', ') ', ';\xa0', 'Li, XL', ' (', 'Li, Xuelong', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') ', ', '],'论文摘要':['Visual saliency is one of the most noteworthy perceptual abilities of human vision. Recent progress in cognitive psychology suggests that: 1) visual saliency analysis is mainly completed by the bottom-up mechanism consisting of feedforward low-level processing in primary visual cortex (area V1) and 2) color interacts with spatial cues and is influenced by the neighborhood context, and thus it plays an important role in a visual saliency analysis. From a computational perspective, the most existing saliency modeling approaches exploit multiple independent visual cues, irrespective of their interactions (or are not computed explicitly), and ignore contextual influences induced by neighboring colors. In addition, the use of color is often underestimated in the visual saliency analysis. In this paper, we propose a simple yet effective color saliency model that considers color as the only visual cue and mimics the color processing in V1. Our approach uses region-/boundary-defined color features with spatiochromatic filtering by considering local color-orientation interactions, therefore captures homogeneous color elements, subtle textures within the object and the overall salient object from the color image. To account for color contextual influences, we present a divisive normalization method for chromatic stimuli through the pooling of contrary/complementary color units. We further define a color perceptual metric over the entire scene to produce saliency maps for color regions and color boundaries individually. These maps are finally globally integrated into a one single saliency map. The final saliency map is produced by Gaussian blurring for robustness. We evaluate the proposed method on both synthetic stimuli and several benchmark saliency data sets from the visual saliency analysis to salient object detection. The experimental results demonstrate that the use of color as a unique visual cue achieves competitive results on par with or better than 12 state-of-the-art approaches.']}
{'论文标题':['Domain adaptation via Multi-Layer Transfer Learning'],'论文作者':['Pan, JH', ' (', 'Pan, Jianhan', ') ', ';\xa0', 'Hu, XG', ' (', 'Hu, Xuegang', ') ', ';\xa0', 'Li, PP', ' (', 'Li, Peipei', ') ', ';\xa0', 'Li, HZ', ' (', 'Li, Huizong', ') ', ';\xa0', 'He, W', ' (', 'He, Wei', ') ', ';\xa0', 'Zhang, YH', ' (', 'Zhang, Yuhong', ') ', ';\xa0', 'Lin, YJ', ' (', 'Lin, Yaojin', ') '],'论文摘要':['Transfer learning, which leverages labeled data in a source domain to train an accurate classifier for classification tasks in a target domain, has attracted extensive research interests recently for its effectiveness proven by many studies. Previous approaches adopt a common strategy that models the shared structure as a bridge across different domains by reducing distribution divergences. However, those approaches totally ignore specific latent spaces, which can be utilized to learn non-shared concepts. Only specific latent spaces contain specific latent factors, lacking which will lead to ineffective distinct concept learning. Additionally, only learning latent factors in one latent feature space layer may ignore those in the other layers. The missing latent factors may also help us to model the latent structure shared as the bridge. This paper proposes a novel transfer learning method Multi-Layer Transfer Learning (MLTL). MLTL first generates specific latent feature spaces. Second, it combines these specific latent feature spaces with common latent feature space into one latent feature space layer. Third, it generates multiple layers to learn the corresponding distributions on different layers with their pluralism simultaneously. Specifically, the pluralism of the distributions on different layers means that learning the distributions on one layer can help us to learn the distributions on the others. Furthermore, an iterative algorithm based on Non-Negative Matrix Tri-Factorization is proposed to solve the optimization problem. Comprehensive experiments demonstrate that MLTL can significantly outperform the state-of-the-art learning methods on topic and sentiment classification tasks. (C) 2016 Elsevier B.V. All rights reserved.']}
{'论文标题':['Cognitive structure learning model for hierarchical multi-label text classification'],'论文作者':['Wang, BY', ';\xa0', 'Hu, XG', ';\xa0', 'Yu, PS'],'论文摘要':['The human mind grows in learning new knowledge, which finally organizes and develops a basic mental pattern called cognitive structure. Hierarchical multi-label text classification (HMLTC), a fundamental but challenging task in many real-world applications, aims to classify the documents with hierarchical labels to form a resembling cognitive structure learning process. Existing approaches for HMLTC mainly focus on partial new knowledge learning or the global cognitive-structure-like label structure utilization in a cognitive view. However, the complete cognitive structure learning model is a unity that is indispensably constructed by the global label structure utilization and partial knowledge learning, which is ignored among those HMLTC approaches. To address this problem, we will imitate the cognitive structure learning process into the HMLTC learning and propose a unified framework called Hierarchical Cognitive Structure Learning Model (HCSM) in this paper. HCSM is composed of the Attentional Ordered Recurrent Neural Network (AORNN) submodule and Hierarchical Bi-Directional Capsule (HBiCaps) submodule. Both submodules utilize the partial new knowledge and global hierarchical label structure comprehensively for the HMLTC task. On the one hand, AORNN extracts the semantic vector as partial new knowledge from the original text by the word-level and hierarchy-level embedding granularities. On the other hand, AORNN builds the hierarchical text representation learning corresponding to the global label structure by the document-level neurons ordering. HBiCaps employs an iteration to form a unified label categorization process similar to cognitive-structure learning: firstly, using the probability computation of local hierarchical relationships to maintain partial knowledge learning; secondly, modifying the global hierarchical label structure based on the dynamic routing mechanism between capsules. Moreover, the experimental results on four benchmark datasets demonstrate that HCSM outperforms or matches state-of-the-art text classification methods. (c) 2021 Elsevier B.V. All rights reserved.']}
{'论文标题':['EC signal pre-processing techniques for scanning inspection of defect in multi-layered conductive structures'],'论文作者':['Huang Ping-jie', ';\xa0', 'Hou Di-bo', ';\xa0', 'Zhou Ze-kui'],'论文摘要':['Eddy current signal pre-processing techniques for noise elimination using the discrete wavelet transform and wavelet packet analysis methods are studied. Combining the multi-layered conductive structures defect inspection experiments, the signal processing methods presented are applied and results are compared with the SNR and RMSE as criteria and the best de-noising method is recommended.', '应用小波多分辨分解及重构技术和小波包分析技术进行检测信号预处理（噪声和干扰信号分离与去除）的基本原理，结合电涡流扫描检测实验，采用离散小波变换强制消噪法、软阈值消噪法和不同熵准则的小波包分析消噪法对检测信号进行预处理，并以SNR和RMSE为判断消噪效果好坏的标准，进行了效果的比较和优选。从理论分析和实验研究结果可知，分离提离等干扰信号，可采用强制法；消除高频噪声，采用基于Shannon熵准则的WPA法，效果最好。']}
{'论文标题':['Clustering analysis in the evaluation of securities investment funds'],'论文作者':['Zhang, JQ', ';\xa0', 'Yang, KY'],'论文摘要':["Clustering analysis as one of the key components of data mining has been widely applied. This paper aimed to apply a clustering algorithm to classify and evaluate securities investment funds. It established a fund evaluation index system by researching the indexes that are influenced by the performance of funds. It drew upon domestic and foreign mature funds evaluation theory and used the data mining function of Excel to establish a clustering analysis model. Finally, this paper used 40 equity funds as sample data to conduct an empirical research. The cluster results would be beneficial in evaluating funds' performance and guiding the decision making on rational investment.", '大脑是生物体内结构和功能最复杂的组织,其中包含上千亿个神经元。作为大脑构造的基本单位,神经元的结构和功能包含很多因素,其中神经元的几何形态特征就是一个重要方面。大脑中神经元的几何形态复杂多样,对其识别分类问题是一个难题。本文在模糊聚类的基础上根据神经元的几何形态建立了模糊集模型,并利用多数据库分类模型中的最优划分模型对模糊聚类分析法进行改进。将改进后的模糊聚类方法用于对神经元的识别分类,得到最优的分类结果。根据聚类的评价方法,与其他的聚类方法比较,证明了改进的模糊聚类方法能够得到更好的聚类效果。']}
{'论文标题':['Research progress and trend of cyberspace big search'],'论文作者':['Fang Bin-xing', ';\xa0', 'Jia Yan', ';\xa0', 'Yin Li-hua'],'论文摘要':['With the expansion of cyberspace, the development of Web application and the arrival of the era of big data, searching for the cyberspace has become an urgent requirement. The concept "cyberspace big search" was proposed and its basic features was discussed. Based on these features, the architecture for the big search was proposed, including five parts: acquiring and mining information from the cyberspace, constructing and managing knowledge wares, understanding and representing the user\'s search intention, matching and reasoning the search intention, security and privacy in big search. The research progress and trend of cyberspace big search are summarized.', '随着网络空间的拓展、网络应用模式的发展及大数据时代的到来,面向网络空间的下一代搜索引擎大搜索已具有迫切的需求。阐述了网络空间大搜索的内涵及其特点,提出了大搜索的研究范畴,包括泛在网络空间信息获取与发掘、知识仓库构建和管理、用户搜索意图准确理解与表示、用户意图高效匹配和推演、大搜索安全可信与隐私保护等方面的内容,并针对上述问题,指出了具有5S 特性的网络空间大搜索技术的发展趋势。']}
{'论文标题':['Short text extensions and concept drift detection based short text data flow classification method for use in text data stream mining field, involves updating classifier, according to concept of drift'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Long-term target tracking combined with re-detection'],'论文作者':['Wang, JJ', ';\xa0', 'Yang, HR', ';\xa0', 'Wu, DO'],'论文摘要':['Long-term visual tracking undergoes more challenges and is closer to realistic applications than short-term tracking. However, the performances of most existing methods have been limited in the long-term tracking tasks. In this work, we present a reliable yet simple long-term tracking method, which extends the state-of-the-art learning adaptive discriminative correlation filters (LADCF) tracking algorithm with a re-detection component based on the support vector machine (SVM) model. The LADCF tracking algorithm localizes the target in each frame, and the re-detector is able to efficiently re-detect the target in the whole image when the tracking fails. We further introduce a robust confidence degree evaluation criterion that combines the maximum response criterion and the average peak-to-correlation energy (APCE) to judge the confidence level of the predicted target. When the confidence degree is generally high, the SVM is updated accordingly. If the confidence drops sharply, the SVM re-detects the target. We perform extensive experiments on the OTB-2015 and UAV123 datasets. The experimental results demonstrate the effectiveness of our algorithm in long-term tracking.']}
{'论文标题':['An intelligent digital library system for biologists'],'论文作者':['Stone, J', ';\xa0', 'Wu, XD', ';\xa0', 'Greenblatt, M'],'论文摘要':['To aid researchers in obtaining, organizing and managing biological data, we have developed a sophisticated digital library system that utilizes advanced data mining techniques. Our digital library system is centralized with Web links to publicly accessible data repositories. Our digital library is based on a framework used for conventional libraries and an object-oriented paradigm, and will provide personalized user-centered services based on the user\'s areas of interests and preferences. To make personalized service possible, a "user profile" that represents the preferences of an individual user is constructed based upon the user\'s past activities, goals indicated by the user, and options. Utilizing these user profiles, our system will make relevant information available to the user in an appropriate form, amount, and level of detail with minimal user effort.']}
{'论文标题':['Multi-document summarization using closed patterns'],'论文作者':['Qiang, JP', ';\xa0', 'Chen, P', ';\xa0', 'Wu, XD'],'论文摘要':["There are two main categories of multi-document summarization: term-based and ontology-based methods. A term-based method cannot deal with the problems of polysemy and synonymy. An ontology-based approach addresses such problems by taking into account of the semantic information of document content, but the construction of ontology requires lots of manpower. To overcome these open problems, this paper presents a pattern-based model for generic multi-document summarization, which exploits closed patterns to extract the most salient sentences from a document collection and reduce redundancy in the summary. Our method calculates the weight of each sentence of a document collection by accumulating the weights of its covering closed patterns with respect to this sentence, and iteratively selects one sentence that owns the highest weight and less similarity to the previously selected sentences, until reaching the length limitation. The sentence weight calculation by patterns reduces the dimension and captures more relevant information. Our method combines the advantages of the term-based and ontology-based models while avoiding their weaknesses. Empirical studies on the benchmark DUC2004 datasets demonstrate that our pattern-based method significantly outperforms the state-of-the-art methods. Multi-document summarization can be used to extract a particular individual's opinions in the form of closed patterns, from this individual's documents shared in social networks, hence provides a useful tool for further analyzing the individual's behavior and influence in group activities. (C) 2016 Elsevier B.V. All rights reserved."]}
{'论文标题':['Towards efficient and effective discovery of Markov blankets for feature selection'],'论文作者':['Wang, H', ';\xa0', 'Ling, ZL', ';\xa0', 'Wu, XD'],'论文摘要':['The Markov blanket (MB), a key concept in a Bayesian network (BN), is essential for large-scale BN structure learning and optimal feature selection. Many MB discovery algorithms that are either efficient or effective have been proposed for addressing high-dimensional data. In this paper, we propose a new algorithm for Efficient and Effective MB discovery, called EEMB. Specifically, given a target feature, the EEMB algorithm discovers the PC (i.e., parents and children) and spouses of the target simultaneously and can distinguish PC from spouses during MB discovery. We compare EEMB with the state-of-the-art MB discovery algorithms using a series of benchmark BNs and real-world datasets. The experiments demonstrate that EEMB is competitive with the fastest MB discovery algorithm in terms of computational efficiency and achieves almost the same MB discovery accuracy as the most accurate of the compared algorithms. (C) 2019 Elsevier Inc. All rights reserved.']}
{'论文标题':['Saliency Detection with a Deeper Investigation of Light Field'],'论文作者':['Zhang, J', ';\xa0', 'Wang, M', ';\xa0', 'Wu, XD'],'论文摘要':["Although the light field has been recently recognized helpful in saliency detection, it is not comprehensively explored yet. In this work, we propose a new saliency detection model with light field data. The idea behind the proposed model originates from the following observations. (1) People can distinguish regions at different depth levels via adjusting the focus of eyes. Similarly, a light field image can generate a set of focal slices focusing at different depth levels, which suggests that a background can be weighted by selecting the corresponding slice. We show that background priors encoded by light field focusness have advantages in eliminating background distraction and enhancing the saliency by weighting the light field contrast. (2) Regions at closer depth ranges tend to be salient, while far in the distance mostly belong to the backgrounds. We show that foreground objects can be easily separated from similar or cluttered backgrounds by exploiting their light field depth. Extensive evaluations on the recently introduced Light Field Saliency Dataset (LFSD) [Li et al., 2014], including studies of different light field cues and comparisons with Li et al.'s method (the only reported light field saliency detection approach to our knowledge) and the 2D/3D state-of-the-art approaches extended with light field depth/focusness information, show that the investigated light field properties are complementary with each other and lead to improvements on 2D/3D models, and our approach produces superior results in comparison with the state-of-the-art."]}
{'论文标题':['A Method Study of Online Publication Time Extraction for Chinese Web News'],'论文作者':['Wang, LL', ';\xa0', 'Wu, GQ'],'论文摘要':['In Web search, the publication time of Web page plays an important role, because the return result is time-based in general. Besides, it is also used to locate the occurrence time of news event and further track the event evolution. We propose an efficient method to extract the publication time of Chinese Web news online in this paper. Focusing on the extraction, the method carries out from two aspects: one is that temporal information is generally hidden in Web news URL, which is exactly the publication time and the other is that the publication time is one of all the text nodes belonging to the DOM parsing tree of the HTML document of the corresponding Web page or part of it usually. Given 20 keywords for 3 popular Chinese search engines, we get 1200 items of Web news URLs, which are used to conduct the experiments and then analyze the accuracy and efficiency of publication time extraction. The experimental results show that the precision of extraction is up to 96.3% and the time consumption is just 60 seconds.']}
{'论文标题':['Quality assessment for image coding based on matching pursuit'],'论文作者':['Pang, JX', ' (', 'Pang, Jianxin', ') ', ';\xa0', 'Zhang, R', ' (', 'Zhang, Rong', ') ', ';\xa0', 'Lu, L', ' (', 'Lu, Lu', ') ', ';\xa0', 'Liu, ZK', ' (', 'Liu, Zhengkai', ') '],'论文摘要':['Valuation of image coding relies on not only the efficiency of the coding, but also the quality of the coded image. We present a new objective quality assessment metric for image coding based on matching pursuit. First of all, we get the characteristics of the most important structure by projecting the reference image onto the base functions from a dictionary using matching pursuit. Secondly, we process the reference image and gain the structure information of the images in the order of importance, projecting the images onto the structural characteristics. Finally the objective quality score is given by comparing the differences of structure information between the reference and coded images. Experimental results show that the proposed approach is well consistent with the subjective quality score.']}
{'论文标题':['Mining bridging rules between conceptual clusters'],'论文作者':['Zhang, SC', ';\xa0', 'Chen, F', ';\xa0', 'Wang, RL'],'论文摘要':['Bridging rules take the antecedent and action from different conceptual clusters. They are distinguished from association rules (frequent itemsets) because (1) they can be generated by the infrequent itemsets that are pruned in association rule mining, and (2) they are measured by their importance including the distance between two conceptual clusters, whereas frequent itemsets are measured only by their support. In this paper, we first design two algorithms for mining bridging rules between clusters, and then propose two non-linear metrics to measure their interestingness. We evaluate these algorithms experimentally and demonstrate that our approach is promising.']}
{'论文标题':['Real-time texture synthesis using s-tile set'],'论文作者':['Xue, F', ';\xa0', 'Zhang, YS', ';\xa0', 'Wang, RG'],'论文摘要':['This paper presents a novel method of generating a set of texture tiles from samples, which can be seamlessly tiled into arbitrary size textures in real-time. Compared to existing methods, our approach is simpler and more advantageous in eliminating visual seams that may exist in each tile of the existing methods, especially when the samples have elaborate features or distinct colors. Texture tiles generated by our approach can be regarded as single-colored tiles on each orthogonal direction border, which are easier for tiling and more suitable for sentence tiling. Experimental results demonstrate the feasibility and effectiveness of our approach.']}
{'论文标题':['Hybrid Collaborative Recommendation via Dual-Autoencoder'],'论文作者':['Dong, BB', ';\xa0', 'Zhu, Y', ';\xa0', 'Wu, XD'],'论文摘要':['With the rapid increase of internet information, personalized recommendation systems are an effective way to alleviate the information overload problem, which has attracted extensive attention in recent years. The traditional collaborative filtering utilizes matrix factorization methods to learn hidden feature representations of users and/or items. With deep learning achieved good performance in representation learning, the autoencoder model is widely applied in recommendation systems for the advantages of fast convergence and no label requirement. However, the previous recommendation systems may take the reconstruction output of an autoencoder as the prediction of missing values directly, which may deteriorate their performance and cause unsatisfactory results of recommendation. In addition, the parameters of an autoencoder need to be pre-trained ahead, which greatly increases the time complexity. To address these problems, in this paper, we propose a Hybrid Collaborative Recommendation method via Dual-Autoencoder (HCRDa). More specifically, firstly, a novel dual-autoencoder is utilized to simultaneously learn the feature representations of users and items in our HCRDa, which obviously reduces time complexity. Secondly, embedding matrix factorization into the training process of the autoencoder further improves the quality of hidden features for users and items. Finally, additional attributes of users and items are utilized to alleviate the cold start problem and to make hybrid recommendations. Comprehensive experiments on several real-world data sets demonstrate the effectiveness of our proposed method in comparison with several state-of-the-art methods.']}
{'论文标题':['Online streaming feature selection based on neighborhood rough set'],'论文作者':['Li, SJ', ' (', 'Li, Shuangjie', ') ', ';\xa0', 'Zhang, KX', ' (', 'Zhang, Kaixiang', ') ', ';\xa0', 'Li, YL', ' (', 'Li, Yali', ') ', ';\xa0', 'Wang, SQ', ' (', 'Wang, Shuqin', ') ', ';\xa0', 'Zhang, SQ', ' (', 'Zhang, Shaoqiang', ') '],'论文摘要':['Feature selection is considered as a necessary and significant pre-processing step in many fields, especially in machine learning. However, in some real problems, in which features flow one by one, many existing approaches do not work well on the online streaming features, and most online streaming feature selection (OSFS) methods face the challenge of requiring domain knowledge before setting optimal parameters in advance. Therefore, an effective feature selection method for online streaming features, named OFS-Gapknn, is proposed in this paper. A new neighborhood rough set relation is firstly defined, which combines the advantages of both the k-nearest and the Gap neighborhood. The proposed neighborhood relation can not only work well on the unevenly distributed sample space, but also need not any parameters and domain knowledge. Then, the relevance and redundancy features are analyzed by using the dependency based on the neighborhood rough set. Finally, one of the optimal feature subsets is obtained. To validate the effectiveness of the proposed algorithm, four traditional methods and three OSFS methods are compared with it on 11 datasets. Experimental results indicate the dominance and significance of the proposed method. (C) 2021 Published by Elsevier B.V.']}
{'论文标题':['A Twitter-based smoking cessation recruitment system'],'论文作者':['Hamed, A.A.', ';\xa0', 'Xindong Wu', ';\xa0', 'Fingar, J.R.'],'论文摘要':['Digital recruitment is increasingly becoming a popular avenue for identifying human subjects for various studies. The process starts with an online ad that describes the task and explains expectations. As social media has exploded in popularity, efforts are being made to use social media to recruit for new career opportunities. Particularly, LinkedIn and Twitter have enabled an emerging trend for matching individuals with possible opportunities based on their interests. This makes finding more relevant jobs easier for both employees and employers. There are, however, many unanswered questions about how best to do that. In this paper, we propose an innovative Twitter-based recruitment system for a smoking cessation nicotine patch study. The goals of the paper are to (1) provide the system specification and design, (2) propose the approach we have taken to solve the problem of digital recruitment, (3) present two new algorithms, one for Twitter user ranking and the other for digital recruitment using social media, and (4) present the promising outcome of the initial version of the system and summarize the results. This is the first effort to introduce a practical solution for digital recruitment campaigns that is large-scale, inexpensive, efficient and reaches out to individuals in real-time as their needs are expressed. A continuous update on how our system is performing, in real-time, can be viewed at https://twitter.com/TobaccoQuit.']}
{'论文标题':['Online streaming feature selection using adapted Neighborhood Rough Set'],'论文作者':['Zhou, P', ';\xa0', 'Hu, XG', ';\xa0', 'Wu, XD'],'论文摘要':['Online streaming feature selection, as a new approach which deals with feature streams in an online manner, has attracted much attention in recent years and played a critical role in dealing with high-dimensional problems. However, most of the existing online streaming feature selection methods need the domain information before learning and specifying the parameters in advance. It is hence a challenge to select unified and optimal parameters before learning for all different types of data sets. In this paper, we define a new Neighborhood Rough Set relation with adapted neighbors named the Gap relation and propose a new online streaming feature selection method based on this relation, named OFS-A3M. OFS-A3M does not require any domain knowledge and does not need to specify any parameters in advance. With the "maximal-dependency, maximal-relevance and maximal-significance" evaluation criteria, OFS-A3M can select features with high correlation, high dependency and low redundancy. Experimental studies on fifteen different types of data sets show that OFS-A3M is superior to traditional feature selection methods with the same numbers of features and state-of-the-art online streaming feature selection algorithms in an online manner. (C) 2018 Published by Elsevier Inc.']}
{'论文标题':['Topic Modeling over Short Texts by Incorporating Word Embeddings'],'论文作者':['Qiang, JP', ';\xa0', 'Chen, P', ';\xa0', 'Wu, XD'],'论文摘要':['Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn semantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo-texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models.']}
{'论文标题':['Pattern Matching with Flexible Wildcards'],'论文作者':['Wu, XD', ' (', 'Wu, Xindong', ') ', ', ', ';\xa0', 'Qiang, JP', ' (', 'Qiang, Ji-Peng', ') ', ';\xa0', 'Xie, F', ' (', 'Xie, Fei', ') ', ', '],'论文摘要':['Pattern matching with wildcards (PMW) has great theoretical and practical significance in bioinformatics, information retrieval, and pattern mining. Due to the uncertainty of wildcards, not only is the number of all matches exponential with respect to the maximal gap flexibility and the pattern length, but the matching positions in PMW are also hard to choose. The objective to count the maximal number of matches one by one is computationally infeasible. Therefore, rather than solving the generic PMW problem, many research efforts have further defined new problems within PMW according to different application backgrounds. To break through the limitations of either fixing the number or allowing an unbounded number of wildcards, pattern matching with flexible wildcards (PMFW) allows the users to control the ranges of wildcards. In this paper, we provide a survey on the state-of-the-art algorithms for PMFW, with detailed analyses and comparisons, and discuss challenges and opportunities in PMFW research and applications.']}
{'论文标题':['Image set classification based on cooperative sparse representation'],'论文作者':['Zheng, P', ';\xa0', 'Zhao, ZQ', ';\xa0', 'Wu, XD'],'论文摘要':['Image set classification has been widely applied to many real-life scenarios including surveillance videos, multi view camera networks and personal albums. Compared with single image based classification, it is more promising and therefore has attracted significant research attention in recent years. Traditional (forward) sparse representation (fSR) just makes use of training images to represent query ones. If we can find complementary information from backward sparse representation (bSR) which represents query images with training ones, the performance will be likely to be improved. However, for image set classification, the way to produce additional bases for bSR is a problem concerned as there is no other bases than the query set itself. In this paper, we extend cooperative sparse representation (CoSR) method, which integrates fSR and bSR together, to image set classification. In this process, we propose two schemes, namely \'Learning Bases\' and "Training Sets Division\', to produce the additional dictionary for bSR. And different from previous work, our work considers scene classification as a problem of image set classification, which will provide a new insight for scene classification. Experimental results show that the proposed model can obtain competitive recognition rates for image set classification. By combining information from these two opposite SRs, better results can be achieved. Also the feasibility for the formulation of image set classification on scene classification is validated.']}
{'论文标题':['Online Learning to Accelerate Neural Network Inference with Traveling Classifiers'],'论文作者':['Beyazit, E', ';\xa0', 'He, Y', ';\xa0', 'Wu, XD'],'论文摘要':['Deep neural networks trained on millions of instances can recognize a wide variety of patterns. It is common to use these pre-trained deep networks in applications where the domain specific training data is not readily available. Once a pre-trained network is deployed to such applications, some of the information contained in the network may be irrelevant due to the difference between the training set and the application data distributions. As a result, parts of the neural network become redundant and slow down inference. This redundancy is unknown until the model is deployed and input data is received. Therefore, it can only be identified and avoided in real-time. Existing works on neural network acceleration can not exploit such redundancy during offline training when the domain-specific datasets are unavailable. In this paper, we study online learning to accelerate neural network inference. We propose traveling classifiers that continuously learn from the activations of two consecutive network layers to accelerate inference in real-time. Traveling classifiers model class conditional probabilities to generate early predictions and bypass unnecessary computation of network layers. The classifiers also adaptively switch the layers they learn from by measuring the feature space differences between the activations. This traveling mechanism automatically adjusts the aggressiveness of the acceleration without sacrificing prediction accuracy. We demonstrate the performance of the proposed algorithm on the ImageNet dataset [10] using the state-of-the-art ResNet-50, ResNet-152 [18] and VGG-16 [38] architectures. Experiments demonstrate that our method significantly outperforms baseline approaches.']}
{'论文标题':['Joint Semi-Supervised Feature Selection and Classification through Bayesian Approach'],'论文作者':['Jiang, BB', ';\xa0', 'Wu, XY', ';\xa0', 'Chen, HH'],'论文摘要':['With the increasing data dimensionality, feature selection has become a fundamental task to deal with high-dimensional data. Semi-supervised feature selection focuses on the problem of how to learn a relevant feature subset in the case of abundant unlabeled data with few labeled data. In recent years, many semi-supervised feature selection algorithms have been proposed. However, these algorithms are implemented by separating the processes of feature selection and classifier training, such that they cannot simultaneously select features and learn a classifier with the selected features. Moreover, they ignore the difference of reliability inside unlabeled samples and directly use them in the training stage, which might cause performance degradation. In this paper, we propose a joint semi-supervised feature selection and classification algorithm (JSFS) which adopts a Bayesian approach to automatically select the relevant features and simultaneously learn a classifier. Instead of using all unlabeled samples indiscriminately, JSFS associates each unlabeled sample with a self-adjusting weight to distinguish the difference between them, which can effectively eliminate the irrelevant unlabeled samples via introducing a left-truncated Gaussian prior. Experiments on various datasets demonstrate the effectiveness and superiority of JSFS.']}
{'论文标题':['Time series indexing by dynamic covering with cross-range constraints'],'论文作者':['Sun, T', ';\xa0', 'Liu, HB', ';\xa0', 'Wu, XD'],'论文摘要':['Time series indexing plays an important role in querying and pattern mining of big data. This paper proposes a novel structure for tightly covering a given set of time series under the dynamic time warping similarity measurement. The structure, referred to as dynamic covering with cross-range constraints (DCRC), enables more efficient and scalable indexing to be developed than current hypercube-based partitioning approaches. In particular, a lower bound of the DTW distance from a given query time series to a DCRC-based cover set is introduced. By virtue of its tightness, which is proven theoretically, the lower bound can be used for pruning when querying on an indexing tree. If the DCRC-based lower bound (LB_DCRC) of an upper node in an index tree is larger than a given threshold, all child nodes can be pruned yielding a significant reduction in computational time. A hierarchical DCRC (HDCRC) structure is proposed to generate the DCRC-tree-based indexing and used to develop time series indexing and insertion algorithms. Experimental results for a selection of benchmark time series datasets are presented to illustrate the tightness of LB_DCRC, as well as the pruning efficiency on the DCRC-tree, especially when the time series have large deformations.']}
{'论文标题':['Mining Complex Patterns across Sequences with Gap Requirements'],'论文作者':['Zhu, XQ', ';\xa0', 'Wu, XD'],'论文摘要':['The recurring appearance of sequential patterns, when confined by the predefined gap requirements, often implies strong temporal correlations or trends among pattern elements. In this paper, we study the problem of mining a set of gap constrained sequential patterns across multiple sequences. Given a set of sequences S-1, S-2,., S-K constituting a single hyper-sequence S, we aim to find recurring patterns in S, say P, which may cross multiple sequences with all their matching characters in S bounded by the user specified gap constraints. Because of the combinatorial candidate explosion, traditional Apriori-based algorithms are computationally infeasible. Our research proposes a new mechanism to ensure pattern growing and pruning. When combining the pruning technique with our Gap Constrained Search (GCS) and map-based support prediction approaches, our method achieves a speed about 40 times faster than its other peers.']}
{'论文标题':['A new descriptive clustering algorithm based on nonnegative matrix factorization'],'论文作者':['Zhao Li', ';\xa0', 'Hong Peng', ';\xa0', 'Xindong Wu'],'论文摘要':['Nonnegative matrix factorization (NMF) provides a way for finding a part-based representation of nonnegative data. An important property of NMF is that it can produce a sparse representation of the data; however, in some applications, especially in text clustering, the sparse representation always consists of separated words, which cannot explicitly express the meaning of the basis vector. This paper presents a new descriptive clustering algorithm based on NMF, called DC-NMF that can avoid this separated word problem. In our proposed method, we embrace the phrase-by-document matrix in addition to the commonly used term-by-document matrix. Then, we use conjunct gradient descent to minimize the mean squared error objective function. Finally, we describe each cluster with the highest weighted element corresponding to one particular phrase. Our experimental results indicate that our method can obtain more ldquopurerdquo clusters than other methods.']}
{'论文标题':['Hierarchical GAN-Tree and Bi-Directional Capsules for multi-label image classification'],'论文作者':['Wang, BY', ';\xa0', 'Hu, XG', ';\xa0', 'Yu, PS'],'论文摘要':['Compared with the flat multi-label image classification, the hierarchical structure reserves a richer source of structural information to represent complicated relationships between labels in the real world. However, existing multi-label image classification methods focus on the accuracy of label prediction, ignoring the structural information embedded in the hierarchical label space. Furthermore, they hardly form the relevant visual feature space corresponding to the hierarchical label structure. In this paper, we propose a novel hierarchical framework based on the feature and label structural information named Hierarchical GAN-Tree and Bi-Directional Capsules (HGT&BC) to address these problems. We conduct Hierarchical GAN-Tree for feature space representation and Hierarchical Bi-Directional Capsules for label space classification, respectively. Hierarchical GAN-Tree generates hierarchical feature space using the unsupervised divisive clustering pattern according to the hierarchical structure, alleviating the mode-collapse of generators and the overfitting manifestation of conventional GANs. Hierarchical Bi-Directional Capsules utilize the hierarchical label structure in iterations of top-down and bottom-up processes: the top-down process integrates hierarchical relationships into the probability computation to enhance partial hierarchical relationships; the bottom-up process modifies the dynamic routing mechanism between capsules to represent semantic objects for the comprehensive global hierarchical classifiers. Owing to the two components, HGT&BC successfully expresses the hierarchical relationships in both feature and label space and improves the performance of multi-label image classification. Extensive experimental results on four benchmark datasets demonstrate the effectiveness and efficiency of our hierarchical framework in practice. (c) 2021 Elsevier B.V. All rights reserved.']}
{'论文标题':['Predictive Online Server Provisioning for Cost-Efficient IoT Data Streaming Across Collaborative Edges'],'论文作者':['Zhou, Z', ';\xa0', 'Chen, X', ';\xa0', 'Zhang, JS'],'论文摘要':['Edge computing is envisioned to be the de-facto paradigm of hosting emerging low latency Internet-of-Things (IoT) data streaming services.For IoT data streaming in edge computing, cost management is of strategic significance, due to the low cost-efficiency of edge servers. While existing literature adopts a reactive approach to dynamically provisioning edge servers to reduce cost, the delay of server activation and instantiation has been mostly ignored. In this paper, we target a proactive approach to dynamic edge server provisioning for real-time IoT data streaming across edge nodes, which adjusts server provisioning ahead of time, based on prediction of the upcoming workload. To effectively predict upcoming workload, a learning-based method online gradient descent is applied. We further combine the online learning method with an online optimization algorithm for server provisioning in a joint online optimization framework, through (1) minimizing of the regret incurred by inaccurate workload prediction, and (2) minimizing the cost incurred by near-optimal online decisions. The resulting predictive online algorithm can well leverage the power of prediction and achieve a good performance guarantee, as verified by both rigorous theoretical analysis and extensive trace-driven evaluations.']}
{'论文标题':['Ontology-Based Business Process Customization for Composite Web Services'],'论文作者':['Liang, QH', ';\xa0', 'Wu, XD', ';\xa0', 'Chi, CH'],'论文摘要':['A key goal of the Semantic Web is to shift social interaction patterns from a producer-centric paradigm to a consumer-centric one. Treating customers as the most valuable assets and making the business models work better for them are at the core of building successful consumer-centric business models. It follows that customizing business processes constitutes a major concern in the realm of a knowledge-pull-based human semantic Web. This paper conceptualizes the customization of service-based business processes leveraging the existing knowledge of Web services and business processes. We represent this conceptualization as a new Extensible Markup Language (XML) markup language Web Ontology Language-Business Process Customization (OWL-BPC), based on the de facto semantic markup language for Web-based information [Web Ontology Language (OWL)]. Furthermore, we report a framework, built on OWL-BPC, for customizing service-based business processes, which supports customization detection and enactment. Customization detection is enabled by a business-goal analysis, and customization enactment is enabled via event-condition-action rule inference. Our solution and framework have the following capabilities in dealing with inconsistencies and misalignments in business process interactions: 1) resolve semantic mismatch of process parameters; 2) handle behavioral mismatches which may or may not be compatible; and 3) process misaligned rendezvous requirements. Such capabilities are applicable to business processes with heterogeneous domain ontology. We present an architectural description of the implementation and a walk-through of an example of solving a customization problem as a validation of the proposed approach.']}
{'论文标题':['Dealing with predictive-but-unpredictable attributes in noisy data sources'],'论文作者':['Yang, Y', ';\xa0', 'Wu, XD', ';\xa0', 'Zhu, XQ'],'论文摘要':['Attribute noise can affect classification learning. Previous work in handling attribute noise has focused on those predictable attributes that can be predicted by the class and other attributes. However, attributes can often be predictive but unpredictable. Being predictive, they are essential to classification learning and it is important to handle their noise. Being unpredictable, they require strategies different from those of predictable attributes. This paper presents a study on identifying, cleansing and measuring noise for predictive-but-unpredictable attributes. New strategies are accordingly proposed. Both theoretical analysis and empirical evidence suggest that these strategies are more effective and more efficient than previous alternatives.']}
{'论文标题':['Predicting the Semantic Characteristics of Pulmonary Nodules using Feature Selection Based on Maximum-relevance Minimum-redundancy'],'论文作者':['Yang, J', ';\xa0', 'Shen, AB', ';\xa0', 'Chen, Y'],'论文摘要':["Computer-aided diagnosis (CAD) is mainly used in disease diagnosis and cause analysis. For example, using CAD to make early predictions of the semantic features of lung nodules is critical for helping physicians judge the semantic features of solitary pulmonary nodules. It is an effective method to predict disease using the features calculated from CT images. But how to select the most relevant features from the large number of image features is still a challenge. In this paper, we perform feature selection using maximum-relevance minimum-redundancy criteria based on applying a support vector machine (MRMR_SVM) on four types of computed image features to predict the semantic characteristics of pulmonary nodules over seven categories. The proposed method has the following advantages. 1) It improves work efficiency and reduces costs compared to manual evaluation by radiologists. 2) It combines a few key image features with specific semantic features to provide a basis for radiologists' diagnosis. 3) It eliminates noisy data and improves accuracy of early prediction of pulmonary nodules compared to using all features. The experimental results show that the proposed method performs well at predicting the semantic features of lung nodules in terms of accuracy and running time."]}
{'论文标题':['Optimizing widths with PSO for center selection of Gaussian radial basis function networks'],'论文作者':['Zhao, ZQ', ';\xa0', 'Wu, XD', ';\xa0', 'Gao, J'],'论文摘要':['The radial basis function (RBF) centers play different roles in determining the classification capability of a Gaussian radial basis function neural network (GRBFNN) and should hold different width values. However, it is very hard and time-consuming to optimize the centers and widths at the same time. In this paper, we introduce a new insight into this problem. We explore the impact of the definition of widths on the selection of the centers, propose an optimization algorithm of the RBF widths in order to select proper centers from the center candidate pool, and improve the classification performance of the GRBFNN. The design of the objective function of the optimization algorithm is based on the local mapping capability of each Gaussian RBF. Further, in the design of the objective function, we also handle the imbalanced problem which may occur even when different local regions have the same number of examples. Finally, the recursive orthogonal least square (ROLS) and genetic algorithm (GA), which are usually adopted to optimize the RBF centers, are separately used to select the centers from the center candidates with the initialized widths, in order to testify the validity of our proposed width initialization strategy on the selection of centers. Our experimental results show that, compared with the heuristic width setting method, the width optimization strategy makes the selected centers more appropriate, and improves the classification performance of the GRBFNN. Moreover, the GRBFNN constructed by our method can attain better classification performance than the RBF LS-SVM, which is a state-of-the-art classifier.']}
{'论文标题':['Qualitative simulation and reasoning with feature reduction based on boundary conditional entropy of knowledge'],'论文作者':['Cheng, Y', ';\xa0', 'Zhang, Y', ';\xa0', 'Jiang, X'],'论文摘要':['The present paper discusses a new definition of knowledge rough entropy based on boundary region from the aspect of Pawlak topology. This definition accurately reflects an idea that the uncertainty of set can be described by boundary region. It thus proves an important conclusion that boundary conditional entropy of knowledge monotonously reduces with the diminishing of information granularity. Combining qualitative reasoning technology with knowledge information entropy based on rough sets theory, a heuristic algorithm for feature reduction is proposed which can be used to eliminate the redundancy in the qualitative description and the qualitative differential equations are obtained. The result shows that the rough sets theory (RST) is of good reliability and prospect in qualitative reasoning and simulation.']}
{'论文标题':['Application of Negative Association Rules in Multi-database'],'论文作者':['Dong, XJ', ';\xa0', 'Shang, SJ', ';\xa0', 'Jiang, H'],'论文摘要':["The increasing use of multi-database technology has led to the development of many multi-database systems for real-world application. Large companies may have to confront the multiple data-source problem,so multi-database training has been an improtant research area. Negative association rules (NARs) play important roles in decision-making, but nowadays the techniques of NARs mining focus on mono-database,mining negative association rules in multi-database have not been caused people's attention. But NARs in multi-database are very important in real-world applications. Knowledge conflicts within databases may occur when mining both the positive and negative association rules simultaneously. This paper proposed synthesis correlation to resolve conflicts and a new algorithm PNAR_MDB for mining NARs in multi-database on base of previous work. The experimental results demonstrate that the algorithm is correct and effective."]}
{'论文标题':['Online Learning from Trapezoidal Data Streams'],'论文作者':['Zhang, Q', ';\xa0', 'Zhang, P', ';\xa0', 'Wu, XD'],'论文摘要':['In this paper, we study a new problem of continuous learning from doubly-streaming data where both data volume and feature space increase over time. We refer to the doubly-streaming data as trapezoidal data streams and the corresponding learning problem as online learning from trapezoidal data streams. The problem is challenging because both data volume and data dimension increase over time, and existing online learning [1], [2], online feature selection [3], and streaming feature selection algorithms [4], [5] are inapplicable. We propose a new Online Learning with Streaming Features algorithm (OLSF for short) and its two variants, which combine online learning [1], [2] and streaming feature selection [4], [5] to enable learning from trapezoidal data streams with infinite training instances and features. When a new training instance carrying new features arrives, a classifier updates the existing features by following the passive-aggressive update rule [2] and updates the new features by following the structural risk minimization principle. Feature sparsity is then introduced by using the projected truncation technique. We derive performance bounds of the OLSF algorithm and its variants. We also conduct experiments on real-world data sets to show the performance of the proposed algorithms.']}
{'论文标题':["Microbloggers' interest inference using a subgraph stream"],'论文作者':['Huang, XL', ';\xa0', 'Wang, H', ';\xa0', 'Hu, CX'],'论文摘要':["Inferring user interest over large-scale microblogs have attracted much attention in recent years. However, the emergence of the massive data, dynamic change of information and persistence of microblogs pose challenges to interest inference. Most of the existing approaches rarely take into account the combination of these microbloggers' characteristics within the model, which may incur information loss with nontrivial magnitude in real-time extraction of user interest and massive social data processing. To address these problems, in this paper, we propose a novel User-Networked Interest Topic Extraction in the form of Subgraph Stream (UNITE_SS) for microbloggers' interest inference. To be specific, we develop several strategies for the construction of subgraph stream to select the better strategy for user interest inference. Moreover, the information of microblogs in each subgraph is utilized to obtain a real-time and effective interest for microbloggers. The experimental evaluation on a large dataset from Sina Weibo, one of the most popular microblogs in China, demonstrates that the proposed approach outperforms the state-of-the-art baselines in terms of precision, mean reciprocal rank (MRR) as well as runtime from the effectiveness and efficiency perspectives."]}
{'论文标题':['Pattern matching with wildcards and gap-length constraints based on a centrality-degree graph'],'论文作者':['Guo, D', ';\xa0', 'Hu, XG', ';\xa0', 'Wu, XD'],'论文摘要':["Pattern matching with wildcards is a challenging topic in many domains, such as bioinformatics and information retrieval. This paper focuses on the problem with gap-length constraints and the one-off condition (The one-off condition means that each character can be used at most once in all occurrences of a pattern in the sequence). It is difficult to achieve the optimal solution. We propose a graph structure WON-Net (WON-Net is a graph structure. It stands for a network with the weighted centralization measure based on each node's centrality-degree. Its details are given in Definition 4.1) to obtain all candidate matching solutions and then design the WOW (WOW stands for pattern matching with wildcards based on WON-Net) algorithm with the weighted centralization measure based on nodes' centrality-degrees. We also propose an adjustment mechanism to balance the optimal solutions and the running time. We also define a new variant of WOW as WOW-delta. Theoretical analysis and experiments demonstrate that WOW and WOW-delta are more effective than their peers. Besides, the algorithms demonstrate an advantage on running time by parallel processing."]}
{'论文标题':['Automatic determination about precision parameter value based on inclusion degree with variable precision rough set model'],'论文作者':['Cheng, YS', ';\xa0', 'Zhan, WF', ';\xa0', 'Zhang, YZ'],'论文摘要':['The rough set theory provides a powerful approach for attributes reduction and data analysis. The variable precision rough set (VPRS) model, an extension of the original rough set approach, tolerates misclassifications of the training data to some degree, which promotes the applications of rough set theory in inconsistent information systems. However, in most existing algorithms of feature reduction based on VPRS, the precision parameter (beta) is introduced as prior knowledge, which restricts their applications because it is not clear how to set the beta value, By studying beta-consistency in the measurement of a decision table and the threshold value of the beta-consistent decision table, this paper presents an algorithm for automatic determination of the precision parameter value from a decision table based on VPRS. At the same time, the precision parameter value from our proposed method is compared with the thresholds from the decision-theoretic rough set (DTRS). The influences of the precision parameter are also discussed on attribute reduction, which shows the necessity of the estimated precision parameter from a decision table. The simulation results including VPRS and other classification methods in real data further indicate that different precision parameter values make a great difference on rules and setting a precise parameter near the threshold value of the beta-consistent decision table can precisely reflect the decision distribution of the decision table. (C) 2014 Elsevier Inc. All rights reserved.']}
{'论文标题':['Learning hierarchical user interest models from Web pages'],'论文作者':['Yang Feng-qin', ';\xa0', 'Sun Tie-Ii', ';\xa0', 'Sun Ji-gui'],'论文摘要':["We propose an algorithm for learning hierarchical user interest models according to the Web pages users have browsed. In this algorithm, the interests of a user are represented into a tree which is called a user interest tree, the content and the structure of which can change simultaneously to adapt to the changes in a user's interests. This expression represents a user's specific and general interests as a continuum. In some sense, specific interests correspond to short-term interests, while general interests correspond to longterm interests. So this representation more really reflects the users' interests. The algorithm can automatically model a user's multiple interest domains, dynamically generate the interest models and prune a user interest tree when the number of the nodes in it exceeds given value. Finally, we show the experiment results in a Chinese Web Site."]}
{'论文标题':['Temporal-spatial consistency of self-adaptive target response for long-term correlation filter tracking'],'论文作者':['Wang, H', ';\xa0', 'Wang, ZP', ';\xa0', 'Bu, YQ'],'论文摘要':['Owning to the impressive performance and high speed of the correlation filter (CF) visual tracking technology, the CF-based tracking gets a significant amount of attentions and obtains a large number of researches. But the traditional CF-based trackers have two inherent issues needed to be solved. On the one hand, the boundary effect caused by the circulant structure will affect the performance. On the other hand, the target response is assumed as Gaussian response and the value is fixed. In order to deal with the boundary effect, a spatially regularized method by imposing space penalties to the spatial coefficients is proposed. About the fixed target response, some researchers propose a framework that can adaptively change the response to deal with it, but the two issues only be coped with separately. We propose a tracker that utilizes the two merits jointly to win a better performance and a more robustness model which can cope with the complex environment to achieve a long-term tracking. We use an iteration solution to solve the joint cost function and ADMM algorithm to solve temporal-spatial consistency problem which can obtain a global solution and fast computation. We evaluate our method on the well-known visual tracking benchmark dataset called OTB50 and get a competitive performance result compared with the relevant trackers.']}
{'论文标题':['Markov Blanket Feature Selection with Non-Faithful Data Distributions'],'论文作者':['Yu, K', ';\xa0', 'Wu, XD', ';\xa0', 'Ding, W'],'论文摘要':['In faithful Bayesian networks, the Markov blanket of the class attribute is a unique and minimal feature subset for optimal feature selection. However, little attention has been paid to Markov blanket feature selection in a non-faithful environment which widely exists in the real world. To tackle this issue, in this paper, we deal with non-faithful data distributions and propose the concept of representative sets instead of Markov blankets. With a standard sparse group lasso for selection of features from the representative sets, we design an effective algorithm, SRS, for Markov blanket feature Selection via Representative Sets with non-faithful data distributions. Empirical studies demonstrate that SRS outperforms the state-of-the-art Markov blanket feature selectors and other well-established feature selection methods.']}
{'论文标题':['PLANT IDENTIFICATION USING TRIANGULAR REPRESENTATION BASED ON SALIENT POINTS AND MARGIN POINTS'],'论文作者':['Zhao, ZQ', ';\xa0', 'Hong, Y', ';\xa0', 'Wu, XD'],'论文摘要':['Leaf classification is an important component of living plant identification. A leaf contains important information for plant species identification in spite of its complexity. This paper introduces a method of recognizing leaf images based on triangular representations. A leaf is represented by local descriptors associated with margin sample points and salient sample points. We introduce three new triangular representations - salient triangle area representation (STAR), salient triangle side lengths representation (STSL), and salient triangle area, side lengths and two angles representation (STASLA), and then we combine two local descriptors - one provides a triangular representation of the leaf margin while the other represents the spatial correlation between salient points of the leaf and leaf margin. Experiments on the Image-CLEF 2011 leaf datasets show the effectiveness and the efficiency of the proposed method.']}
{'论文标题':['User-centered agents for structured information location'],'论文作者':['Xindong Wu', ';\xa0', 'Ngu, D.', ';\xa0', 'Pradhan, S.S.'],'论文摘要':["This paper designs an electronic commerce system that integrates conventional electronic commerce services with contemporary WWW advantages, such as comprehensive coverage and agents for information search and selection. We use a user-centered approach and apply data mining techniques in the design of agents for information search and selection. There are various agents in this electronic commerce system to perform different functions. Among them, SiteHelper is a unique agent in our system compared to existing electronic commerce systems. It acts as a housekeeper for the system and as a helper for the users to find relevant information. In order to assist the users in finding relevant information at the centralized location (with Web links to the global Web), SiteHelper interactively and incrementally learns about each user's areas of interest and aids them accordingly, by deploying data mining techniques with incremental learning facilities as its learning and inference engines."]}
{'论文标题':['Expectation maximization based ordering aggregation for improving the K2 structure learning algorithm'],'论文作者':['Amirkhani, H', ';\xa0', 'Rahmati, M'],'论文摘要':['Some of the basic algorithms for learning the structure of Bayesian networks, such as the well-known K2 algorithm, require a prior ordering over the nodes as part of the input. It is well known that the accuracy of the K2 algorithm is highly sensitive to the initial ordering. In this paper, we introduce the aggregation of ordering information provided by multiple experts to obtain a more robust node ordering. In order to reduce the effect of novice participants, the accuracy of each person is used in the aggregation process. The accuracies of participants, not known in advance, are estimated by the expectation maximization algorithm. Any possible contradictions occurred in the resulting aggregation are resolved by modelling the result as a directed graph and avoiding the cycles in this graph. Finally, the topological order of this graph is used as the initial ordering in the K2 algorithm. The experimental results demonstrate the effectiveness of the proposed method in improving the structure learning process.']}
{'论文标题':['AN INCREMENTAL DECISION TREE FOR MINING MULTILABEL DATA'],'论文作者':['Li, PP', ';\xa0', 'Wu, XD', ';\xa0', 'Wang, H'],'论文摘要':['Mining with multilabel data is a popular topic in data mining. When performing classification on multilabel data, existing methods using traditional classifiers, such as support vector machines (SVMs), k-nearest neighbor (k-NN), and decision trees, have relatively poor accuracy and efficiency. Motivated by this, we present a new algorithm adaptation method, namely, a decision tree-based method for multilabel classification in domains with large-scale data sets called decision tree for multi-label classification (DTML). We build an incremental decision tree to reduce the learning time and divide the training data and adopt the k-NN classifier at leaves to improve the classification accuracy. Extensive studies show that our algorithm can efficiently learn from multilabel data while maintaining good performance on example-based evaluation metrics compared to nine state-of-the-art multilabel classification methods. Thus, we draw a conclusion that we provide an efficient and effective incremental algorithm adaptation method for multilabel classification especially in domains with large-scale multilabel data.']}
{'论文标题':['Scalable representative instance selection and ranking'],'论文作者':['Xingquan Zhu', ';\xa0', 'Xindong Wu'],'论文摘要':['Finding a small set of representative instances for large datasets can bring various benefits to data mining practitioners so they can (1) build a learner superior to the one constructed from the whole massive data; and (2) avoid working on the whole original dataset all the time. We propose in this paper a scalable representative instance selection and ranking (SRISTAR pronounced 3STAR) mechanism, which carries two unique features: (1) it provides a representative instance ranking list, so that users can always select instances from the top to the bottom, based on the number of examples they prefer; and (2) it investigates the behaviors of the underlying examples for instance selection, and the selection procedure tries to optimize the expected future error. Given a dataset, we first cluster instances into small data cells, each of which consists of instances with similar behaviors. Then we progressively evaluate data cells and their combinations, and order them into a list such that the learners built from the top cells are more accurate.']}
{'论文标题':['Learning Wasserstein Distance-Based Gaussian Graphical Model for Multivariate Time Series Classification'],'论文作者':['Hu, XG', ';\xa0', 'Liao, JX', ';\xa0', 'Li, L'],'论文摘要':['Multivariate time series classification occupies an important position in time series data mining tasks and has been applied in many fields. However, due to the statistical coupling between different variables of Multivariate Time Series (MTS) data, traditional classification methods cannot find complex dependencies between different variables, so most existing methods perform not well in MTS classification with many variables. Thus, in this paper, a novel model-based classification method is proposed, called Wasserstein Distance-based Gaussian Graphical Model classification (WD-GGMC), which converts the original MTS data into two important parameters of the Gaussian Graphical Model: the sparse inverse covariance matrix and the mean vector. Among them, the former is the most important parameter, which contains the information between variables and solved by Alternating Direction Method of Multipliers (ADMM). Furthermore, the Wasserstein Distance is applied as the similarity measure for different subsequences because it can measure the similarity between different distributions. Experimental results on the eight public MTS datasets demonstrate the effectiveness of the proposed method in MTS classification.']}
{'论文标题':['An Unsupervised Domain Adaptation Being Aware of Domain-specific and Label Information'],'论文作者':['Zhang, YH', ';\xa0', 'Zhang, Q'],'论文摘要':['Domain adaptation aims to leverage the knowledge in a label-rich source domain to facilitate the learning task in an unlabeled target domain with a different distribution. Adversarial-based domain adaptation methods have attracted increasing attention due to their remarkable performance. However, most methods learn the invariant features by aligning distribution between domains, while ignoring the domain-specific and label information. It will lead to unsatisfying invariant features and cause improper label alignment when there is much specific information. Therefore, in this paper, we propose an unsupervised method to learn more robust and discriminative invariant features for domain adaptation by using the specific features and label information. Specially, a separate batch normalization layer is introduced to replace the completely shared layer to capture domain-specific information, which will benefit the learning of invariant features. Then, to make the adaptation more sufficiently, a symmetric design of classifier and the corresponding adversarial training loss are used to realize domain-wise and label-wise alignment. The two steps are optimized iteratively to improve the performance of the model. The extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method.']}
{'论文标题':['10 Challenging problems in data mining research'],'论文作者':['Yang, Q', ' (', 'Yang, Qiang', ') ', ';\xa0', 'Wu, XD', ' (', 'Wu, Xindong', ') '],'论文摘要':['In October 2005, we took an initiative to identify 10 challenging problems in data mining research, by consulting some of the most active researchers in data mining and machine learning for their opinions on what are considered important and worthy topics for future research in data mining. We hope their insights will inspire new research efforts, and give young researchers (including PhD students) a high-level guideline as to where the hot problems are located in data mining.', 'Due to the limited amount of time, we were only able to send out our survey requests to the organizers of the IEEE ICDM and ACM KDD conferences, and we received an overwhelming response. We are very grateful for the contributions provided by these researchers despite their busy schedules. This short article serves to summarize the 10 most challenging problems of the 14 responses we have received from this survey. The order of the listing does not reflect their level of importance.']}
{'论文标题':['A Study on Big Knowledge and Its Engineering Issues'],'论文作者':['Lu, RQ', ';\xa0', 'Jin, XL', ';\xa0', 'Wu, XD'],'论文摘要':["After entering the big data era, a new term of 'big knowledge' has been coined to deal with challenges in mining a mass of knowledge from big data. While researchers used to explore the basic characteristics of big data, we have not seen any studies on the general and essential properties of big knowledge. To fill this gap, this paper studies the concepts of big knowledge, big-knowledge system, and big-knowledge engineering. Ten massiveness characteristics for big knowledge and big-knowledge systems, including massive concepts, connectedness, clean data resources, cases, confidence, capabilities, cumulativeness, concerns, consistency, and completeness, are defined and explored. Based on these characteristics, a comprehensive investigation is conducted on some large-scale knowledge engineering projects, including the Fifth Comprehensive Traffic Survey in Shanghai, the China's Xia-Shang-Zhou Chronology Project, the Troy and Trojan War Project, and the International Human Genome Project, as well as the online free encyclopedia Wikipedia. We also investigate the recent research efforts on knowledge graphs, where they are analyzed to determine which ones can be considered as big knowledge and big-knowledge systems. Further, a definition of big-knowledge engineering and its life cycle paradigm is presented. All of these projects are accordingly checked to determine whether they belong to big-knowledge engineering projects. Finally, the perspectives of big knowledge research are discussed."]}
{'论文标题':['The Device-Object Pairing Problem: Matching IoT Devices with Video Objects in a Multi-Camera Environment'],'论文作者':['Tong, KL', ';\xa0', 'Wu, KR', ';\xa0', 'Tseng, YC'],'论文摘要':['IoT technologies enable millions of devices to transmit their sensor data to the external world. The device-object pairing problem arises when a group of Internet of Things is concurrently tracked by cameras and sensors. While cameras view these things as visual "objects", these things which are equipped with "sensing devices" also continuously report their status. The challenge is that when visualizing these things on videos, their status needs to be placed properly on the screen. This requires correctly pairing visual objects with their sensing devices. There are many real-life examples. Recognizing a vehicle in videos does not imply that we can read its pedometer and fuel meter inside. Recognizing a pet on screen does not mean that we can correctly read its necklace data. In more critical ICU environments, visualizing all patients and showing their physiological signals on screen would greatly relieve nurses\' burdens. The barrier behind this is that the camera may see an object but not be able to see its carried device, not to mention its sensor readings. This paper addresses the device-object pairing problem and presents a multi-camera, multi-IoT device system that enables visualizing a group of people together with their wearable devices\' data and demonstrating the ability to recover the missing bounding box.']}
{'论文标题':['Method for processing security event data, involves using new version of knowledge graph that is to process security event data and built using software executing in multiple processors'],'论文作者':[],'论文摘要':[]}
{'论文标题':['Security and privacy protocols for perceptual image hashing'],'论文作者':['Hu, DH', ';\xa0', 'Su, B', ';\xa0', 'Wu, XD'],'论文摘要':['Perceptual hashing plays an increasingly important role in digital forensics, pirate detection and source tracking of the massive networking information. Security and privacy problems exist in existing digital image hashing efforts and have negatively influenced their applications. In this paper we propose and design security and privacy protocols for perceptual image hashing. Specifically we propose the concepts "commutative perceptual hashing and encryption" and "distance preserving feature encryption" to ensure that perceptual hashes in the encryption domain of digital images can be calculated and compared. We further propose three sets of protocols to provide security assurance and privacy protection for the whole process. Each set of protocols are applied on registration, authentication and identification stages of the digital image hashing application models. We improve existing perceptual hashing methods by implementing the security-enhanced protocols. Experimental results demonstrate the effectiveness of the improved perceptual hashing methods.']}
{'论文标题':['Induction by attribute elimination'],'论文作者':['Xindong Wu', ';\xa0', 'Urpani, D.'],'论文摘要':['In most data mining applications where induction is used as the primary tool for knowledge extraction from real world databases, it is difficult to precisely identify a complete set of relevant attributes. The paper introduces a novel rule induction algorithm called Rule Induction Two In One (RITIO), which eliminates attributes in the order of decreasing irrelevancy. Like ID3-like decision tree construction algorithms, RITIO makes use of the entropy measure as a means of constraining the hypothesis search space; but, unlike IDS-like algorithms, the hypotheses language is the rule structure and RITIO generates rules without constructing decision trees. The final concept description produced by RITIO is shown to be largely based on only the most relevant attributes. Experimental results confirm that, even on noisy, industrial databases, RITIO achieves high levels of predictive accuracy.']}
{'论文标题':['Consensus algorithms for biased labeling in crowdsourcing'],'论文作者':['Zhang, J', ';\xa0', 'Sheng, VS', ';\xa0', 'Wu, XD'],'论文摘要':['Although it has become an accepted lay view that when labeling objects through crowd sourcing systems, non-expert annotators often exhibit biases, this argument lacks sufficient evidential observation and systematic empirical study. This paper initially analyzes eight real-world datasets from different domains whose class labels were collected from crowdsourcing systems. Our analyses show that biased labeling is a systematic tendency for binary categorization; in other words, for a large number of annotators, their labeling qualities on the negative class (supposed to be the majority) are significantly greater than are those on the positive class (minority). Therefore, the paper empirically studies the performance of four existing EM-based consensus algorithms, DS, GLAD, RY, and ZenCrowd, on these datasets. Our investigation shows that all of these state-of-the-art algorithms ignore the potential bias characteristics of datasets and perform badly although they model the complexity of the systems. To address the issue of handling biased labeling, the paper further proposes a novel consensus algorithm, namely adaptive weighted majority voting (AWMV), based on the statistical difference between the labeling qualities of the two classes. AWMV utilizes the frequency of positive labels in the multiple noisy label set of each example to obtain a bias rate and then assigns weights derived from the bias rate to negative and positive labels. Comparison results among the five consensus algorithms (AWMV and the four existing) show that the proposed AWMV algorithm has the best overall performance. Finally, this paper notes some potential related topics for future study. (C) 2016 Elsevier Inc. All rights reserved.']}
{'论文标题':['Improving Crowdsourced Label Quality Using Noise Correction'],'论文作者':['Zhang, J', ';\xa0', 'Sheng, VS', ';\xa0', 'Wu, XD'],'论文摘要':['Crowdsourcing systems provide a cost effective and convenient way to collect labels, but they often fail to guarantee the quality of the labels. This paper proposes a novel framework that introduces noise correction techniques to further improve the quality of integrated labels that are inferred from the multiple noisy labels of objects. In the proposed general framework, information about the qualities of labelers estimated by a front-end ground truth inference algorithm is utilized to supervise subsequent label noise filtering and correction. The framework uses a novel algorithm termed adaptive voting noise correction (AVNC) to precisely identify and correct the potential noisy labels. After filtering out the instances with noisy labels, the remaining cleansed data set is used to create multiple weak classifiers, based on which a powerful ensemble classifier is induced to correct these noises. Experimental results on eight simulated data sets with different kinds of features and two real-world crowdsourcing data sets in different domains consistently show that: 1) the proposed framework can improve label quality regardless of inference algorithms, especially under the circumstance that each instance has a few repeated labels and 2) since the proposed AVNC algorithm considers both the number of and the probability of potential label noises, it outperforms the state-of-the-art noise correction algorithms.']}
{'论文标题':['A Novel Blockchain Based Smart Contract System for eReferral in Healthcare: HealthChain'],'论文作者':['Chenthara, S.', ';\xa0', 'Ahmed, K.', ';\xa0', 'Whittaker, F.'],'论文摘要':['The privacy of Electronic Health Records is facing a major issue while outsourcing data in the cloud or sharing the records among stakeholders which includes the leakage of private and sensitive information to unauthorized entities. This research mainly focuses on introducing an efficient referral mechanism employing advanced smart contracts for the effective sharing of healthcare records between stakeholders in healthcare industry. This referral system is designed on a patient-centric model and are limited to authorized providers in the healthdata network. This system is built by employing Hyperledger Fabric as the permissioned blockchain utilising Hyperledger composer which visualizes the couchDB and Interplanetary File System as decentralised data storage are combined for efficient and secure big data sharing in healthcare sector.']}
{'论文标题':['Fault Diagnosis of Rotating Rectifier Based on Waveform Distortion and Polarity of Current'],'论文作者':['Wei, ZH', ';\xa0', 'Liu, WG', ';\xa0', 'Ma, P'],'论文摘要':['In this paper, a rotating rectifier fault detection method is proposed and tested, by using the differences between the total harmonic distortion (THD) and polarity of asynchronous exciter armature current, when the rotating rectifier works in various conditions. To describe these differences, the conception of THD, residual and sum are used, and corresponding functions are defined. With the THD, residual and sum functions, the fault can be confirmed. Considering the influence of interference and the error of measurement, a pre-set value is employed and compared with the values of residual, THD, and sum functions. Then, the fault diagnosis table is given. The experimental results verify the feasibility and effectiveness of the proposed method both in the stationary state and rotating state.']}
